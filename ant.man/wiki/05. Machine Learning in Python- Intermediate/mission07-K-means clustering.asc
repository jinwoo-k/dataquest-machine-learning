== K-means clustering
=== 1. Clustering NBA Players
* 데이터 과학을 이용해 NBA플래이어가 얼마나 다른지 확인

=== 2. Point Guards
* 포인트가드를 그룹화
** 턴오버와 슛 기회창출

`point_guards=nba[nba["pos"]=="PG"]`

=== 3. Points Per Game
* Points per game :총 포인트와 게임수로 계산

```
point_guards['ppg'] = point_guards['pts'] / point_guards['g']
point_guards[['pts', 'g', 'ppg']].head(5) # check data
```

=== 4. Assist Turnover Ratio
* 턴오버 어시스트 구하기
** atr 컬럼 = 전체 어시스트 / 턴오버
** 중요점: 나누는 것이 0 이 아니어야 하기 때문에 체크
```
# drop data: tov = 0
point_guards = point_guards[point_guards['tov'] != 0]
point_guards['atr'] = point_guards['ast'] / point_guards['tov']
```

=== 5. Visualizing the Point Guards
```
plt.scatter(point_guards['ppg'], point_guards['atr'], c='y')
plt.title("Point Guards")
plt.xlabel('Points Per Game', fontsize=13)
plt.ylabel('Assist Turnover Ratio', fontsize=13)
plt.show()
```
image:./images/point_guard_plt.png[]


=== 6. Clustering players
* 5개의 지역 (그림을 보고 판단한듯?)
* 중심기반 클러스터링에 초점 -> 클러스터가 원과 유사(센터로 모여있는)할 경우 잘 동작
** 중심이 클러스터 내의 모든 데이터의 산술평균
* K-means clustring: 중신기반 클러스터링 알고리즘, 적합한 k 로 나눔 (k 갯수가 핵심)

=== 7. The Algorithm
* 각 클러스터의 중심과 해당 클러스터에 속한 플래이어의 재계산을 반복하는 알고리즘
** 5명의 플레이어를 무작위로 선택 후 방금 생성한 클러스터의 초기 중심을 지정
- 1단계: 클러스터에 포인트 할당 -> 각 플래이어와 중심간의 유클리드 거리 계산
- 2단계: 클러스터의 새로운 중심 업데이트 -> 각 클러스터에 대해 평균을 계산해 중심을 다시 계산
- 클러스터가 안움직일때까지 1,2 단계 반복

```
num_clusters = 5
# Use numpy's random function to generate a list, length: num_clusters, of indices
random_initial_points = np.random.choice(point_guards.index, size=num_clusters)
# Use the random indices to create the centroids
centroids = point_guards.loc[random_initial_points]
```

=== 8. Visualize Centroids
```
plt.scatter(point_guards['ppg'], point_guards['atr'], c='yellow')
plt.scatter(centroids['ppg'], centroids['atr'], c='red')
plt.title("Centroids")
plt.xlabel('Points Per Game', fontsize=13)
plt.ylabel('Assist Turnover Ratio', fontsize=13)
plt.show()
```
image:./images/point_guard_centroids.png[]

=== 9. Setup (continued)
* cluster_id 라는 걸 만들어서 반복계산 쉽게함
** key : 중심의 클러스터 아이디
** value : ppg, atr
```
def centroids_to_dict(centroids):
    dictionary = dict()
    # iterating counter we use to generate a cluster_id
    counter = 0

    # iterate a pandas data frame row-wise using .iterrows()
    for index, row in centroids.iterrows():
        coordinates = [row['ppg'], row['atr']]
        dictionary[counter] = coordinates
        counter += 1

    return dictionary

centroids_dict = centroids_to_dict(centroids)
```

=== 10. Step 1 (Euclidean Distance)
* 유클리안 거리 : 공식은 피타고라스정리와 비슷
```
import math

def calculate_distance(centroid, player_values):
    root_distance = 0
    # 아래 for loop 통해 n 차원이라도 계산 할 수 있도록 함.
    for x in range(0, len(centroid)):
        difference = centroid[x] - player_values[x]
        squared_difference = difference**2
        root_distance += squared_difference

    euclid_distance = math.sqrt(root_distance)
    return euclid_distance

q = [5, 2]
p = [3,1]

# Sqrt(5) = ~2.24
print(calculate_distance(q, p))
```

=== 11. Step 1 (Continued)
* 계산된 유클리안 거리로 클러스터에 데이터 포인트 할당
```
def assign_to_cluster(row):
    lowest_distance = -1
    closest_cluster = -1

    for cluster_id, centroid in centroids_dict.items():
        df_row = [row['ppg'], row['atr']]
        euclidean_distance = calculate_distance(centroid, df_row)

        if lowest_distance == -1:
            lowest_distance = euclidean_distance
            closest_cluster = cluster_id
        elif euclidean_distance < lowest_distance:
            lowest_distance = euclidean_distance
            closest_cluster = cluster_id
    return closest_cluster

# 하나씩 람다에 적용 (apply 활용)
point_guards['cluster'] = point_guards.apply(lambda row: assign_to_cluster(row), axis=1)
```

=== 12. Visualizing Clusters
```
def visualize_clusters(df, num_clusters):
    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']

    for n in range(num_clusters):
        clustered_df = df[df['cluster'] == n]
        plt.scatter(clustered_df['ppg'], clustered_df['atr'], c=colors[n-1])
        plt.xlabel('Points Per Game', fontsize=13)
        plt.ylabel('Assist Turnover Ratio', fontsize=13)
    plt.show()

visualize_clusters(point_guards, 5)
```
image:./images/visualize_clusters.png[]

=== 13. Step 2
* 중심 다시 계산
```
def recalculate_centroids(df):
    new_centroids_dict = dict()

    for cluster_id in range(0, num_clusters):
        values_in_cluster = df[df['cluster'] == cluster_id]
        # Calculate new centroid using mean of values in the cluster
        new_centroid = [np.average(values_in_cluster['ppg']), np.average(values_in_cluster['atr'])]
        new_centroids_dict[cluster_id] = new_centroid
    return new_centroids_dict
centroids_dict = recalculate_centroids(point_guards)
```

=== 14. Repeat Step 1
* 위 두 스탭 반복 -> 수렴할때까지
```
point_guards['cluster'] = point_guards.apply(lambda row: assign_to_cluster(row), axis=1)
visualize_clusters(point_guards, num_clusters)
```
image:./images/repeat_step1.png[]

=== 15. Repeat Step 2 and Step 1
image:./images/repeat_step2.png[]

=== 16. Challenges of K-Means
* 반복 실행 시 조금씩 변화함
** 반복 사이에 엄청난 변화가 안일어남 -> 항상 수렴하고 안정화 됨
** 변화가 보수적 -> 초기 할당이 중요
* 이때문에 sklearn 에서는 임의 초기화 실행을 여러번 해서 실행 => 초기 값에 대한 편향을 줄임

```
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=num_clusters)
kmeans.fit(point_guards[['ppg', 'atr']])
point_guards['cluster'] = kmeans.labels_

visualize_clusters(point_guards, num_clusters)
```
image:./images/sklearn_result.png[]

=== 17. Conclusion
* 중심점을 사용하지 않는 방법을 다음에 배움
* sklearn 을 사용하면 몇 줄이면 간단하고 좋은 결과를 얻을 수 있음
