== The Linear Regression Model
=== 1. Instance Based Learning Vs. Model Based Learning

* k-nearest neighbors : 비슷한 가까운 것을 찾음
** https://en.wikipedia.org/wiki/Instance-based_learning[instance-basesd-leanring] : 예측을 위해 이전값에 완전 의존적
** 이해 혹은 특성과 목표 간의 관계를 알 필요가 없음
** 전체 데이터 셋이 가까운 이웃만 찾아 예측하기 때문에 중간/큰 데이터 셋트로 확장 안함
** 트래이닝 셋에서 기본 유클리안 계산 후 테스트 셋에서 sort n^2 * n개 인스턴스 = n^3

* 매개변수 기계학습 필요 (linear regression or logistic regression)
** k-nearest와 달리 가장 근사한 패턴을 가진 수학적 함수 => 이 함수를 모델이라 함.
** 선형회귀 방정식: 특성과 목표(열)가 관계를 근사적으로 선형이라 가정
** 트래이닝 셋에서 최대한의 모델을 찾아냄 n^2, 이걸로 테스트셋에서의 계산은 쉽게하여 예측
image:./images/regression_complexity.png[]

=== 2.Introduction To The Data
* 집(단일 주택) 팔기 예제, 최종 판매 가격 예측 모델
** 가격에 최대 영향을 주는 속성?
** 속성들은 얼마나 가격에 영향을 끼치나?

```
import pandas as pd
data = pd.read_csv(filepath_or_buffer="AmesHousing.txt",sep="\t")
# data = pd.read_csv('AmesHousing.txt', delimiter="\t")

train=data.iloc[0:1460]
test=data.iloc[1460:]

data.info()

#data.rename(columns = {'SalePrice':'target'}, inplace = True)
target = 'SalePrice'
```
ref: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv

=== 3.Simple Linear Regression
* simple linear regression 부터, 직선의 방정식
** 최적의 파라미터 찾기

image:./images/simple_linear_plot.png[width="500px"]
```
import matplotlib.pyplot as plt
# For prettier plots.
import seaborn

fig = plt.figure(figsize=(7,15))

ax1 = fig.add_subplot(3, 1, 1)
ax2 = fig.add_subplot(3, 1, 2)
ax3 = fig.add_subplot(3, 1, 3)

train.plot(x="Garage Area", y="SalePrice", ax=ax1, kind="scatter")
train.plot(x="Gr Liv Area", y="SalePrice", ax=ax2, kind="scatter")
train.plot(x="Overall Cond", y="SalePrice", ax=ax3, kind="scatter")

plt.show()
```
ref: https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.figure

=== 4.Least Squares

* 두 컬럼 관계: pandas.DataFrame.corr()
** 사용 예: `train[['Garage Area', 'Gr Liv Area', 'Overall Cond', 'SalePrice']].corr()`
** 표에 나온것이 1에 가까울수록 상관 관계가 높음
* 최적의 매개변수
** residual sum of squares (RSS,잔차 합)를 최적화
** 대상열과 실제 값에 대한 예측값의 차이를 최소화
image:./images/rss.png[]
** MSE (mean squared error)
image:./images/mse.png[]

=== 5.Using Scikit-Learn To Train And Predict

```
from sklearn import linear_model
lr = linear_model.LinearRegression()

# [] [[]] 의 차이점은? x1, x2 값들에 대해서 array로 그리고 x1 값들의 array 로 들어가기 때문에 이렇게 넣은듯
lr.fit(train[['Gr Liv Area']], train['SalePrice'])

a0 = lr.intercept_  # y = a1*X + a0
a1 = lr.coef_
```
http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html[linearRegression]

=== 6.Making Predictions
* 예측방법
** 1차 방정식으로 나온 예측으로 기울기를 보고 x 값 증가에 따라 y 예측 가능 ()
** predict() 메소드로 training 데이터로 실제를 비교하여 MSE를 사용하여 유효성 검사

```
import numpy as np
from sklearn.metrics import mean_squared_error

lr = LinearRegression()
lr.fit(train[['Gr Liv Area']], train['SalePrice'])

train_predictions = lr.predict(train[['Gr Liv Area']])
test_predictions = lr.predict(test[['Gr Liv Area']])

train_mse = mean_squared_error(train_predictions, train['SalePrice'])
test_mse = mean_squared_error(test_predictions, test['SalePrice'])

train_rmse = np.sqrt(train_mse)
test_rmse = np.sqrt(test_mse)
```

=== 7.Multiple Linear Regression
* 다중 회귀
** 여러 속성과 목표의 관계
image:./images/multi_linear_regression_equation.png[]

** 중요한건 주요한 속성을 선택하기 (선택 방법에 대해 알아봄)
```
cols = ['Overall Cond', 'Gr Liv Area']
# 이전과 다른것은 array 로 특성 넣기
lr.fit(train[cols], train['SalePrice'])
train_predictions2 = lr.predict(train[cols])
```
