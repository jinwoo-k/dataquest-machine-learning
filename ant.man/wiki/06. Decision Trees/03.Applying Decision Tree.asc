== Applying Decision Trees
=== 1. Introduction to the Data Set
* 이전에 트리생성 알고리즘은  ID3 수정된 버전 사용 (https://en.wikipedia.org/wiki/C4.5_algorithm[C4.5] and https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees[CART]의 간단 버전)
* 언제 의사 결정 트리를 사용할지, 효과적으로 사용하는 방법에 대해 배움
* 이전에 사용한 50k 이상, 이하의 소득을 목표 (data ref: http://archive.ics.uci.edu/ml/datasets/Adult[캘리포니아대학자료])

=== 2. Using Decision Trees With scikit-learn
* 분류문제: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier[DecisionTreeClassifier]
* 회귀 문제: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor[DecisionTreeRegressor]
```
from sklearn.tree import DecisionTreeClassifier

# A list of columns to train with
# We've already converted all columns to numeric
columns = ["age", "workclass", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "hours_per_week", "native_country"]

# Instantiate the classifier
# Set random_state to 1 to make sure the results are consistent
clf = DecisionTreeClassifier(random_state=1)

# We've already loaded the variable "income," which contains all of the income data
clf.fit(income[columns], income["high_income"])
```

=== 3. Splitting the Data into Train and Test Sets
* 테스트와 훈련 셋트로 나누기
* 오류를 평가하여 overfit 적용을 피하기
```
import numpy
import math

# Set a random seed so the shuffle is the same every time
numpy.random.seed(1)

# Shuffle the rows
# This permutes the index randomly using numpy.random.permutation
# Then, it reindexes the dataframe with the result
# The net effect is to put the rows into random order
income = income.reindex(numpy.random.permutation(income.index))

train_max_row = math.floor(income.shape[0] * .8)
train = income[:train_max_row]
test = income[train_max_row:]
```

=== 4. Evaluating Error With AUC
* 오류 평가 https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_curve[AUC]
** 0~1 까지, 높을수록 예측치가 정확함
** http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score[roc_auc_score]
*** y_true: true labels
*** y_score: predicted labels
```
from sklearn.metrics import roc_auc_score

clf = DecisionTreeClassifier(random_state=1)
clf.fit(train[columns], train["high_income"])

predictions = clf.predict(test[columns])
error = roc_auc_score(test["high_income"], predictions)
print(error) # 0.693
```

=== 5. Computing Error on the Training Set
* 테스트의 AUC는 약 0.694, 트래이닝셋트와 비교해보자 (이걸로 오버핏 확인)
** 모델이 테스트셋트보다 훈련셋트를 더 잘 예측하는게 정상
```
predictions = clf.predict(train[columns])
error_train = roc_auc_score(train["high_income"], predictions)
print(error_train) # 0.947
```

=== 6. Decision Tree Overfitting
* 훈련의 AUC는 0.947, 테스트는 0.694, 오버핏의 빠른 룰은 없지만 훈련셋의 예측이 좋아야함
* 데이터를 테스트&훈련 데이터로 나눈다고 오버핏을 막진 못한다. (단지 감지하고 수정하는데 도움이 됨)
* AUC로 보았을때 오버핏이 보인다. 그 이유를 보기
image:./images/full_tree.png[]

* 이 나무는 모든 값을 예측할 수 있다. 훈련셋트에 대해 항상 옳은 값이지만, 룰을 계속 만든것과 같다. (이 훈련셋트에 대한 예외처리를 계속했다는 말 같음)
** 즉, 훈련 셋트에 완벽한 예외처리로만 만들어졌다는 말 (트리를 통해 데이터 해석에 대해 해석)

image:./images/small_tree.png[]
* 위는 일반화하여 만든 것
** 너무 세세하게 만든 트리에서는 25~55세 미만을 너무 세분화하여서 훈련셋트에만 특화된 예측치를 만들었음
** 이걸 좀 약하게 만들어서 25~55세를 합쳐서 평가하면 일반화에 좋음

=== 7. Reducing Overfitting With a Shallower Tree
* 오버핏에 대처하는 세가지 주요 방법
** 불필요한 나뭇잎을 제거하기 위해 나무를 쌓은 후 나무를 잘라내기
** 앙상블을 이용하여 나무의 예측을 혼합
** 트리를 만드는 동안 나무의 깊이를 제한

* 일단 세번째 나무 깊이 제한부터 보기
** DecisionTreeClassifier 클래스를 초기화 할 때 몇 개의 매개 변수를 추가하여 트리 깊이를 제한 할 수 있음
*** max_depth - 트리가 갈 수있는 깊이를 전역 적으로 제한
*** min_samples_split - 분할되기 전에 노드에 있어야하는 최소 행 수. 예를 들어 2로 설정하면 2 행이있는 노드가 분할되지 않고 대신 잎
*** min_samples_leaf - 리프가 가져야하는 최소 행 수
*** min_weight_fraction_leaf - 리프가 가져야하는 입력 행의 비율
*** max_leaf_nodes - 최대 총 리프 수. 이것은 트리가 빌드 될 때 리프 노드 수를 제한
*** 그러나 이러한 매개 변수 중 일부는 호환되지 않음. 예를 들어 max_depth와 max_leaf_nodes를 함께 사용할 수 없음

```
# Decision trees model from the last screen
clf = DecisionTreeClassifier(random_state=1, min_samples_split=13)


clf.fit(train[columns], train["high_income"])
predictions = clf.predict(train[columns])
train_auc = roc_auc_score(train["high_income"], predictions)
print(train_auc) # 0.842

predictions_test = clf.predict(test[columns])
test_auc = roc_auc_score(test["high_income"], predictions_test)
print(test_auc) # 0.699
```

=== 8. Tweaking Parameters to Adjust AUC
[width="40%",frame="topbot",options="header,footer"]
|======================
|settings |	train AUC	| test AUC
|default	|0.947	|0.694
|min_samples_split: 13	|0.843	|0.70
|======================
```
# The first decision trees model we trained and tested
clf = DecisionTreeClassifier(random_state=1, max_depth=7, min_samples_split=13)
clf.fit(train[columns], train["high_income"])
predictions = clf.predict(test[columns])
test_auc = roc_auc_score(test["high_income"], predictions)

train_predictions = clf.predict(train[columns])
train_auc = roc_auc_score(train["high_income"], train_predictions)

print(test_auc) # 0.743
print(train_auc) $0.748
```

=== 9. Tweaking Tree Depth to Adjust AUC
* 훈련 결과가 0.748로 줄었다.
** 훈련과 테스트의 값이거의 동일하기 떄문에 더이상 오버핏이 없다
[width="40%",frame="topbot",options="header,footer"]
|======================
|settings |	train AUC	| test AUC
|default	|0.947	|0.694
|min_samples_split: 13	|0.843	|0.70
|min_samples_split: 13, max_depth: 7	|0.748	|0.7744
|======================

```
# The first decision tree model we trained and tested
clf = DecisionTreeClassifier(random_state=1, max_depth=2, min_samples_split=100)
clf.fit(train[columns], train["high_income"])
predictions = clf.predict(test[columns])
test_auc = roc_auc_score(test["high_income"], predictions)

train_predictions = clf.predict(train[columns])
train_auc = roc_auc_score(train["high_income"], train_predictions)

print(test_auc) # 0.655
print(train_auc) # 0.662
```

=== 10. Underfitting in Simplistic Trees
* 아래에 보면 이전보다 더 낮아졌다. 이것은 https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted[언더핏]이다.
[width="40%",frame="topbot",options="header,footer"]
|======================
|settings |	train AUC	| test AUC
|default	|0.947	|0.694
|min_samples_split: 13	|0.843	|0.70
|min_samples_split: 13, max_depth: 7	|0.748	|0.7744
|min_samples_split: 100, max_depth: 2	|0.662	|0.655
|======================
image:./images/right_tree.png[widht="400px"]
image:./images/underfit_tree.png[widht="200px"]

=== 11. The Bias-Variance Tradeoff
* 너무 작게 만들어서 분류하기에 충분하지 못했음, 근데 너무 세분화 하면 훈련데이터에만 적합해지고 일반화 안됨
** 이것또한 분산-편향 trade-off 이다.
** http://scott.fortmann-roe.com/docs/BiasVariance.html[BiasVariance]
* 매개변수를 수동으로 조절하여 올바른 값을 찾아야 함

=== 12. Exploring Decision Tree Variance
* 분산을 유도하면 의사결정트리에 어떤 일이 일어나는지 볼 수 있음
** 일부러 노이즈를 데이터에 넣기 -> 과부하 (분산이 큰 모델은 데이터의 작은 변화에 민감하게 변함)
```
numpy.random.seed(1)

# Generate a column containing random numbers from 0 to 4
income["noise"] = numpy.random.randint(4, size=income.shape[0])

# Adjust "columns" to include the noise column
columns = ["noise", "age", "workclass", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "hours_per_week", "native_country"]

# Make new train and test sets
train_max_row = math.floor(income.shape[0] * .8)
train = income.iloc[:train_max_row]
test = income.iloc[train_max_row:]

# Initialize the classifier
clf = DecisionTreeClassifier(random_state=1)
clf.fit(train[columns], train["high_income"])
predictions = clf.predict(test[columns])
test_auc = roc_auc_score(test["high_income"], predictions)
print(test_auc) # 0.691

predictions_train = clf.predict(train[columns])
train_auc = roc_auc_score(train["high_income"], predictions_train)
print(train_auc) #0.975
```

=== 13. Pruning Leaves to Prevent Overfitting
* 이전 결과로 보아 랜덤 노이즈는 상당한 오버핏을 초래함. 테스트 정확도는 0.69로 감소, 훈련은 0.97로 증가
** 오버피팅을 방지하는 한가지 방법은 트리를 특정깊이 이상 커지지 않게 함
** 또다른 방법은 가지치기 (pruning). 전체트리를 만든 후 예상 정확도에 추가하지 않을 잎을 제거 (지나치게 복잡해 지는걸 방지)

=== 14. Knowing When to Use Decision Trees
* 의사결정트리 장점
** 해석이 쉽다.
** 비교적 빠르게 적응하고 예측
** 여러 유형의 데이터 처리
** 비선형성 사용가능, 일반적으로 상당히 정확
** 주요 단점은 지나치게 오버핏될 경향
* 의사결정 트리는 자신의 일을 왜하는지 해석하고 전달하는 작업에 적합
* 오버핏을 줄이는 방법은 나무의 앙상블 만들기, 랜덤 포리스트 알고리즘이 널리 사용됨
** 예측 정확도가 중요하다면 랜덤 포리스트가 더 잘 수행됨
