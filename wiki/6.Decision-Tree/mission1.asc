= Introduction To Decision Trees

== 1. Introduction
    * 의사 결정 트리는 강력하고 널리 사용되는 기계 학습 기술입니다.
    * 기본 개념은 의사 결정을 돕기 위해 흔히 사용되는  트리와 매우 유사합니다.
image:./images/image1-1.PNG[]
    * 위의 다이어그램에서 우리는 우리 앞에있는 곰과 씨름해야하는지 여부를 결정
    * 다양한 기준을 사용하여 곰의 크기와 도망 갈 수 있는지 여부 등 최종 결정을 내립니다.
image:./images/image1-2.PNG[]
    * 우리가 곰을 만났을 때 살아남을 확률을 최적화하고 싶다면 취할 조치를 알려주는 의사 결정 트리를 구성 할 수 있습니다.
    * 우리의 데이터 세트가 커짐에 따라, 이는 점점 더 실용적이지 않게됩니다.
    * 의사 결정 트리 알고리즘은 감독 학습 알고리즘입니다.
    * 먼저 이력 데이터로 트리를 구성한 다음이를 사용하여 결과를 예측
    * 의사 결정 트리의 주요 이점 중 하나는 선형 회귀가 수행 할 수없는 데이터에서 변수 간의 비선형 상호 작용을 선택할 수 있다는 것
    * 우리의 곰 레슬링 예제에서, 결정 트리는 탈출이 불가능할 때 큰 곰과 씨름해야 한다는 사실을 받아 들일 수 있습니다.
    * 반면에 선형 회귀는 대안이 없을 때 두 요인 모두를 가중 시켰을 것입니다.
    * 분류나 회귀문제에 트리를 사용할 수 있습니다.
    * 이 미션에서는 자동으로 의사 결정 트리를 만드는 기본 구성 요소를 살펴 보겠습니다.

== 2. Overview of the Data Set
    * 우리는 미국의 개인 소득을 살펴볼 것입니다.
    * 이 데이터는 1994 년 인구 조사에서 나온 것으로 개인의 혼인 상태, 나이, 직업 유형 등에 대한 정보가 포함되어 있습니다.
    * 목표 칼럼 또는 우리가 예측하고자하는 것은 개인이 연간 50k 또는 연간 50k 이상의 소득을 가지는지 여부입니다.

== 3. Converting Categorical Variables
    * 데이터에서 알 수 있듯이 문자열 값이있는 workclass 와 같은 범주형 변수가 있습니다.
       ** 여러 개인이 동일한 문자열 값을 공유 할 수 있습니다.
       ** 업무 유형에는 State-gov, Self-emp-not-inc, Private 등이 포함됩니다.
       ** 이 문자열 각각은 범주의 레이블입니다.
    * 범주 열의 또 다른 예는 성별이며, 옵션은 남성과 여성입니다
    * 의사 결정 트리를 시작하기 전에 데이터 세트의 범주 형 변수를 숫자 형 변수로 변환해야 합니다.
    * 여기에는 각 범주 레이블에 숫자를 할당 한 다음 열의 모든 레이블을 해당 숫자로 변환하는 작업이 포함됩니다.
    * 한 가지 전략은 열을 범주형으로 변환하는 것입니다.
    * 이 접근법에서 pandas는 라벨을 문자열로 표시 할 것이지만, 내부적으로 숫자로 저장하여 계산할 수 있습니다.
    * 숫자는 Scikit-learn과 같은 다른 라이브러리와 항상 호환되는 것은 아니기 때문에 숫자로 미리 변환하는 것이 더 쉽습니다.
    * pandas의 Categorical.from_array 메소드를 사용하여 숫자로 변환 할 수 있습니다.

```
# Convert a single column from text categories to numbers
col = pandas.Categorical.from_array(income["workclass"])
income["workclass"] = col.codes
print(income["workclass"].head(5))
for name in ["education", "marital_status", "occupation", "relationship", "race", "sex", "native_country", "high_income"]:
    col = pandas.Categorical.from_array(income[name])
    income[name] = col.codes
```

== 4. Splitting Data
    * 의사 결정 트리는 일련의 노드와 분기로 구성됩니다.
       ** 노드는 변수를 기반으로 데이터를 분할하는 곳이며
       ** 분기는 분할의 한면입니다.
image:./images/image1-3.PNG[]
    * 위의 다이어그램에서 노드는 개인이 개인부문에서 일하는지 여부에 따라 데이터를 두 개의 분기로 분할합니다.
     * workclass 열의 Private 값을 숫자 코드 4에 매핑했습니다.
     * 따라서 No 분기는 workclass! = 4에 해당하고 Yes 분기는 workclass == 4에 해당합니다.
     * 이것은 정확하게 의사 결정 트리가 작동하는 방식입니다.
     * 우리는 변수를 기반으로 데이터를 계속 분할합니다. 이렇게하면 트리는 더 많은 레벨을 축적합니다.
     * 위에서 만든 트리는 두 개의 레벨로 구성되어 있습니다. 왜냐하면 하나의 분할과 두 개의 "레벨"노드가 있기 때문입니다.
     * 아래 트리는 3단계 깊이입니다.
image:./images/image1-4.PNG[]
     * 이 트리의 첫 번째 분리점 "아래"에 또 다른 분리 점을 추가했습니다. 이것은 나무를 3 단계로 깊게 만들었습니다.

== 5. Creating Splits
    * 이제 의사 결정 트리를 통해 흐르는 데이터 행을 생각해보십시오.
    * 위의 다이어그램에서 개인이 개인부문에서 근무하는지 여부에 따라 데이터 세트를 두 부분으로 나눌 수 있습니다.
```
    # Enter your code here
    private_incomes = income[income["workclass"] == 4]
    public_incomes = income[income["workclass"] != 4]
    print(private_incomes.shape)
    print(public_incomes.shape)
```

== 6. Decision Trees as Flows of Data
    * 분할을 수행 할 때 workclass 가 4와 같지 않은 왼쪽에 9865 개의 행이 있고, workclass 4 인 22696 개의 행이 오른쪽으로 이동했습니다.
    * 의사 결정 트리를 데이터 행의 흐름으로 생각하면 유용합니다.
    * 분할하면 일부 행이 오른쪽으로 이동하고 일부는 왼쪽으로 이동합니다.
    * 우리가 트리를 깊고 깊게 만들수록 각 노드는 더 적은 수의 행을 받습니다.
    * 다음은 분할과 각 노드에 존재할 행 수를 보여줍니다.
image:./images/image1-5.PNG[]

== 7. Splitting Data to Make Predictions
    * 분할을 중지하기로 결정한 트리의 맨 아래에있는 노드를 종단 노드 또는 종단이라고합니다.
    * 우리가 쪼개 질 때, 우리는 무작위로하지 않습니다. 우리에게는 목표가 있습니다.
    * 우리의 목표는 미래의 데이터에 대한 예측을 할 수있게하는 것입니다.
    * 이렇게하려면 각 리프의 모든 행에 목표 열에 대해 하나의 값만 있어야 합니다.
    * 우리는 high_income 열을 예측하려고합니다.
       ** high_income이 1 인 경우, 그 사람이 연간 50k 이상 소득을 가지고 있음을 의미합니다.
       ** high_income이 0이면 연간 소득이 50k 이하인 것입니다.
    * 소득 데이터를 사용하여 나무를 만든 후에는 예측을하고 싶습니다.
    * 이렇게하기 위해 우리는 새로운 행을 가져와 결정 트리를 통해 피드를 만듭니다.
      ** 먼저 우리는 그 사람이 민간 부문에서 일하고 있는지 확인합니다.
      ** 만약 그렇다면, 우리는 그들이 미국 출신인지 등을 점검 할 것입니다.
      ** 결국 우리는 잎에 닿을 것입니다.
      ** 잎은 high_income에 대해 우리가 예측해야하는 가치를 알려줍니다.
     * 이 예측을 수행하려면 리프의 모든 행에 high_income에 대한 단일 값만 있어야합니다.
     * 리프은 high_income 열에 0과 1 값을 둘 다 가질 수 없습니다.
     * 각 리프에는 대상 열에 대해 동일한 값을 가진 행만있을 수 있습니다. 그렇지 않은 경우 효과적인 예측을 할 수 없습니다.
     * 노드의 모든 행이 high_income에 대해 동일한 값을 가질 때까지 노드를 계속 분할해야 합니다.

== 8. Overview of Data Set Entropy
    * 이제 의사 결정 트리가 어떻게 작동하는지에 대해 자세히 살펴 보았습니다.
    * 세부 정보를 탐색하고 분할을 수행하는 방법을 배우겠습니다.
    * 특정 척도를 사용하여 노드를 분할해야하는 변수를 파악합니다.
    * 분할 후, 우리는 두 개의 데이터 세트를 가지며, 각각은 분할의 한 지점에서 행을 포함합니다.
    * 우리가 high_income에서 오직 1 또는 0만을 갖는 나뭇잎에 도달하려고하기 때문에 각 분할은 우리를 그 목표에 더 가깝게 할 필요가 있습니다.
    * 분할 할 때 가능한 한 high_income 열에서 1과 많은 0을 분리하려고 시도합니다.
    * 이렇게 하려면 high_income에 <얼마나  다른 여러 값이 함께 있는가에 대한 메트릭> 이 필요합니다.
    * 데이터 과학자들은 일반적으로 이러한 목적으로 엔트로피라는 측정 기준을 사용합니다.
    * 엔트로피는 무질서를 지칭합니다.
    * 혼재하고 있는 1과 0이 많을수록 엔트로피가 높아집니다.
    * high_income 열에 1을 완전히 포함하는 데이터 세트는 엔트로피가 낮습니다.
    * 엔트로피는 물리학의 엔트로피와 혼동되어서는 안되며, 정보 이론에서 비롯됩니다.
    * 정보 이론은 확률 및 통계를 기반으로하며 정보의 전송, 처리, 활용 및 추출을 다룹니다.
        ** 참고 :  https://www.youtube.com/watch?v=zJmbkp9TCXY[정보량을 나타내는 엔트로피]
    * 정보를 이진수로 표현할 수 있는데, 그 이유는 1 또는 0 값이기 때문입니다.
      ** 내일 맑음 (1) 또는 맑음 (0)이 될 확률이 같다고 가정합니다.
      * 양지 바른 날이라고 말하면, 나는 당신에게 정보를 전해주었습니다.
    * 우리는 또한 엔트로피에 대한 정보를 생각할 수 있습니다.
      **  우리가 양쪽이  head인 동전을 던지면, 그 결과가 머리가 될 것이라는 것을 미리 알 수 있습니다.
      ** 동전을 뒤집어서 새로운 정보를 얻을 수 없으므로 엔트로피는 0입니다.
      ** 반면에 동전에 머리 쪽과 꼬리 쪽이 있으면 50 %의 가능성이 있습니다.
      ** 따라서 동전을 뒤집으면 동전이 어느쪽에 상륙했는지 한 비트의 정보를 얻을 수 있습니다.
     * 엔트로피는 훨씬 더 복잡 할 수 있습니다.
     * 특히 두 가지 이상의 가능한 결과 또는 차별 가능성이있는 사례를 발견 할 때 더욱 그렇습니다.
     * 그러나 엔트로피에 대한 깊은 이해는 의사 결정 트리를 구성하는 데 반드시 필요한 것은 아닙니다.
     * 엔트로피에 대한 공식은 다음과 같습니다.
image:./images/image1-6.PNG[]
     * 단일 열 (이 경우 high_income)의 각 고유 값을 반복하여 i에 할당합니다.
     * 그런 다음 데이터 (P (xi))에서 그 값이 발생할 확률을 계산합니다.
     * 다음으로 우리는 곱셈을하고 모든 값을 합산합니다.
     * b는 대수의 밑입니다. 일반적으로 값 2를 사용하지만 10 또는 다른 값으로 설정할 수도 있습니다.
        ** 참고 : https://ko.wikipedia.org/wiki/%EB%82%B4%ED%8A%B8[비트,트리트,밴,내트]
     * 다음과 같은 데이터가 있다고 하자

image:./images/image1-7.PNG[]

     * 이 데이터의 엔트로피를 계산하면 다음과 같다.

image:./images/image1-8.PNG[]

    * 샘플 데이터에 0보다 약간 더 많은 1이 있기 때문에 .97의 정보 만 가져옵니다.
    * 즉, 우리가 새로운 값를 예측한다면, 대답은 1이고, 잘못된 것보다 더 자주 맞을 수 있습니다.

== 9. Overview of Data Set Entropy
     * 소득 데이터 프레임에서 high_income 열의 엔트로피를 계산하고 결과를 income_entropy에 할당합니다.
```
import math
# We'll do the same calculation we did above, but in Python
# Passing in 2 as the second parameter to math.log will take a base 2 log
prob_0 = income[income["high_income"] == 0].shape[0] / income.shape[0]
prob_1 = income[income["high_income"] == 1].shape[0] / income.shape[0]
income_entropy = -(prob_0 * math.log(prob_0, 2) + prob_1 * math.log(prob_1, 2))
```

== 10. Information Gain
    * 우리는 엔트로피 계산에서 어떤 변수를 분리 할 것인지 파악하는 방법이 필요합니다.
    * 정보 이득은 엔트로피를 가장 많이 줄일 수있는 부분을 알려줍니다.
    * 정보 이득을 위한 공식은 다음과 같습니다

image:./images/image1-9.PNG[]

    * 복잡한 것처럼 보일 수도 있지만, 우리는 그것을 분해 할 것입니다.
       ** 우리는 주어진 목표 변수 (T)에 대한 정보 이득 (IG)과 분할하고자하는 변수 (A)를 계산하고 있습니다.
       ** 이를 계산하기 위해 먼저 T에 대한 엔트로피를 계산합니다.
       ** 그런 다음 변수 A의 각 고유 값 v에 대해 A가 값 v를 취하는 행 수를 계산하고 총 행 수로 나눕니다.
       ** 다음으로, 결과에 A가 v 인 행의 엔트로피를 곱합니다.
       ** 모든 하위 엔트로피를 모두 더한 다음 전체 엔트로피에서 빼서 정보이득을 얻습니다.
    * 다른 설명이 있습니다.
       ** 분할 후 각 집합의 엔트로피를 찾고,
       ** 각 분할의 항목 수로 가중치를 부여한 다음
       ** 현재 엔트로피에서 뺍니다.
       ** 결과가 양수이면 분할로 엔트로피를 낮춥니다. 결과가 높을수록 엔트로피가 낮아집니다.
     * 트리를 구성하기위한 한 가지 전략은 분할 할 변수에 대해 고유 한 값이 있으므로 각 노드에 분기를 많이 만드는 것입니다.
     * 변수에 3 ~ 4 개의 값이 있으면 3 ~ 4 개의 분기로 끝납니다.
     * 이 방법은 대개 가치가있는 것보다 더 복잡하고 예측 정확도를 향상시키지 않지만 알만한 가치가 있습니다.
     * 정보 획득 계산을 단순화하고 더 쉽게 나누기 위해 각 고유 값에 대해 이를 수행하지 않습니다. 대신에 분할 할 변수의 중간값을 찾습니다.
     * 변수의 값이 중앙값보다 낮은 모든 행은 왼쪽 분기로 이동하고 나머지 행은 오른쪽 분기로 이동합니다.
     * 정보 획득을 계산하기 위해 두 개의 하위 집합에 대한 엔트로피 만 계산하면됩니다.
     * 이전에 사용한 데이터 세트와 동일한 데이터 세트를 사용하는 예가 있습니다.

image:./images/image1-10.PNG[]

    * 이 데이터 세트를 나이를 기준으로 분할하려고한다고 가정 해 보겠습니다.
      ** 먼저 중앙값 인 50을 계산합니다.
      ** 그런 다음 중간 값 인 0보다 작은 값 (split_age라는 새 열에 있음)과 다른 행 1을 할당합니다.
    * 이제 엔트로피를 계산합니다.

image:./images/image1-11.PNG[]

    * 우리는 .17로 끝납니다.
    * 이는 나이 변수에 데이터 세트를 나누어서 .17 비트의 정보를 얻음을 의미합니다.

== 11. Information Gain
    * 소득의 연령 항목에서 나누기위한 정보 이득을 계산하십시오.
    * 먼저, 연령의 중앙값을 계산하십시오.
    * 그런 다음 왼쪽 브랜치에 중앙값보다 작거나 같은 값을 지정하고 올바른 브랜치의 중앙값보다 큰 값을 지정하십시오.
    * 정보 이득을 계산하여 age_information_gain에 할당합니다.

```
import numpy

def calc_entropy(column):
    """
    Calculate entropy given a pandas series, list, or numpy array.
    """
    # Compute the counts of each unique value in the column
    counts = numpy.bincount(column)
    # Divide by the total column length to get a probability
    probabilities = counts / len(column)

    # Initialize the entropy to 0
    entropy = 0
    # Loop through the probabilities, and add each one to the total entropy
    for prob in probabilities:
        if prob > 0:
            entropy += prob * math.log(prob, 2)

    return -entropy

# Verify that our function matches our answer from earlier
entropy = calc_entropy([1,1,0,0,1])
print(entropy)

information_gain = entropy - ((.8 * calc_entropy([1,1,0,0])) + (.2 * calc_entropy([1])))
print(information_gain)
income_entropy = calc_entropy(income["high_income"])
```

== 12. Finding the Best Split
    * 이제 정보 획득을 계산하는 방법을 알게되었으므로 노드를 분할하는 최상의 변수를 결정할 수 있습니다.
    * 우리가 트리를 시작할 때, 우리는 초기 분할을하고 싶습니다.
    * 우리는 어떤 분할이 가장 높은 정보 이득을 가질지를 계산하여 분할 할 변수를 찾습니다.

```
def calc_information_gain(data, split_name, target_name):
    """
    Calculate information gain given a data set, column to split on, and target
    """
    # Calculate the original entropy
    original_entropy = calc_entropy(data[target_name])

    # Find the median of the column we're splitting
    column = data[split_name]
    median = column.median()

    # Make two subsets of the data, based on the median
    left_split = data[column <= median]
    right_split = data[column > median]

    # Loop through the splits and calculate the subset entropies
    to_subtract = 0
    for subset in [left_split, right_split]:
        prob = (subset.shape[0] / data.shape[0])
        to_subtract += prob * calc_entropy(subset[target_name])

    # Return information gain
    return original_entropy - to_subtract

# Verify that our answer is the same as on the last screen
print(calc_information_gain(income, "age", "high_income"))

columns = ["age", "workclass", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "hours_per_week", "native_country"]
information_gains = []
# Loop through and compute information gains
for col in columns:
    information_gain = calc_information_gain(income, col, "high_income")
    information_gains.append(information_gain)

# Find the name of the column with the highest gain
highest_gain_index = information_gains.index(max(information_gains))
highest_gain = columns[highest_gain_index]
```

== 13. Build the Whole Tree
    * 이제 우리는 하나의 분할을 만드는 방법을 알고 있습니다.
    * 전체 트리를 구성하려면 리프가 단일 클래스 만 가질 때까지 계속 분할을 작성해야합니다.
    * 다음은 이 모양의 예입니다.

image:./images/image1-12.PNG[]


    * 보시다시피, 우리는 트리 아래의 모든 데이터를 나뭇잎으로 나누었습니다.
    * 이 예는 다른 변수입니다. 분할 할 단일 변수 만 있기 때문입니다.
    * 결과적으로 한 행에 두 행의 나이가 50이므로 결과를 나눌 수 없습니다.
    * 그러나 하나는 high_income이 1이고 다른 하나는 high_income이 0입니다.
    * 즉, 일반적으로 다른 변수로 분할 할 수 있지만이 경우에는 불가능합니다.
    * 대신, 우리는 이 리프에서 끝나는 모든 행을 0.5로 예측합니다.
    * 이와 같이 트리를 구성하려면 각 노드를 여러 번 분할 할 수 있어야합니다.

== 14. Next Steps
    * 지금까지 우리는 의사 결정 트리를 구성하기 위해 ID3 알고리즘을 따라 왔습니다.
    * 분할 기준에 대해 다른 측정 값을 사용하는 CART와 같은 다른 알고리즘이 있습니다.
    * 우리는 미래의 미션에서 이러한 다른 알고리즘에 대해 더 많이 배우게 될 것입니다.
    * 다음 임무에서는 ID3 알고리즘을 사용하여 전체 트리를 재귀 적으로 생성하는 방법을 알아 봅니다.
        
