== Multivariate K-Nearest Neighbors

 * 예측 정확도 및 학습 성능 향상을 위해서 여러개의 변수에 대해서 처리하는 기법에 대한 학습
 * 필드중 숫자이며 값의 크기가 의미가 있는 경우에 한해 처리 가능
 * 숫자 필드중 누락된 값이 있는 필드에 대한 처리 (누락 비율에 따라 필드 제거 또는 row 제거)
 * 값의 범위가 다름으로 인하여 유클리드 거리에 미치는 영향이 차이가 날 수 있으므로 평균을 0, 표준편차를 1로 변경하는데 이를 Normalize 한다 라고 함
 * Normalized Value = (x - 평균) / 표준편차
 * 다변량 변수를 활용하는 유클리드 거리를 구하는 함수
 * 관련 라이브러리를 사용하면 K-Nearest Neighbor 학습을 쉽게 할 수 있다. 예) scikit-learn lib
 * MSE, RMSE 가 적어질 수록 예측 정확도가 높음
 * 적절한 모델을 사용해야 예측 정확도 향상이 가능하다. (많이 사용하다고 좋아지는건 아님)

== pandas/numpy 활용 주요 function
 * function
   ** 작업
     *** drop : row 또는 필드 제거 예) dc_listings.drop(drop_columns, axis=1)
     *** dropna : 값이 없는 필드 또는 row 를 제거 (axis = 0 은 row, 1 은 col ?)
     *** isnull().sum() : 값이 없는 항목의 갯수 (필드별)
     *** std : 표준편차 예) dc_listings.std()
     *** head(n) : 첫 줄부터 n 개 가져옴

== scipy.spatial lib
 * 라이브러리 사용 선언
   ** from scipy.spatial import distance : 유클리드 거리 함수 사용
 * function
   ** 작업
     *** distance.euclidean : 주어진 두 리스트간의 유클리드 거리를 구함 예) distance.euclidean(first_listing, fifth_listing)

== sklearn lib
 * 라이브러리 사용 선언
   ** from sklearn.neighbors import KNeighborsRegressor
   ** from sklearn.metrics import mean_squared_error
 * function
   ** KNeighborsRegressor 사용 순서
     *** 알고리즘 정의 : 예) knn = KNeighborsRegressor(n_neighbors=5, algorithm='brute', metric='euclidean')
     **** n_neighbors : k-nearest 에서의 k 로 학습에 사용할 가장 가까운 이웃의 수
     **** algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}
     **** metric : {'minkowski', 'euclidean'}
     *** 학습 셋 정의 : 예) knn.fit(train_df[train_columns], train_df['price'])
     *** 테스트 셋 정의 및 예측 : 예) predictions = knn.predict(test_df[train_columns])
   *** mean_squared_error(정답, 예측) : 예) mean_squared_error(test_df['price'], predictions)
