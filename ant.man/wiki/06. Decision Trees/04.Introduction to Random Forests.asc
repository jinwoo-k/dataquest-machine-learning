== Introduction to Random Forests
=== 1. Introduction
* 랜덤 포리스트 만들고 적용하는 방법 배우기
* 그동안 했던 동일한 데이터 50k 이상 이하 소득

=== 2. Combining Model Predictions With Ensembles
* 랜덤 포리스트는 일종의 앙상블 모델
** 앙상블: 여러 모델의 예측을 결합해 더 정확한 최종 예측을 만듦
* 약간 다른 매개변수를 가진 두 의사결정 트리를 만듦
** min_samples_leaf가 2로 설정된 경우
** max_depth가 5로 설정된 경우
* 그 후 정확도를 개별적으로 확인, 그다음 예측을 결합 후 결합된 정확도와 비교

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score

columns = ["age", "workclass", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "hours_per_week", "native_country"]

clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)
clf.fit(train[columns], train["high_income"])

clf2 = DecisionTreeClassifier(random_state=1, max_depth=5)
clf2.fit(train[columns], train["high_income"])

predictions = clf.predict(test[columns])
print(roc_auc_score(test["high_income"], predictions)) # 0.687

predictions = clf2.predict(test[columns])
print(roc_auc_score(test["high_income"], predictions)) # 0.675
```

=== 3. Combining Our Predictions
* 예측 분류를 여러개로할때, 각 예상 집합을 행렬의 열로 처리. 아래에 3개의 예
```
DT1     DT2    DT3
0       1      0
1       1      1
0       0      1
1       0      0
```
* 원래는 한행당 하나의 예측을 원함. 이를 위해 아래처럼 단일 숫자로 변환하는 규칙 만들어야 함
```
DT1     DT2    DT3    Final Prediction
0       1      0      0
1       1      1      1
0       0      1      0
1       0      0      0
```
* 최종벡터 얻는 방법 : 과반수표 (홀수개)
* 이전에 했던것은 두 분류 기준만 있기 때문에 모든 항목의 평균을 취할 예정
** RandomForestClassifier.predict_proba()를 대신 사용하여 0,1 이 아닌 0~1 사이의 확률을 구함
*** 이를 통해 평균으로 확률 계산 가능, 평균에서 반올림을 통해 0 / 1 예측 가능

=== 4. Combining Our Predictions
```
predictions = clf.predict_proba(test[columns])[:,1]
predictions2 = clf2.predict_proba(test[columns])[:,1]

# 0.715
print(roc_auc_score(test["high_income"],numpy.round((predictions+predictions2)/2)))
```

=== 5. Why Ensembling Works
* 이전화면에서 볼 수 있듯, 두 트리의 결합이 자체 보다 높은 AUC를 가짐
[width="40%",frame="topbot",options="header,footer"]
|======================
|settings	|test AUC
|min_samples_leaf: 2	|0.688
|max_depth: 2	|0.676
|combined predictions	|0.715
|======================
* 왜 합리적일까? 두 분류가 다른 경험의 대한 강점을 결합했기 때문에..
** 반면 앙상블이 예측을 만드는 방법이 매우 유사하면 무시무시한 결과를 낳을 것임
** 매우 다른 정확도를 가진 모델을 앙상블해도 일반적으로 정확도는 향상되지 않음
*** 이를 해결하는 방법은 나중에 배움. (가중치,weighting)

=== 6. Introducing Variation With Bagging
* 랜덤 포리스트는 의사결정트리의 앙상블임
** 나무를 수정하지 않으면 앙상블에 도움이 안됨
** 앙상블을 효과적으로 하기 위해 각 개별 의사결정 트리 모델에 변형을 해야함
** 서로 다른 예측을 하게 하여 랜덤 포리스트에 넣음
* 랜덤 포리스트에 두가지 방법이 있음. 짐싸기를 먼저 시작
** 전체 데이터 셋트에 대해 각 트리를 교육하지 않음. 대신 데이터의 무작위 샘플 (가방이라 부름)을 교육
** 이 샘플링은 교체로 수행. 원래 데이터의 일부 행은 "가방"에 여러번 나타날 수 있음

```
# We'll build 10 trees
tree_count = 10

# Each "bag" will have 60% of the number of original rows
bag_proportion = .6

predictions = []
for i in range(tree_count):
    # We select 60% of the rows from train, sampling with replacement
    # We set a random state to ensure we'll be able to replicate our results
    # We set it to i instead of a fixed value so we don't get the same sample in every loop
    # That would make all of our trees the same
    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)

    # Fit a decision tree model to the "bag"
    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)
    clf.fit(bag[columns], bag["high_income"])

    # Using the model, make predictions on the test data
    predictions.append(clf.predict_proba(test[columns])[:,1])

combined = numpy.sum(predictions, axis=0) / 10
rounded = numpy.round(combined)

print(roc_auc_score(test["high_income"], rounded)) # 0.732
```

=== 7. Selecting Random Features
* 이전에서 가방을 이용해 하나의 의사결정트리로 더 정확성을 얻음
[width="40%",frame="topbot",options="header,footer"]
|======================
|settings	|test AUC
|min_samples_leaf: 2	|0.688
|max_depth: 2	|0.676
|combined predictions	|0.715
|min_samples_leaf: 2, with bagging	|0.732
|======================
* 임의의 피쳐 셋트 설명
** 먼저 분할할 때마다 평가할 최대 feature 수 (이 값은 데이터의 총 열 수보다 작아야 함)
** 분할할때마다 데이터에서 무작위로 샘플 선택
** 정보획득량을 계산하고 가장높은 걸 선택하고 분할
*** 노드의 엔트로피를 최소화하는 최적의 분할을 하기 위해 위 프로세스 반복
```
# Create the data set that we used two missions ago
data = pandas.DataFrame([
    [0,4,20,0],
    [0,4,60,2],
    [0,5,40,1],
    [1,4,25,1],
    [1,5,35,2],
    [1,5,55,1]
    ])
data.columns = ["high_income", "employment", "age", "marital_status"]

# Set a random seed to make the results reproducible
numpy.random.seed(1)

# The dictionary to store our tree
tree = {}
nodes = []

# The function to find the column to split on
def find_best_column(data, target_name, columns):
    information_gains = []

    # Insert your code here
    cols = numpy.random.choice(columns, 2)
    for col in columns:
        information_gain = calc_information_gain(data, col, "high_income")
        information_gains.append(information_gain)

    # Find the name of the column with the highest gain
    highest_gain_index = information_gains.index(max(information_gains))
    highest_gain = columns[highest_gain_index]
    return highest_gain

# The function to construct an ID3 decision tree
def id3(data, target, columns, tree):
    unique_targets = pandas.unique(data[target])
    nodes.append(len(nodes) + 1)
    tree["number"] = nodes[-1]

    if len(unique_targets) == 1:
        if 0 in unique_targets:
            tree["label"] = 0
        elif 1 in unique_targets:
            tree["label"] = 1
        return

    best_column = find_best_column(data, target, columns)
    column_median = data[best_column].median()

    tree["column"] = best_column
    tree["median"] = column_median

    left_split = data[data[best_column] <= column_median]
    right_split = data[data[best_column] > column_median]
    split_dict = [["left", left_split], ["right", right_split]]

    for name, split in split_dict:
        tree[name] = {}
        id3(split, target, columns, tree[name])


# Run the ID3 algorithm on our data set and print the resulting tree
id3(data, "high_income", ["employment", "age", "marital_status"], tree)
print(tree)
```

=== 8. Random Subsets in scikit-learn
* scikit-learn으로 부분집합 선택 과정 반복 가능
* DecisionTreeClassifier 에서 splitter 를 "random", max_features 를 "auto"로 하면 이전 코드와 동일
```
# We'll build 10 trees
tree_count = 10

# Each "bag" will have 60% of the number of original rows
bag_proportion = .6

predictions = []
for i in range(tree_count):
    # We select 60% of the rows from train, sampling with replacement
    # We set a random state to ensure we'll be able to replicate our results
    # We set it to i instead of a fixed value so we don't get the same sample every time
    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)

    # Fit a decision tree model to the "bag"
    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2, splitter="random", max_features="auto")
    clf.fit(bag[columns], bag["high_income"])

    # Using the model, make predictions on the test data
    predictions.append(clf.predict_proba(test[columns])[:,1])

combined = numpy.sum(predictions, axis=0) / 10
rounded = numpy.round(combined)

print(roc_auc_score(test["high_income"], rounded))
```

=== 9. Practice Putting it All Together
* 하나만 사용했을 때보다 더 향상됨
[width="40%",frame="topbot",options="header,footer"]
|======================
|settings	|test AUC
|min_samples_leaf: 2	|0.688
|max_depth: 2	|0.676
|combined predictions	|0.715
|min_samples_leaf: 2, with bagging	|0.732
|min_samples_leaf: 2, with bagging and random subsets	|0.735
|======================
* RandomForestClassifier 클래스와 RandomForestRegressor 클래스가있어서 랜덤 포리스트 모델을 쉽게 만듦
```
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=5, random_state=1, min_samples_leaf=2)
clf.fit(train[columns], train["high_income"])

predictions = clf.predict(test[columns])
print(roc_auc_score(test["high_income"], predictions)) # 0.734
```

=== 10. Tweaking Parameters to Increase Accuracy
* 의사결정 트리와 같이 파라미터 조절 가능
* 전체 구조를 변경하는 랜덤 포리스트의 특정한 파라미터가 있음
** n_estimators: Bootstrap aggregation (다른말론 bagging), 기본값은 true
** 가장 쉬운 조정은 사용할 측정의 수 늘리기
*** 10에서 100으로 늘리는게 100에서 500 늘리는것보다 더 큰 변화를 가져옴 (보통 200을 넘는 나무는 도움안됨)
```
from sklearn.ensemble import RandomForestClassifier

# n_estimators를 150으로 늘림 (기존에는 5)
clf = RandomForestClassifier(n_estimators=150, random_state=1, min_samples_leaf=2)

clf.fit(train[columns], train["high_income"])

predictions = clf.predict(test[columns])
print(roc_auc_score(test["high_income"], predictions)) #  0.738
```

=== 11. Reducing Overfitting
* 정확도를  0.739 로 향상했지만.. 150 나무를 사용하여 휠씬 오래 걸림 (이 시간을 절약해 다른 작업을 할 수도 있음)
* 단일 결정 트리에 비해 랜덤 포리스트의 가장 큰 장점은 오버핏이 작은 경향
** 데이터에 덜 민감함, 하나의 트리는 부정확하고 과도한 모델을 만들 수 있지만 100개가 넘어가면 노이즈를 무시하게 되므로.
** 평균값이 노이즈를 무시하고 신호를 유지함

* 단일 의사 결정 트리에서 훈련은 0.819, 테스트는 0.714 -> 오퍼빗
* 랜덤 포레스트를 사용하고나서의 값을 확인 -> 훈련과 테스트 두 값의 차이가 거의 없음

```
clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=5)

clf.fit(train[columns], train["high_income"])

predictions = clf.predict(train[columns])
print(roc_auc_score(train["high_income"], predictions)) # 0.819

predictions = clf.predict(test[columns])
print(roc_auc_score(test["high_income"], predictions)) # 0.713

clf = RandomForestClassifier(n_estimators=150, random_state=1, min_samples_leaf=5)

clf.fit(train[columns], train["high_income"])

predictions = clf.predict(train[columns])
print(roc_auc_score(train["high_income"], predictions)) # 0.791

predictions = clf.predict(test[columns])
print(roc_auc_score(test["high_income"], predictions)) # 0.749
```

=== 12. When to Use Random Forests
* 랜덤 포리스트로 오버핏이 감소하고 정확도도 높아짐
** 매우 강력하지만 랜덤 포리스트는 모든 작업에 적용 못함
** 장점: 매우 정확한 예측, 오버피팅이 적음 (그래도 max_depth 같은 변수 넣어야 함)
** 약점: 해석하기 어려움. (무작위 이기때문에 왜 이런 예측인지 파악이 어려울 수 있음), 나무 만드는데 오래 걸림 (다행이 멀티코어로 병렬수행 가능)
