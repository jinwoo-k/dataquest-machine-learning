## Multivariate K-Nearest Neighbors
### step.1 개요

아까는 k-nearest neighbors ML model을 쉽게 feature (attribute)한 개로만 함

정확성 늘리기 위해 시도하는 2가지 방법 (유효성 검증하면서 RMSE 줄이기)

. attribute 늘리기 : ranking에 가까운 이웃의 attribute 수
. k 늘리기 : 예측 계산시 사용하는 가까운 이웃 수

이번에는 attribute를 늘림. 단, 아래가 포함된 컬럼은 빼야함

. 숫자 아닌 것 (예: 도시)
. 없는 값
. 일반적인 순서가진 값이 아닌 것 (예: 위도/경도 같이 한 쌍이여야 의미, zipcode)

### step2. Removing Features

.숫자 아닌 것 (룸타입, 도시 등)
.순서값 아닌 것 (위도/경도 등)
.ML 목적에 안맞는 값 (주거에 관계된 목적일 경우 host 에 관한 값 삭제)

### step3. 빈 값 처리
컬럼에 빈 값 적으면 그냥 해당 row 삭제, 빈값 많으면 그 컬럼 자체 삭제

### step4. Normalize Columns
각 컬럼들을 조합해서 분석한다면 큰 범위 컬럼에 영향을 많이 받게 됨 (예: 1~12 범위 컬럼 vs 4~ 1825 범위 컬럼)

따라서 normalize 해서 0~1의 분포를 갖게 함

latexmath:[$x=\frac{x-\mu}{\sigma}$]

asciimath:[`x=\frac{x-\mu}{\sigma}`]

normalized_listings = (dc_listings - dc_listings.mean()) / (dc_listings.std())

### step5. Euclidean Distance For Multivariate Case

asciimath:[`d = \sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \cdots + (q_n-p_n)^2}`]

2개로 유클리안거리 계산 방법
```
from scipy.spatial import distance
first_listing = [-0.596544, -0.439151]
second_listing = [-0.596544, 0.412923]
dist = distance.euclidean(first_listing, second_listing)
```

### step6. Introduction To Scikit-Learn
Scikit-Learn: 파이썬의 유명 머신러닝 라이브러리, simple/unified workflow

. 사용할 머신러닝 모델을 인스턴스화
. 트레이닝 데이터를 맞춤
. 모델 사용하여 예측
. 예측의 정확도 계산

image:http://scikit-learn.org/stable/_static/ml_map.png[width="1000px"]

KNeighborsRegressor class를 사용할 예정 (회귀모델)

regression model (회귀모델): 수치를 예측하는데 도움되는 모든 모델

classification (분류): 기계학습의 주요 다른 클래스이며, 고정된 라벨 셋트에서 라벨을 예측 (예: 혈액형, 성별 등)

[source,python]
----
from sklearn.neighbors import KNeighborsRegressor
knn = KNeighborsRegressor()
#knn = KNeighborsRegressor(algorithm='brute')
----

### step.7 Fitting A Model And Making Predictions
"fit(matrix, list)" 를 사용하여 모델을 맞출 수 있음.

. matrix : training set으로 부터 사용. 특징 컬럼을 포함
. list : 옳은 목표 값 포함한 객체

[source,python]
----
# Split full dataset into train and test sets.
train_df = normalized_listings.iloc[0:2792]
test_df = normalized_listings.iloc[2792:]
# Matrix-like object, containing just the 2 columns of interest from training set.
train_features = train_df['accommodates', 'bathrooms']
# List-like object, containing just the target column, `price`.
train_target = normalized_listings['price']
# Pass everything into the fit method.
knn.fit(train_features, train_target)

predictions = knn.predict(test_df[train_columns])
----

ref: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor

### step.8 Calculating MSE Using Scikit-Learn

mean_squared_error()

. 실제 값
. 예측 값

[source,python]
----
from sklearn.metrics import mean_squared_error
two_features_mse = mean_squared_error(test_df['price'],predictions)
----
ref: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error

### step.9 Using More Features
여러 항목 한번에 knn (fit, predicate) + mse,rmse 가능

[source,python]
----
features = ['accommodates', 'bedrooms', 'bathrooms', 'number_of_reviews']
knn.fit(train_df[features], train_df['price'])
four_predictions = knn.predict(test_df[features])

four_mse = mean_squared_error(test_df['price'],four_predictions)
----

### step.10 Using All Features
마찬가지 방법으로 전체 feature로 계산 가능

[source,python]
----
features = train_df.columns.tolist()
features.remove('price')
----

### step.11 Next Steps

모든 걸 다사용한다고 좋은게 아니다. (온갖걸 다 사용한다고 자동으로 정확도 향상 안됨)




## 참고 & 추천자료
scikit-learn 강좌: https://www.datacamp.com/community/tutorials/machine-learning-python#gs.XbS_Ts0

scikit-learn 사진 출처: http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
