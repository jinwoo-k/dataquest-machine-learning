= Feature Selection

== 1. Mission Values
* 이 미션에서는 속성과 대상 열간의 상관관계, 속성간 상관관계 및 속성을 선택하는 속성의 다양성을 사용하는 방법에 대해 알아본다.
* 누락된 값이 없거나 유효한 값으로 변환이 필요없는 속성들 중에 선택

== 2. Correlating Feature Columns With Target Column
* 일부 속성이 대상 열과 어떻게 상관되는지를 비교하면서 단순선형회귀 모델의 속성을 선택
* `pandas.DataFrame.corr()`를 이용해 상관계수를 반환

== 3. Correlation Matix Heatmap
* 상관관계가 0.3 이상인 속성만 유지
* 비슷한 속성은 하나로 축약

image::./images/2-1.PNG[상관관계]

* 상관행렬 히트맵을 생성하려면 상관행렬을 포함하는 데이터 프레임을 `seaborn.heatmap()`함수에 전달

== 4. Train And Test Model
* 상관행렬 히트맵을 기반으로 다음 열의 상관관계를 알아낼 수 있다.
** `Gr Liv Area` and `TotRms AbvGrd`
** `Garage Area` and `Garage Cars`

* 테스트 셋에 사용할 열 중 누락된 값이 있을 경우 제거

== 5. Removing Low Variance Features
* 분산이 낮은 속성을 제거
** 분산이 낮으면 예측에 의미있는 기여를 하지 않음


== 6. Final Model
* 0.015 컷오프 분선을 설정하고 `Open Porch SF` 속성을 제거하고 모델을 교육하고 테스트


== 예제 모음

=== 1.

[source,python]
----
import pandas as pd
data = pd.read_csv('AmesHousing.txt', delimiter="\t")
train = data[0:1460]
test = data[1460:]
numerical_train = train.select_dtypes(include=['int', 'float'])
numerical_train = numerical_train.drop(['PID', 'Year Built', 'Year Remod/Add', 'Garage Yr Blt', 'Mo Sold', 'Yr Sold'], axis=1)
null_series = numerical_train.isnull().sum()
full_cols_series = null_series[null_series == 0]
print(full_cols_series)
----

=== 2.

[source,python]
----
train_subset = train[full_cols_series.index]
corrmat = train_subset.corr()
sorted_corrs = corrmat['SalePrice'].abs().sort_values()
print(sorted_corrs)
----

=== 3.

[source,python]
----
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
strong_corrs = sorted_corrs[sorted_corrs > 0.3]
corrmat = train_subset[strong_corrs.index].corr()
sns.heatmap(corrmat)
----

image::./images/2-2.PNG[결과]

=== 4.

[source,python]
----
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np

final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])
features = final_corr_cols.drop(['SalePrice']).index
target = 'SalePrice'
clean_test = test[final_corr_cols.index].dropna()

lr = LinearRegression()
lr.fit(train[features], train['SalePrice'])

train_predictions = lr.predict(train[features])
test_predictions = lr.predict(clean_test[features])

train_mse = mean_squared_error(train_predictions, train[target])
test_mse = mean_squared_error(test_predictions, clean_test[target])

train_rmse = np.sqrt(train_mse)
test_rmse = np.sqrt(test_mse)

print(train_rmse)
print(test_rmse)
----

=== 5.

[source,python]
----
unit_train = train[features]/(train[features].max())
sorted_vars = unit_train.var().sort_values()
print(sorted_vars)
----


=== 6.

[source,python]
----
features = features.drop(['Open Porch SF'])

clean_test = test[final_corr_cols.index].dropna()

lr = LinearRegression()
lr.fit(train[features], train['SalePrice'])

train_predictions = lr.predict(train[features])
test_predictions = lr.predict(clean_test[features])

train_mse = mean_squared_error(train_predictions, train[target])
test_mse = mean_squared_error(test_predictions, clean_test[target])

train_rmse_2 = np.sqrt(train_mse)
test_rmse_2 = np.sqrt(test_mse)

print(train_rmse_2)
print(test_rmse_2)
----
