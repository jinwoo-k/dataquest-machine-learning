= Introduction to neural networks

== 1. Neural networks and iris flowers
* 많은 머신러닝 예측 문제는 복잡한 데이터와 피쳐간의 비선형 관계에 기인한다.
* 신경 회로망은 변수들 사이의 이러한 비선형 상호작용을 학습할 수 있는 일련의 모델
* 다음과 같은 특징을 가진 홍채 꽃의 종을 예측하는 신경망을 소개

[source,python]
----
import pandas
import matplotlib.pyplot as plt
import numpy as np

# Read in dataset
iris = pandas.read_csv("iris.csv")

# shuffle rows
shuffled_rows = np.random.permutation(iris.index)
iris = iris.loc[shuffled_rows,:]

print(iris.head())

# There are 2 species
print(iris.species.unique())
iris.hist()
plt.show()
----

image::./images/9-1.PNG[plot]

== 2. Neurons
* 지금까지 우리는 많은 양의 비선형성을 허용하지 않는 방법에 대해 얘기했다.
** 예를 들어 아래에 표시된 2차원의 경우, X를 O에서 완전히 분리할 수 있는 함수를 찾고자 한다.

image::./images/9-2.PNG[그래프]

* 선형모델이나 로지스틱 모델은 이런 기능을 충족할 수 없으므로 신경망과 같은 다른 옵션을 찾아야한다.
* 신경망 모델은 뉴런으로 알려진 일련의 활성화 단위를 사용해 결과를 예측한다.
* 아래는 뉴런을 표현한 그림이다.

image::./images/9-3.PNG[뉴런]

* 이 뉴런은 5개의 x, 바이어스 유닛, 4개의 피쳐로 구성되어 있다.
* 바이어스 유닛(1)은 선형 회귀 분석에서 절편과 개념이 비슷하며 뉴런의 활동을 한방향 또는 다른 방향으로 이동시킨다.
* 그 후 활성화 함수 h에 공급된다.
* 0에서 1사이의 값을 반환하고 확률로 취급될 수 있으므로 로지스틱 활성화 함수를 사용한다.

image::./images/9-4.PNG[시그모이드함수]

image::./images/9-5.PNG[시그모이드활성화함수]

* 자세히 살펴보면 이전 강의에서 배운 로지스틱 회귀함수를 뉴런으로 나타낼수 있다.

[source,python]
----
z = np.asarray([[9, 5, 4]])
y = np.asarray([[-1, 2, 4]])

# np.dot is used for matrix multiplication
# z is 1x3 and y is 1x3,  z * y.T is then 1x1
print(np.dot(z,y.T))

# Variables to test sigmoid_activation
iris["ones"] = np.ones(iris.shape[0])
X = iris[['ones', 'sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values
y = (iris.species == 'Iris-versicolor').values.astype(int)

# The first observation
x0 = X[0]

# Initialize thetas randomly
theta_init = np.random.normal(0,0.01,size=(5,1))
def sigmoid_activation(x, theta):
    x = np.asarray(x)
    theta = np.asarray(theta)
    return 1 / (1 + np.exp(-np.dot(theta.T, x)))

a1 = sigmoid_activation(x0, theta_init)
----

== 3. Cost function
* 그라디언트 디센트를 사용해 단일 뉴런을 2계층 네트워크로 교육할 수 있다.
* 모델의 오류를 측정하는 비용 함수를 최소화 해야한다.
* 비용함수 : 원하는 출력과 실제 출력 간의 차이를 측정

image::./images/9-6.PNG[비용함수]

* 우리의 목표는 yi의 바이너리값이므로 yi 또는 (1-yi)는 0과 같다고 할수 있다.
* 활성화 함수를 사용해 오류를 계산한다.
* 예를 들어 진정한 목표인 yi = 1을 관찰하면 hθ(xi)도 1에 가까워지길 원한다.
* hθ(xi)이 1에 가까워지면 로그( hθ(xi) )은 0에 매우 가깝게 된다.
* 0과 1사이의 값의 로그가 음수이므로 비용을 계산하기 위해 전체 합계의 음수를 취해야한다.
* 매개변수는 분산이 0.1 미만인 정상 임의 변수를 사용해 무작위로 초기화 된다.

[source,python]
----
# First observation's features and target
x0 = X[0]
y0 = y[0]

# Initialize parameters, we have 5 units and just 1 layer
theta_init = np.random.normal(0,0.01,size=(5,1))
def singlecost(X, y, theta):
    # Compute activation
    h = sigmoid_activation(X.T, theta)
    # Take the negative average of target*log(activation) + (1-target) * log(1-activation)
    cost = -np.mean(y * np.log(h) + (1-y) * np.log(1-h))
    return cost

first_cost = singlecost(x0, y0, theta_init)
----

== 4. Compute the Gradients
* 이전 임무에서 그라디언트를 얻기 위해 비용 함수의 편미분을 계산한다는 것을 배웠다.
* 도함수의 계산은 선형 회귀에서보다 신경 네트워크에서 더 복잡하다.
* 체인규칙을 사용해 미분을 계산해라

image::./images/9-8.PNG[도함수]

* 첫번째 부분은 목표변수와 예측간의 오차를 계산하는것
* 두번째 부분은 각 매개변수와 관련된 감도를 계산한다.
* 그라디언트는 다음과 같이 계산된다.

image::./images/9-9.PNG[그라디언트]

* (yi - hθ(xi))는 스칼라 값이고 목표값과 예측 사이의 오차를 뜻함
* hθ(xi) * (1-hθ(xi))도 스칼라이고 활성화 함수의 민감도이다.
* xi는 우리의 관찰을 위한 특징이다. ∂는 길이 5의 벡터고 4개의 특징과 함께 기울기에 해당하는 바이어스 단위
* 이를 구현하기 위해 각 관측치에 대해 ∂를 계산한 다음 평균을 구한다. 그 후 평균 그라디언트를 사용해 해당 매개변수를 업데이트 한다.

[source,python]
----
# Initialize parameters
theta_init = np.random.normal(0,0.01,size=(5,1))

# Store the updates into this array
grads = np.zeros(theta_init.shape)

# Number of observations
n = X.shape[0]
for j, obs in enumerate(X):
    # Compute activation
    h = sigmoid_activation(obs, theta_init)
    # Get delta
    delta = (y[j]-h) * h * (1-h) * obs
    # accumulate
    grads += delta[:,np.newaxis]/X.shape[0]
----

== 5. Two Layer network
* 그라디언트 디센트를 통해 4가지 기능이 주어진 홍채 꽃의 종을 예측할 수 있다.
* 그라디언트 디센트는 매개변수를 조정하여 비용함수를 최소화 한다.
* 그라데이션의 곱과 이전 매개변수의 학습률을 더해서 매개변수를 조정하라

[source,python]
----
theta_init = np.random.normal(0,0.01,size=(5,1))

# set a learning rate
learning_rate = 0.1
# maximum number of iterations for gradient descent
maxepochs = 10000
# costs convergence threshold, ie. (prevcost - cost) > convergence_thres
convergence_thres = 0.0001

def learn(X, y, theta, learning_rate, maxepochs, convergence_thres):
    costs = []
    cost = singlecost(X, y, theta)  # compute initial cost
----

== 6. Neural Network
* 신경회로망은 일반적으로 다중 층의 뉴런을 사용해 제작된다.
* 네트워크에 레이어를 추가하면 더 복잡한 기능을 배울수 있다.

image::./images/9-10.PNG[3층신경망]

* 우리는 4개의 입력 변수 x1, x2, x3, x4와 바이어스 유닛을 가진 3 레이어 신경망을 가지고 있다.
* 각 변수 및 바이어스 단위는 다음 4개의 숨겨진 단위 a1, a2, a3, a4
* 숨겨진 유닛은 매개변수 θ 의 다른 셋을 갖는다.

image::./images/9-11.PNG[단위]

* 레이어 j 내의 유닛을 활성화 유닛 ai(j+1) 로 변환하는 입력 유닛 K의 파라미터

image::./images/9-12.PNG[파라미터]

* 이 계층은 사용자가 데이터를 전달하거나 검색해 상호작용 하지 않기 때문에 숨겨진 계층이라고 한다.
* 세번째이자 마지막 계층은 산출물 or 예측결과 이다.

image::./images/9-13.PNG[예측결과]

[source,python]
----
theta0_init = np.random.normal(0,0.01,size=(5,4))
theta1_init = np.random.normal(0,0.01,size=(5,1))
def feedforward(X, theta0, theta1):
    # feedforward to the first layer
    a1 = sigmoid_activation(X.T, theta0).T
    # add a column of ones for bias term
    a1 = np.column_stack([np.ones(a1.shape[0]), a1])
    # activation units are then inputted to the output layer
    out = sigmoid_activation(a1.T, theta1)
    return out

h = feedforward(X, theta0_init, theta1_init)
----

== 7. Multiple neural network cost function
* 다중 계층 신경망에 대한 비용함수

image::./images/9-14.PNG[비용함수]

[source,python]
----
theta0_init = np.random.normal(0,0.01,size=(5,4))
theta1_init = np.random.normal(0,0.01,size=(5,1))


# X and y are in memory and should be used as inputs to multiplecost()
def multiplecost(X, y, theta0, theta1):
    # feed through network
    h = feedforward(X, theta0, theta1)
    # compute error
    inner = y * np.log(h) + (1-y) * np.log(1-h)
    # negative of average error
    return -np.mean(inner)

c = multiplecost(X, y, theta0_init, theta1_init)
----

== 8. Backpropagation
* 여러계층의 매개변수가 있으므로 backpropagation 메서드를 구현해야한다.
* 우리는 각 레이어를 통해 데이터를 공급하고 출력을 반환함으로써 순방향 전파를 구현했다.
* 역방향 전파는 마지막 레이어에서 시작하여 각 레이어를 순환하는 매개변수를 업데이트 하는데 초점을 둔다.
* 여기서 l는 층이다. 3계층 네트워트는 아래와 같은 방법을 사용해라

image::./images/9-15.PNG[3계층네트워크]

[source,python]
----
class NNet3:
    def __init__(self, learning_rate=0.5, maxepochs=1e4, convergence_thres=1e-5, hidden_layer=4):
        self.learning_rate = learning_rate
        self.maxepochs = int(maxepochs)
        self.convergence_thres = 1e-5
        self.hidden_layer = int(hidden_layer)

    def _multiplecost(self, X, y):
        # feed through network
        l1, l2 = self._feedforward(X)
        # compute error
        inner = y * np.log(l2) + (1-y) * np.log(1-l2)
        # negative of average error
        return -np.mean(inner)

    def _feedforward(self, X):
        # feedforward to the first layer
        l1 = sigmoid_activation(X.T, self.theta0).T
        # add a column of ones for bias term
        l1 = np.column_stack([np.ones(l1.shape[0]), l1])
        # activation units are then inputted to the output layer
        l2 = sigmoid_activation(l1.T, self.theta1)
        return l1, l2

    def predict(self, X):
        _, y = self._feedforward(X)
        return y

    def learn(self, X, y):
        nobs, ncols = X.shape
        self.theta0 = np.random.normal(0,0.01,size=(ncols,self.hidden_layer))
        self.theta1 = np.random.normal(0,0.01,size=(self.hidden_layer+1,1))

        self.costs = []
        cost = self._multiplecost(X, y)
        self.costs.append(cost)
        costprev = cost + self.convergence_thres+1  # set an inital costprev to past while loop
        counter = 0  # intialize a counter

        # Loop through until convergence
        for counter in range(self.maxepochs):
            # feedforward through network
            l1, l2 = self._feedforward(X)

            # Start Backpropagation
            # Compute gradients
            l2_delta = (y-l2) * l2 * (1-l2)
            l1_delta = l2_delta.T.dot(self.theta1.T) * l1 * (1-l1)

            # Update parameters by averaging gradients and multiplying by the learning rate
            self.theta1 += l1.T.dot(l2_delta.T) / nobs * self.learning_rate
            self.theta0 += X.T.dot(l1_delta)[:,1:] / nobs * self.learning_rate

            # Store costs and check for convergence
            counter += 1  # Count
            costprev = cost  # Store prev cost
            cost = self._multiplecost(X, y)  # get next cost
            self.costs.append(cost)
            if np.abs(costprev-cost) < self.convergence_thres and counter > 500:
                break

# Set a learning rate
learning_rate = 0.5
# Maximum number of iterations for gradient descent
maxepochs = 10000
# Costs convergence threshold, ie. (prevcost - cost) > convergence_thres
convergence_thres = 0.00001
# Number of hidden units
hidden_units = 4

# Initialize model
model = NNet3(learning_rate=learning_rate, maxepochs=maxepochs,
              convergence_thres=convergence_thres, hidden_layer=hidden_units)
# Train model
model.learn(X, y)

# Plot costs
plt.plot(model.costs)
plt.title("Convergence of the Cost Function")
plt.ylabel("J($\Theta$)")
plt.xlabel("Iteration")
plt.show()
----

image::./images/9-16.PNG[결과]

== 9. Splitting data
* 신경망과 역전파에 대해 배웠으므로 모델을 실행한다

[source,python]
----
# Last 30 rows to X_test and y_test
X_train = X[:70]
y_train = y[:70]

X_test = X[-30:]
y_test = y[-30:]
----

== 10. Predicting iris flowers

[source,python]
----
from sklearn.metrics import roc_auc_score
# Set a learning rate
learning_rate = 0.5
# Maximum number of iterations for gradient descent
maxepochs = 10000
# Costs convergence threshold, ie. (prevcost - cost) > convergence_thres
convergence_thres = 0.00001
# Number of hidden units
hidden_units = 4

# Initialize model
model = NNet3(learning_rate=learning_rate, maxepochs=maxepochs,
              convergence_thres=convergence_thres, hidden_layer=hidden_units)
model.learn(X_train, y_train)

yhat = model.predict(X_test)[0]

auc = roc_auc_score(y_test, yhat)
----
