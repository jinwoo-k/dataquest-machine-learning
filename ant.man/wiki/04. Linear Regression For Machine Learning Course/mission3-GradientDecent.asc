== Gradient Decent
=== 1. Introduction

* model fitting : 선형회귀에서 최적의 파라미터 찾는 과정 (앞으로 두 장에서 할 예정)
image:./images/mse.png[]

* "Gr Liv Area" 한 컬럼을 예제로 y (예측값) 을 구하는 것을 해보는 예제

=== 2. Single Variable Gradient Descent
* 최적화 함수가 최소값을 갖는 곡선 -> 미적분에서 상대적 최소값 => 도함수가 0 인 x 값
** 이 방법은 다른 매개변수값이 증가할 수 있어서 여러 매개변수일 경우 안됨!!
** 1개 파라미터일때 MSE 곡선은 그리고 최소점을 찾을 수 있어도, 여러 파라미터일 떈 못함

* gradient descent: 반복적인 기술, 최소 MSE가 젤 낮을때까지 다른 파라미터로 반복 시도함.
** 신경망과 같은 다른 모델에도 일반적으로 사용되는 최적화 기술
** 알고리즘 개요
.. 매개변수 초기값 선택
.. 수렴될떄 까지 아래 반복 (일반적으로 최대 반복 횟수로 구현)
... 현재 매개변수로 MSE 계산, image:./images/mse_a1.png[height=32]
... 현재 매개변수에서 MSE의 미분을 계산, image:./images/decent_mse.png[]
... 미분x상수를 기존 파라미터에서 뺀 값으로 파라미터를 업데이트 (미분에 곱하는 상수{알파}를 learning rate라 함.), image:./images/pram_sub_decent_mse.png[]

** 적잘한 초기 매개 변수 및 학습률 (learning rate)를 선택하면 반복횟수가 줄어듦 (이걸 hyperparameter 최적화라함.)

추가: http://blog.naver.com/PostView.nhn?blogId=tjdudwo93&logNo=221067763334[parameter vs hyperparameter]

=== 3. Derivative Of The Cost Function
* cost function / loss function : 수학적 최적화에서 최적화하는 함수
** image:./images/mse_replaced.png[] ( 하나의 매개 변수 모델에 맞추기 때문에 변형 image:./images/y_replace_ax.png[] )

* 미적분 속성을 적용 예정 (수학 이해를 통해 추후 간혹 발생하는 문제를 쉽게 디버깅하자.)
image:./images/calcul_decent_mse.png[]

```
def derivative(a1, xi_list, yi_list):
    numOfSum = 0
    for i in range(0, len(xi_list)):
        numOfSum += xi_list[i]*(a1*xi_list[i] - yi_list[i])
    return 2/len(xi_list) * numOfSum

def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial):
    a1_list = [a1_initial]

    for i in range(0, max_iterations):
        a1 = a1_list[i]
        deriv = derivative(a1, xi_list, yi_list)
        a1_new = a1 - alpha*deriv
        a1_list.append(a1_new)
    return(a1_list)

param_iterations = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150)
```

=== 4. Understanding Multi Parameter Gradient Descent
* 여러 파라미터로 gradient descent 확인 (a1, a0 를 움직이면 그 방향으로만 움직임)

=== 5. Gradient Of The Cost Function
* 이제 두개의 파라미터로 MSE 최소구하기 : 두 개 모두 업데이트하는 방식
** 이전에 계산한 MSE 미분값을 확장하여 두 파라미터 업데이트 방식은 아래와 같다.
image:./images/parameter2_mse.png[]

* MSE 계산을 정리하면
image:./images/parameter2_mse_decent.png[]
image:./images/parameter2_mse_decent2.png[]

<정답이라 예상(?) 되는 코드>

```
def a1_derivative(a0, a1, xi_list, yi_list):
    len_data = len(xi_list)
    error = 0
    for i in range(0, len_data):
        error += xi_list[i]*(a0 + a1*xi_list[i] - yi_list[i])
    deriv = 2*error/len_data
    return deriv

def a0_derivative(a0, a1, xi_list, yi_list):
    len_data = len(xi_list)
    error = 0
    for i in range(0, len_data):
        error += (a0 + a1*xi_list[i] - yi_list[i])
    deriv = 2*error/len_data
    return deriv

def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial, a0_initial):
    a1_list = [a1_initial]
    a0_list = [a0_initial]

    for i in range(0, max_iterations):
        a1 = a1_list[i]
        a0 = a0_list[i]

        a1_deriv = a1_derivative(a0, a1, xi_list, yi_list)
        a0_deriv = a0_derivative(a0, a1, xi_list, yi_list)

        a1_new = a1 - alpha*a1_deriv
        a0_new = a0 - alpha*a0_deriv

        a1_list.append(a1_new)
        a0_list.append(a0_new)
    return(a0_list, a1_list)

a0_params, a1_params = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 100, .0000003, 150, 1000)
```

=== 6. Gradient Descent For Higher Dimensions
* 더 많은 파라미터는 비슷하게 계속 추가하면 됨
image:./images/multi_param_update_rule.png[]

* MSE의 미분 값외의 다름 매개변수의 미분값도 비슷.
image:./images/multi_decent.png[]

=== 7. Next Steps

* 이번에 선현회귀는 gradient descent 를 사용, gradient descent 에서 주요 문제
** 초기값
** learning rate

* 다음엔 저 두 파라미터를 최적화하는 hyperparameter 가 필요없는 OSL 배움
