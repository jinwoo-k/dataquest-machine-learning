= Introduction to Random Forests

== 1. Introduction
* Decision tree의 오버피팅을 줄이는 가장 강력한 도구 중 랜덤 포레스트 알고리즘이 있다.
* 본 미션에서는 랜덤 포레스트를 만들고 적용하는 법을 배운다.
* 데이터는 이전 미션에서 사용했던 1994년 미국 가계수입 데이터를 이용한다.

== 2. Combining Model Predictions With Ensembles
* 랜덤 포리스트는 일종의 앙상블 모델이다.
* 앙상블이란 여러 모델의 예측을 결합하여 더욱 정확한 최종 예측을 만든다.
* 어떻게 작동하는지 이해하기 위해 간단한 앙상블을 만든다.
  ** 파라미터가 약간 다른 두개의 Decision tree를 만든다.
    *** min_samples_leaf set to 2
    *** max_depth set to 5
  ** 각각의 Decision tree의 정확도를 확인한다.
  ** 다음 스탭에서 예측을 결합하고 결합 된 정확도를 개별 정확도와 비교

[source,python]
----
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score

columns = ["age", "workclass", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "hours_per_week", "native_country"]

clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)
clf.fit(train[columns], train["high_income"])
predictions = clf.predict(test[columns])
auc = roc_auc_score(test["high_income"], predictions)

clf2 = DecisionTreeClassifier(random_state=1, max_depth=5)
clf2.fit(train[columns], train["high_income"])
predictions2 = clf2.predict(test[columns])
auc2 = roc_auc_score(test["high_income"], predictions2)

print(auc)
print(auc2)
# auc  : 0.687896422606
# auc2 : 0.675985390651
----

== 3. Combining Our Predictions
* 여러개의 Decision tree 모델을 만든 후 각 모델이 예측한 예측치를 나열하면 다음과 같이 매트릭스 형태로 나타낼 수 있다.
  ** image:./images/m4_3_1.png[, 200]
* 한개의 예측치는 한개의 결과물로 표현해야한다. (Final Prediction)
  ** image:./images/m4_3_2.png[, 400]
* 여러 모델의 산출물로부터 최종 예측을 구하는 방법 (결과가 0 또느 1인 범주형 모델에서 사용하는 방법)
  ** 과반수 투표(majority voting) - 세개 이상의 모델이 있는 경우 동작, 이상적으로는 홀수여야 함 (동점 처리 불필요)
  ** 평균값
    *** 각 모델별 0 또는 1에 대한 확률(predict_proba 메서드 이용)을 구함
      **** image:./images/m4_3_3.png[, 130]
    *** 그 중 1에 대한 확률을 더한 후 앙상블 갯수로 나눈다.
    *** 결과를 반올림하여 0 또는 1로 예측한다.
  ** 최고 확률
    *** 모든 모델에서 단일 최고 확률을 획득하는 값을 사용

== 4. Combining Our Predictions
* 평균 확률을 이용한 예측 실습

[source,python]
----
predictions = clf.predict_proba(test[columns])[:,1]
predictions2 = clf2.predict_proba(test[columns])[:,1]

result = numpy.round((predictions + predictions2) / 2)
auc = roc_auc_score(test["high_income"], result)
print(auc)
----

== 5. Why Ensembling Works
* image:./images/m4_5_1.png[, 300]
* (경험이 다른 두 개발자가 동일한 프로젝트를 진행하고 각각 나은 부분을 결합하면 더 좋은 프로그램이 나온다라는 예를 듬...)
* 앙상블은 모델에 각각 다른 매개 변수를 사용했기 때문에 약간 다른 방식으로 동일한 문제에 접근한다.
* 파라미터가 다른 여러개의 의사결정나무가 거의 비슷한 정확도를 갖는 경우, 이를 결합하면 더 강력한 예측이 가능해진다.
* 앙상블을 구성하는 모델의 파라미터가 많이 변경되거나, 모델이 다양해 질 수록 더 강력해진다.
  ** 의사결정나무와 로지스틱 회귀 모델을 통합하면 예측을 위한 접근이 많이 다르기 때문에 비슷한 매개 변수를 가진 두 개의 의사결정나무를 앙상블하는 것보다 더 강력하다.
  ** 반면 파라미터가 비슷한 의사결정나무를 앙상블하면 개선효과가 크지 않다.
* 정확도가 많이 다른 모델을 앙상블하면 개선효과가 크지 않다. (일반적으로 두 정확도 사이의 값이 나온다.)
  ** 가중치를 통해 이를 해결가능하며, 나중에 다룬다.

== 6. Introducing Variation With Bagging
* 랜덤 포레스트는 의사결정나무의 앙상블
* 무작위성을 이용해 트리를 변형시키는 방법 으로는 bagging 과 random feature subsets 이 있다.
* bagging
  ** 복원추출을 이용해 데이터의 무작위 일부 (bag) 을 이용해 모델을 학습 (동일한 데이터가 여러 bag 에 나타날 수 있음)
* random feature subsets
  ** 노드에서 분기시 피쳐들 중 일부만 이용해 IG가 높은 피쳐 선출

[source,python]
----
tree_count = 10

bag_proportion = .6

predictions = []
for i in range(tree_count):
    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)

    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)
    clf.fit(bag[columns], bag["high_income"])

    predictions.append(clf.predict_proba(test[columns])[:,1])

summation = numpy.sum(predictions, axis=0) / 10
result = numpy.round(summation)
auc = roc_auc_score(test["high_income"], result)
print(auc)
----

== 7. Selecting Random Features
* image:./images/m4_7_1.png[, 400]

* ID3 알고리즘에서의 트리 빌드 과정
  1. 먼저 트리를 분할 할 때마다 평가할 최대 피쳐 수 를 선택합니다.
    ** 이 값은 데이터의 총 컬럼수보다 작아야 한다.
  2. 분할 할 때마다 데이터에서 무작위로 샘플을 선택
  3. 임의의 표본에서 각 피쳐에 대한 information gain을 계산하고 가장 높은 IG를 갖는 피쳐를 선택하여 분할

* 엔트로피
  ** 데이터 셋의 무질서도의 척도
  ** 데이터 셋에 모두 동일한 값이 있는 경우 엔트로피가 낮다.
  ** 데이터 셋에 모두 다른 값이 있는 경우 엔트로피가 높다.
  ** 데이터에 대한 더 많은 정보를 제공하는 분할은 이상적으로 엔트로피를 최소화 한다.
    *** 트리는 이상적으로 가능한 한 적은 조합으로 값을 그룹으로 분리한다.

* 위에서 설명한 피쳐 셀렉션 과정을 랜덤으로 선택된 몇개의 피쳐만을 대상으로 해 강력한 앙상블을 만든다. -> 랜덤 피쳐 셀렉팅

[source,python]
----
# Create the data set that we used two missions ago
data = pandas.DataFrame([
    [0,4,20,0],
    [0,4,60,2],
    [0,5,40,1],
    [1,4,25,1],
    [1,5,35,2],
    [1,5,55,1]
    ])
data.columns = ["high_income", "employment", "age", "marital_status"]

# Set a random seed to make the results reproducible
numpy.random.seed(1)

# The dictionary to store our tree
tree = {}
nodes = []

# The function to find the column to split on
def find_best_column(data, target_name, columns):
    information_gains = []

    # Insert your code here
    random_columns = numpy.random.choice(columns, 2)

    for col in random_columns:
        information_gain = calc_information_gain(data, col, "high_income")
        information_gains.append(information_gain)

    # Find the name of the column with the highest gain
    highest_gain_index = information_gains.index(max(information_gains))
    highest_gain = columns[highest_gain_index]
    return highest_gain

# The function to construct an ID3 decision tree
def id3(data, target, columns, tree):
    unique_targets = pandas.unique(data[target])
    nodes.append(len(nodes) + 1)
    tree["number"] = nodes[-1]

    if len(unique_targets) == 1:
        if 0 in unique_targets:
            tree["label"] = 0
        elif 1 in unique_targets:
            tree["label"] = 1
        return

    best_column = find_best_column(data, target, columns)
    column_median = data[best_column].median()

    tree["column"] = best_column
    tree["median"] = column_median

    left_split = data[data[best_column] <= column_median]
    right_split = data[data[best_column] > column_median]
    split_dict = [["left", left_split], ["right", right_split]]

    for name, split in split_dict:
        tree[name] = {}
        return id3(split, target, columns, tree[name])


# Run the ID3 algorithm on our data set and print the resulting tree
id3(data, "high_income", ["employment", "age", "marital_status"], tree)
print(tree)
----


== 8. Random Subsets in scikit-learn
* DecisionTreeClassifier의 splitter 파라미터를 "random" 으로, max_features 파라미터를 "auto" 로 세팅하면 랜덤 피쳐가 선택된다.
* N 개의 열이있는 경우 루트 N개의 하위 집합을 임의로 선택하고 각각에 대한 지니 계수를 계산하며(정보 게인과 유사 함) 하위 집합의 최상의 열에서 노드를 분할합니다. (scikit-learn 이 해줌!)

[source,python]
----
# We'll build 10 trees
tree_count = 10

# Each "bag" will have 60% of the number of original rows
bag_proportion = .6

predictions = []
for i in range(tree_count):
    # We select 60% of the rows from train, sampling with replacement
    # We set a random state to ensure we'll be able to replicate our results
    # We set it to i instead of a fixed value so we don't get the same sample every time
    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)

    # Fit a decision tree model to the "bag"
    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2, splitter="random", max_features="auto")
    clf.fit(bag[columns], bag["high_income"])

    # Using the model, make predictions on the test data
    predictions.append(clf.predict_proba(test[columns])[:,1])

combined = numpy.sum(predictions, axis=0) / 10
rounded = numpy.round(combined)

print(roc_auc_score(test["high_income"], rounded))
----

== 9. Practice Putting it All Together
* image:./images/m4_9_1.png[, 500]
* 랜덤 포레스트를 이용하는 더욱 간편한 방법으로 scikit-learn 에서 제공하는 RandomForestClassifier / RandomForestRegressor 이용
* RandomForestClassifier 를 인스턴스화 할때 n_estimators 파라미터를 이용해 만들 트리 수 를 입력한다.

[source,python]
----
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=5, random_state=1, min_samples_leaf=2)
clf.fit(train[columns], train["high_income"])

predictions = clf.predict(test[columns])
print(roc_auc_score(test["high_income"], predictions))
----

== 10. Tweaking Parameters to Increase Accuracy
* Decision trees 와 마찬가지로 random forest 클래스도 아래의 옵션 제공 (각 개별적인 모델에 적용)
  ** min_samples_leaf
  ** min_samples_split
  ** max_depth
  ** max_leaf_nodes

* 아래의 파라미터는 랜덤 포레스트 전체에 영향을 줌
  ** n_estimators (일반적으로 200 이상은 더이상 효과를 갖지 못함)
  ** bootstrap - "Bootstrap aggregation" is another name for bagging; this parameter indicates whether to turn it on (Defaults to True)

[source,python]
----
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=150, random_state=1, min_samples_leaf=2)

clf.fit(train[columns], train["high_income"])

predictions = clf.predict(test[columns])
print(roc_auc_score(test["high_income"], predictions))
----

== 11. Reducing Overfitting
* estimator 를 조정해 AUC를 0.735에서 0.738로 향상시킬 수 있었지만 150개의 트리를 사용하는 모델은 훈련하는 데 훨씬 오래 걸린다.
  ** 큰 데이터를 다룬땐 문제가 된다.
* 랜덤 포레스트의 장점은 단일 의사결정나무에 비해 덜 오버핏 된다는 것이다. (앙상블을 통한 노이즈 절감)

[source,python]
----
clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=5)

clf.fit(train[columns], train["high_income"])

predictions = clf.predict(train[columns])
print(roc_auc_score(train["high_income"], predictions))

predictions = clf.predict(test[columns])
print(roc_auc_score(test["high_income"], predictions))

clf = RandomForestClassifier(n_estimators=150, random_state=1, min_samples_leaf=5)
clf.fit(train[columns], train["high_income"])
predictions = clf.predict(test[columns])
print(roc_auc_score(test["high_income"], predictions))
# 0.819257048953 (train)
# 0.713932589928 (test - singular decision tree)
# 0.749887434396 (test - random forest)
----

== 12. When to Use Random Forests
* 랜덤 포레스트의 장점
  ** 매우 정확한 예측 - 무작위 포리스트는 많은 기계 학습 작업에서 최첨단 성능을 발휘, 신경망과 gradient-boosted trees 와 함께 최고 성능 알고리즘 중 하나이다.
  ** 오버 피팅에 대한 저항성 - 앙상블을 통해 오버피팅에 저항이 있다. 그래도 max_depth와 같은 매개 변수를 설정하고 조정해야한다.
* 약점
  ** 해석하기가 어려움 - 많은 모델 결과를 평균했기 때문에 왜 그런지 예측을 내리는 이유를 파악하기가 어려울 수 있다.
  ** 모델 생성이 오래걸림 - n개의 트리를 만드는데는 n배의 시간이 걸린다. 하지만 Scikit을 사용하면 RandomForestClassifier의 n_jobs 파라미터를 통해 병렬 생성을 제공한다.

* 결론 : 시간은 좀 더 걸리더라도 정확도를 높이고자 하면 랜덤포레스트가 적절한 선택일 수 있다.
