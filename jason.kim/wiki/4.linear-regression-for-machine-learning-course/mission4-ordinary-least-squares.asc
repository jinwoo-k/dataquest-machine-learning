= Ordinary Least Squares
Learn how to fit a model using OLS.

== 1. Introduction
* 이전장에서는 반복적 모델 피팅 방법인 Gradient Descent 방식을 알아봤으며, 이는 초기값과 학습률에 따라 반복횟수가 크게 달라진다.
* OSL 추정법은 Gradient Descent 와 달리 최적의 파라미터 값을 직접 계산할 수 있는 수식을 제공한다.
* 일반적인 선형회귀 모델 : image:./images/m4_1_1.png[, 80]
* 파라미터 매트릭스 A를 얻는 방법 : image:./images/m4_1_2.png[, 150]
* 사이킷 런에서 LinearRegression instance 의 fit 은 OSL을 이용한다.

[source,python]
----
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv('AmesHousing.txt', delimiter="\t")
train = data[0:1460]
test = data[1460:]

features = ['Wood Deck SF', 'Fireplaces', 'Full Bath', '1st Flr SF', 'Garage Area',
       'Gr Liv Area', 'Overall Qual']

X = train[features]
y = train['SalePrice']

a = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(Y)
----

== 2. Cost Function
* OSL은 closed form solution 이다. (예측가능한 양의 연산을 통해 해를 구하게 됨.)
* Gradient descent 는 연산의 횟수가 정해져 있지 않은 알고리즘 방식이다.
* 오류는 벡터로 표현된다. : image:./images/m4_2_1.png[, 100] image:./images/m4_2_2.png[, 100]
* 파라미터 벡터 a 로 표현되는 모델을 찾는 것은 y 와 y^ 사이의 MSE 를 최소화하는 것이다. -> 이를 cost function 이라 부른다.
* matrix form 으로 표현된 cost function : image:./images/m4_2_3.png[, 250]

== 3. Derivative Of The Cost Function
* 행렬의 미분법은 따로 보기로..

image::./images/m4_3_1.png[, 250]

== 4. Gradient Descent vs. Ordinary Least Squares
* OSL 계산 중 역행렬 계산 부분은 O(n^3)의 복잡도를 갖기 때문에 데이터셋이 큰 경우 사용이 불가능하다.
* 일반적으로 데이터셋의 수가 수백만건 미만인 경우 사용가능 하다.
* 데이터셋이 큰 경우에는 정확도의 임계치와 반복회수를 적절히 설정한 Gradient descent 의 사용이 적합하다.
