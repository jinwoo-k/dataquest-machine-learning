= Introduction To Neural Networks
Learn the basics of neural networks while predicting the species of iris flowers.

== 1. Neural networks and iris flowers
* 많은 머신러닝 예측 문제는 복잡한 데이터나 피쳐간 비선형 관계에 기인한다.
* 뉴럴넷웍은 변수들 사이의 비선형 관계를 학습할 수 있는 모델이다.
* 뉴럴넷을 소개하기 위해 꽃의 특징으로부터 종을 예측하는 예제를 이용한다.

[source,python]
----
import pandas
import matplotlib.pyplot as plt
import numpy as np

# Read in dataset
iris = pandas.read_csv("iris.csv")

# shuffle rows
shuffled_rows = np.random.permutation(iris.index)
iris = iris.loc[shuffled_rows,:]

print(iris.head())

# There are 2 species
print(iris.species.unique())

iris.hist()
----

== 2. Neurons
* 아래에 표시된 2 차원의 경우, X를 O에서 완전히 분리 할 수있는 함수를 찾고자 하는 경우 선형모델이나 분류 모델로는 불가능하다.
image:./images/m9_2_1.png[, 300]

* 뉴럴넷 모델 이용
  ** 뉴런으로 알려진 일련의 활성화 단위를 사용하여 결과 예측
  ** 뉴런은 입력을 받아 변환 함수를 적용해 출력
  image:./images/m9_2_2.png[, 400]
  ** 위의 뉴런은 하나의 바이아스 유닛(선형회귀모델에서 절편 역할)과 4개의 피쳐로 총 5개의 유닛을 갖는다.
  ** 이 값들은 액티베이션 함수 h 에 공급된다. (0,1을 갖는 확률문제를 다룰때는 시그모이드 함수를 이용한다.)
  ** image:./images/m9_2_3.png[, 300]
  ** image:./images/m9_2_4.png[, 650]

[source,python]
----
z = np.asarray([[9, 5, 4]])
y = np.asarray([[-1, 2, 4]])

# np.dot is used for matrix multiplication
# z is 1x3 and y is 1x3,  z * y.T is then 1x1
print(np.dot(z,y.T))

# Variables to test sigmoid_activation
iris["ones"] = np.ones(iris.shape[0])
X = iris[['ones', 'sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values
y = (iris.species == 'Iris-versicolor').values.astype(int)

# The first observation
x0 = X[0]

# Initialize thetas randomly
theta_init = np.random.normal(0,0.01,size=(5,1))

def sigmoid_activation(x, theta):
    coef = 0
    for i in range(0, len(x)):
        coef += x[i] * theta[i]
    return 1 / (1 + np.exp(-coef))

a1 = sigmoid_activation(x0, theta_init)
----

== 3. Cost function
* gradient descent 를 이용해 뉴런을 학습
  ** cost function : image:./images/m9_3_1.png[, 500]
  ** 위의 비용함수에서 y 값은 0 또는 1이므로 둘 중 한개 항만 남음
  ** 초기 파라미터는 0.01 미만의 작은값으로 랜덤 초기화 됨

[source,python]
----
# First observation's features and target
x0 = X[0]
y0 = y[0]

# Initialize parameters, we have 5 units and just 1 layer
theta_init = np.random.normal(0,0.01,size=(5,1))

def singlecost(x, y, theta):
    h = sigmoid_activation(x, theta)
    return -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))

first_cost = singlecost(x0, y0, theta_init)
----

== 4. Compute the Gradients
* 시그모이드 함수에 대한 비용함수의 도함수는 다음과 같이 구한다.
  ** chain rule 이용 : image:./images/m9_4_1.png[, 150]
  ** image:./images/m9_4_2.png[, 400]
  ** δ는 4개의 백터와 한개의 바이어스에 대한 길이 5의 벡터이고, 기울기를 나타낸다.

[source.python]
----
# Initialize parameters
theta_init = np.random.normal(0,0.01,size=(5,1))

# Store the updates into this array
grads = np.zeros(theta_init.shape)

# Number of observations
n = X.shape[0]

grads = np.zeros(theta_init.shape)

for i in range(0, len(X)):
    h = sigmoid_activation(X[i], theta_init)
    delta = (y[i] - h) * h * (1 - h) * X[i]
    grads += delta[:,np.newaxis]/X.shape[0]

print(grads)
----

== 5. Two layer network
* 그라디언트 디센트를 이용해 2 layer network을 학습하는 메서드인 lean() 메서드를 구현한다.

[source,python]
----
theta_init = np.random.normal(0,0.01,size=(5,1))

# set a learning rate
learning_rate = 0.1
# maximum number of iterations for gradient descent
maxepochs = 10000
# costs convergence threshold, ie. (prevcost - cost) > convergence_thres
convergence_thres = 0.00001

def learn(X, y, theta, learning_rate, maxepochs, convergence_thres):
    costs = []
    cost = singlecost(X, y, theta)  # compute initial cost
    costprev = cost + convergence_thres + 0.01  # set an inital costprev to past while loop
    counter = 0  # add a counter
    # Loop through until convergence
    for counter in range(maxepochs):
        grads = np.zeros(theta.shape)
        for j, obs in enumerate(X):
            h = sigmoid_activation(obs, theta)   # Compute activation
            delta = (y[j]-h) * h * (1-h) * obs   # Get delta
            grads += delta[:,np.newaxis]/X.shape[0]  # accumulate

        # update parameters
        theta += grads * learning_rate
        counter += 1  # count
        costprev = cost  # store prev cost
        cost = singlecost(X, y, theta) # compute new cost
        costs.append(cost)
        if np.abs(costprev-cost) < convergence_thres:
            break

    plt.plot(costs)
    plt.title("Convergence of the Cost Function")
    plt.ylabel("J($\Theta$)")
    plt.xlabel("Iteration")
    plt.show()
    return theta

theta = learn(X, y, theta_init, learning_rate, maxepochs, convergence_thres)
----

== 6. Neural Network
* 뉴럴넷은 종종 더욱 복잡한 학습을 위해 뉴런을 멀티 레이어로 구성한다.
image:./images/m9_6_1.png[, 500]

* 위의 그림은 4개의 입력 변수 x1, x2, x3, x4와 바이어스 항목을 가진 3 레이어 뉴럴넷을 갖는다.
* 각 변수 및 바이어스는 모든 히든레이어 a1~a4로 전송되며, 이 히든레이어는 새로운 파라미터 셋 세타를 갖는다.
image:./images/m9_6_2.png[, 500]

* 결과가 나오는 세번째 레이어는 다음과 같다.
image:./images/m9_6_3.png[, 500]

[source,python]
----
theta0_init = np.random.normal(0,0.01,size=(5,4))
theta1_init = np.random.normal(0,0.01,size=(5,1))

print(theta0_init)
print(theta1_init)

def feedforward(X, theta0, theta1):
    a1 = sigmoid_activation(X.T, theta0).T
    a1 = np.column_stack([np.ones(a1.shape[0]), a1])
    out = sigmoid_activation(a1.T, theta1)
    return out

h = feedforward(X, theta0_init, theta1_init)
----

== 7. Multiple neural network cost function
* 멀티플 뉴럴넷의 비용함수는 싱글 뉴럴넷과 동일하다. 하지만 hΘ(xi) 는 더욱 복잡하다.

image:./images/m9_7_1.png[, 500]

[source,python]
----
theta0_init = np.random.normal(0,0.01,size=(5,4))
theta1_init = np.random.normal(0,0.01,size=(5,1))

# X and y are in memory and should be used as inputs to multiplecost()

def multiplecost(X, y, theta0, theta1):
    h = feedforward(X, theta0, theta1)
    return -np.mean(y * np.log(h) + (1-y) * np.log(1-h))

c = multiplecost(X, y, theta0_init, theta1_init)
----

== 8. Backpropagation
* backpropagation(역전파)는 마지막 레이어에서 시작해 각 레이어를 순환하며 파라미터 변수를 업데이트함을 의미한다.
* 각 레이어에서 l 에서의 계산 : image:./images/m9_8_1.png[, 100]
* image:./images/m9_8_2.png[, 700]

* 기존에 작성했던 모듈을 동적인 히든 유닛에 대해 적용가능하며, 재사용하기 좋은 형태로 리팩토링한다.
[source,python]
----
# Use a class for this model, it's good practice and condenses the code
class NNet3:
    def __init__(self, learning_rate=0.5, maxepochs=1e4, convergence_thres=1e-5, hidden_layer=4):
        self.learning_rate = learning_rate
        self.maxepochs = int(maxepochs)
        self.convergence_thres = 1e-5
        self.hidden_layer = int(hidden_layer)

    def _multiplecost(self, X, y):
        # feed through network
        l1, l2 = self._feedforward(X)
        # compute error
        inner = y * np.log(l2) + (1-y) * np.log(1-l2)
        # negative of average error
        return -np.mean(inner)

    def _feedforward(self, X):
        # feedforward to the first layer
        l1 = sigmoid_activation(X.T, self.theta0).T
        # add a column of ones for bias term
        l1 = np.column_stack([np.ones(l1.shape[0]), l1])
        # activation units are then inputted to the output layer
        l2 = sigmoid_activation(l1.T, self.theta1)
        return l1, l2

    def predict(self, X):
        _, y = self._feedforward(X)
        return y

    def learn(self, X, y):
        nobs, ncols = X.shape
        self.theta0 = np.random.normal(0,0.01,size=(ncols,self.hidden_layer))
        self.theta1 = np.random.normal(0,0.01,size=(self.hidden_layer+1,1))

        self.costs = []
        cost = self._multiplecost(X, y)
        self.costs.append(cost)
        costprev = cost + self.convergence_thres+1  # set an inital costprev to past while loop
        counter = 0  # intialize a counter

        # Loop through until convergence
        for counter in range(self.maxepochs):
            # feedforward through network
            l1, l2 = self._feedforward(X)

            # Start Backpropagation
            # Compute gradients
            l2_delta = (y-l2) * l2 * (1-l2)
            l1_delta = l2_delta.T.dot(self.theta1.T) * l1 * (1-l1)

            # Update parameters by averaging gradients and multiplying by the learning rate
            self.theta1 += l1.T.dot(l2_delta.T) / nobs * self.learning_rate
            self.theta0 += X.T.dot(l1_delta)[:,1:] / nobs * self.learning_rate

            # Store costs and check for convergence
            counter += 1  # Count
            costprev = cost  # Store prev cost
            cost = self._multiplecost(X, y)  # get next cost
            self.costs.append(cost)
            if np.abs(costprev-cost) < self.convergence_thres and counter > 500:
                break

# Set a learning rate
learning_rate = 0.5
# Maximum number of iterations for gradient descent
maxepochs = 10000
# Costs convergence threshold, ie. (prevcost - cost) > convergence_thres
convergence_thres = 0.00001
# Number of hidden units
hidden_units = 4

# Initialize model
model = NNet3(learning_rate=learning_rate, maxepochs=maxepochs,
              convergence_thres=convergence_thres, hidden_layer=hidden_units)
# Train model
model.learn(X, y)

# Plot costs
plt.plot(model.costs)
plt.title("Convergence of the Cost Function")
plt.ylabel("J($\Theta$)")
plt.xlabel("Iteration")
plt.show()
----

== 9. Splitting data
* 모델을 동작시키기 위해 트레이닝셋과 테스트셋으로 데이터를 나눈다.

[source,python]
----
X_train = X[:70]
y_train = y[:70]

X_test = X[-30:]
y_test = y[-30:]
----

== 10. Predicting iris flowers
* 앞의 스탭에서 작성한 NNet3 객체를 이용해 예측후 검증한다
* 뉴럴넷의 성능을 측정하기 위해 ROC(receiver operating characteristic) curve의 AUC(area under the curve)를 이용한다.


[source,python]
----
from sklearn.metrics import roc_auc_score
# Set a learning rate
learning_rate = 0.5
# Maximum number of iterations for gradient descent
maxepochs = 10000
# Costs convergence threshold, ie. (prevcost - cost) > convergence_thres
convergence_thres = 0.00001
# Number of hidden units
hidden_units = 4

# Initialize model
model = NNet3(learning_rate=learning_rate, maxepochs=maxepochs,
              convergence_thres=convergence_thres, hidden_layer=hidden_units)

model.learn(X_train, y_train)
species = model.predict(X_test)[0]

auc = roc_auc_score(y_test, species)
print(auc)
----
