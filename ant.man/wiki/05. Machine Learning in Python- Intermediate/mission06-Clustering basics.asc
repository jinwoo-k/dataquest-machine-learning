== Clustering basics
=== 1. Clustering overview
* 회귀와 분류: 관리하는 타입의 기계학습 (알수없는 변수를 예측하는 알고리즘을 학습)
* 자율학습: 예측하려하지 않고 패턴을 찾는다.
** 자율학습 기술 중 하나, 클러스터링
** 클러스터링: 알려지지 않은 데이터를 탐색하는 핵심 방법
*** 유사한 행을 그룹화, 데이터에 하나이상의 그룹 넣음.
*** 예제, 투표 방법에 따라 미국 상원의원을 클러스터링하는 작업

=== 2. The dataset
* 법안 통과를 위해선 다수결 필요
* 정당별 투표 방식에 따라 투표
** 2개 주요정당 민주당, 공화당에 따르는 투표. 또한, 개인이 독립적으로 투표 가능
* 데이터
** 0 은 반대, 1은 찬성, 0.5는 기권
** name: 의원의 이름
** party: [D: 민주당, R:공화당, I:독립]
** 00001, 00004 등의 번호지정 열: 각각 단일 투표 결과 (header 에서는 투표의 제목)
* 데이터 수집으로, 파티 (정당)보다 더 깊은 패턴을 확인 할 수 있음.
* 또한 당원의 주류 의원을 확인 가능

=== 3. Exploring the data
```
votes["party"].value_counts() # 값들 갯수출력 => 각 정당의 갯수 출력
votes.mean() # 각 열의 평균 => 각 투표마다 0.5 이상/이하로 찬/반 확인 가능
```

=== 4. Distance between Senators
* 의원을 모으기 위해서, 얼마나 가까운지 파악 필요 => 가까움을 그룹화
** 두 의원의 가까움을 유클리드 거리 사용 => 투표의 숫자를 공식에 대입
*** http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html[euclidean_distances]
*** `euclidean_distances(votes.iloc[0,3:], votes.iloc[1,3:])`
**** 위 공식에서 3번째 부터인 이유 (name,party,state를 제외하여 공식에 투표숫자만 대입하기 위함)
** print(euclidean_distances(votes.iloc[0,3:].reshape(1, -1), votes.iloc[1,3:].reshape(1, -1)))

=== 5. Initial clustering
* k-means 클러스터링 : 유클리안 거리로 클러스터 형성
** 클러스터의 수를 지정해야 함
*** 예제에서 주요 당의 노선을 따를것으로 예상되기에 2로 선택
** 아무것도 예측못하기 때문에 overfit 위험 없음. => 전체 데이터로 학습
** `kmeans_model = KMeans(n_clusters=2, random_state=1)`
** http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit_transform[fit_transform]
*** 결과 : `array([[ 3.12141628,  1.3134775 ],[ 2.6146248 ,  2.05339992],... ])`
**** 첫번째 열은 첫번재 클러스터와의 유클리드거리, 두번째 열은 두번째 클러스터까지의 유클리드거리, 각 열은 각각의 상원의원
```
kmeans_model = KMeans(n_clusters=2, random_state=1)
senator_distances = kmeans_model.fit_transform(votes.iloc[:,3:]) # 3부터의 투표 결과만
```

=== 7. Exploring the clusters
* http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.crosstab.html[crosstab]
** 각 클러스터의 의원 수 계산
** 두개의 벡터 혹은 series(pandas) 를 사용하고 첫 벡터의 각 고유값에 대해 두번째 벡터의 고유값 몇번 발생하는지 계산
** labels_ : : Labels of each point
```
labels = kmeans_model.labels_
print(pd.crosstab(labels, votes["party"]))
```
* output
```
party   D  I   R
row_0
0      41  2   0
1       3  0  54
```

=== 8. Exploring Senators in the wrong cluster
* 결과 해석하면 D (민주당) 이면서 1 (두번째 클러스터에) 속한 3명이 보임
** 이 사람들을 걸러내서 보기 위해 아래의 수식을 활용
** `democratic_outliers = votes[(labels == 1) & (votes["party"] == "D")]`
*** 팁: 각 조건은 괄호안, & 로 구분되어야 함

=== 9. Plotting out the clusters
* 클러스터 탐색하기 좋은건 시각화 -> matplotlib 사용
** http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter[pyplot.scatter]
```
X = senator_distances[:,0]
y = senator_distances[:,1]
c = labels

plt.scatter(x=X,y=y,c=labels)
plt.show()
```
image:./images/senator_distance_plt.png[]

=== 10. Finding the most extreme
* 극단주의자를 찾기 위한 수식
** 세제곱 (큐브,cubing)을 이용하여 높은 값과 낮은 값 사이의 차이를 늘림
** 예제에서는 `senator_distances ** 3` 로 간단하게 코드 작성 가능
** http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html#pandas.DataFrame.sort_values[sort_values]
```
extremism = (senator_distances ** 3).sum(axis=1)
votes["extremism"] = extremism
votes.sort_values("extremism", inplace=True, ascending=False)
print(votes.head(10))
```

===
===
