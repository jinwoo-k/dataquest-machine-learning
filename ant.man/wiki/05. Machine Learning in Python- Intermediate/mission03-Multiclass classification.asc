== Multiclass classification
=== 1. Introduction to the data
* 자동차의 다양한 정보로 북미, 아시아, 유럽 등의 차의 출처 예측하기 (이전과 달리 세가지 카테고리)
```
<test data example>
18.0   8   307.0      130.0      3504.      12.0   70  1    "chevrolet chevelle malibu"
15.0   8   350.0      165.0      3693.      11.5   70  1    "buick skylark 320"
```
```
import pandas as pd
cars = pd.read_csv("auto.csv")

unique_regions = cars['origin'].unique()
print(unique_regions)
```

=== 2. Dummy variables
* 범주형 데이터는 더미 변수를 사용 -> 카테고리가 2개 이상일때 열 더 만들어서 표현
** 예: 차에 3개의 실린더가 있는가? 0 or 1 ,이런식으로 변형 (4개,..8개가 있는가? 로 질문을 0,1로 확장)
** http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html[pandas.get_dummies()] 사용

```
dummy_cylinders = pd.get_dummies(cars["cylinders"], prefix="cyl")
cars = pd.concat([cars, dummy_cylinders], axis=1)
cars.drop('cylinders', axis=1)
```

=== 3. Multiclass classification
* multiclass classification : 3개 이상의 카테고리
* 여러 방법 중 "one-versus-all" : 하나의 카테고리를 긍정, 나머지 카테고리를 부정으로 그룹화

```
shuffled_rows = np.random.permutation(cars.index)
shuffled_cars = cars.iloc[shuffled_rows]

#highest_train_row = int(cars.shape[0] * .70)
sizeOfTrain=int(len(shuffled_cars)*0.7)
train = shuffled_cars.iloc[0:sizeOfTrain]
test = shuffled_cars.iloc[sizeOfTrain:]
```

=== 4. Training a multiclass logistic regression model
* n 개의 이진 분류 문제로 변환
** 3개로 구분할 경우, 3개의 훈련 모델 필요
*** 이번 예로, 북미, 유럽, 아시아 구분 시: 북미일경우와 나머지 두개일 경우를 나눈게 한 모델
*** 각 모델에서 0,1 사이의 확률로 나오며, 가장 높은 확률을 뽑으면 됨
```
unique_origins = cars["origin"].unique()
unique_origins.sort()

models = {}
# 요 구문이 필요
features = [c for c in train.columns if c.startswith("cyl") or c.startswith("year")]

for origin in unique_origins:
    model = LogisticRegression()

    X_train = train[features]
    y_train = train["origin"] == origin # 1,2,3 에 해당하는 row 만 거르기

    model.fit(X_train, y_train)
    models[origin] = model
```

=== 5. Testing the models
* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit[LogisticRegression]
```
testing_probs = pd.DataFrame(columns=unique_origins)

# models[1].predict_proba(test[features])
# model.fit(X_train, y_train).predict_proba(test[features])

for origin in unique_origins:
    # Select testing features.
    X_test = test[features]
    # Compute probability of observation being in the origin.
    # [:x] 이걸로 열->행 바꿈 + ,1 로 두번째 열을 표현
    # 원래는 one vs rest 인 2차원 배열 여러개 형식으로 결과 나옴
    # ex: [[one1, rest1] , [one2, rest2 ] , .. ,[oneN, restN] ]
    # 근데 왜 밑에 ,1로 했을지 의문.. ,0 으로 해야하지 않았나?
    # https://stackoverflow.com/questions/36681449/scikit-learn-return-value-of-logisticregression-predict-proba
    # 정리하면, 두번째 인덱스 1이, positive 임 , 첫번째 인덱스인 0 은 negative 따라서 ,1 을 하는게 맞음
    testing_probs[origin] = models[origin].predict_proba(X_test)[:,1]
```

=== 6. Choose the origin
* 각 모델 분류 확률에서 최대치 구하기 (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.idxmax.html[idxmax])
** `predicted_origins=testing_probs.idxmax(1)`

=== 7. next step
