= Gradient Descent

== 1. Introdution
* 이전 미션에서 선현 회귀 모델이 속성열과 대상열 간의 관계를 예측하는 방법과 이를 예측에 사용하는 방법을 배웠다.
* 선형회귀모델에 대한 최적의 매개변수 값을 찾는 방법을 모델 피팅이라고 함


== 2. Single Variable Gradient Descent
* Gradient Descent : 평균제곱오류가 가장 낮은 모델이 발견 될 때까지 다른 매개 변수 값을 반복적으로 넣는 기술.
* 이 기술은 신경망과 같은 다른 모델에서도 일반적으로 사용되는 최적화 기술
* 단일 매개변수 선형회귀 모델에서의 Gradient Descent 알고리즘
** 매개변수 초기값 선택 (a1)
** 결과 값에 수렴될때까지 계속 반복 (일반적으로 최대 반복 횟수로 구현)
*** 현재 매개변수 값을 사용하는 모델의 오류(MSE)를 계산
*** 현재 매개변수 값에서 오류의 미분(MSE)을 계산
*** a1을 ⍺d로 변경해서 위 내용 반복

== 3. Derivative Of The Cost Function
* 최적화하는 함수는 비용 함수(손실 함수)라고 함
* 미적분으로부터 미분 속성의 선형성을 적용함으로써, 합산 내에 미분 항을 가져올 수 있다.

image::./images/3-1.PNG

* power rule을 적용하면 이렇게 됨

image::./images/3-2.PNG

* 단순화 하면

image::./images/3-3.PNG

* gradient descent 반복마다
** 이 도함수는 현재의 a1값을 사용하여 게산됨
** 그 미분값에 학습속도(⍺)를 곱함
** 결과는 현재 매개변수 값에서 뺀 다음 새 매개변수 값으로 지정됨
** 다음은 10번 반복에 대해 gradient descent를 실행한 코드

[source,python]
----
a1_list = [1000]
alpha = 10

for x in range(0, 10):
    a1 = a1_list[x]
    deriv = derivative(a1, alpha, xi_list, yi_list)
    a1_new = a1 - alpha*deriv
    a1_list.append(a1_new)
----

== 4. Understanding Multi Parameter Gradient Descent
* 다중 매개변수 gradient descent에 대해 만들어본다.

image::./images/3-4.PNG


== 5. Gradient Of The Cost Function
* 첫번째 화면 위젯에서 매개변수 슬라이더를 이용해 잔여 제곱합을 줄일수 있었다.
** 프록시에 의해 평균 제곱오류가 감소
* Gradient는 미분의 다중 변수 일반화이다.
* 2개의 매개변수 값(a0 및 a1)이 있는 경우 비용 함수는 이제 1이 아닌 2개 변수의 함수다.
* 하나의 업데이트 규칙 대신 두가지 업데이트 규칙이 필요하다.

image::./images/3-5.PNG

== 6. Gradient Descent For Higher Dimensions
* 모델에서 많은 매개변수를 사용하려면 어떻게 해야할까?
* gradient descent는 실제로 원하는 만큼 많은 변수에 맞게 조절된다.
* 각 매개변수값에는 자체 업데이트 규칙이 필요하며 a1의 업데이트 규칙과 거의 일치한다.

image::./images/3-6.PNG

image::./images/3-7.PNG

== 7. Next Steps
* Gradient descent 주요과제
** 좋은 초기 매개 변수 값 선택
* 좋은 학습속도 선택 (하이퍼 매개변수 최적화 영역에 속함)

== 예제 모음

=== 3.

[source,python]
----
def derivative(a1, xi_list, yi_list):
    len_data = len(xi_list)
    error = 0
    for i in range(0, len_data):
        error += xi_list[i]*(a1*xi_list[i] - yi_list[i])
    deriv = 2*error/len_data
    return deriv

def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial):
    a1_list = [a1_initial]

    for i in range(0, max_iterations):
        a1 = a1_list[i]
        deriv = derivative(a1, xi_list, yi_list)
        a1_new = a1 - alpha*deriv
        a1_list.append(a1_new)
    return(a1_list)

param_iterations = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150)
----


=== 5.
[source,python]
----
def a1_derivative(a1, xi_list, yi_list):
    len_data = len(xi_list)
    error = 0
    for i in range(0, len_data):
        error += xi_list[i]*(a1*xi_list[i] - yi_list[i])
    deriv = 2*error/len_data
    return deriv

def a0_derivative(a0, xi_list, yi_list):
    len_data = len(xi_list)
    error = 0
    for i in range(0, len_data):
        error += a0*xi_list[i] - yi_list[i]
    deriv = 2*error/len_data
    return deriv

def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial, a0_initial):
    a1_list = [a1_initial]
    a0_list = [a0_initial]

    for i in range(0, max_iterations):
        a1 = a1_list[i]
        a0 = a0_list[i]

        a1_deriv = a1_derivative(a1, xi_list, yi_list)
        a0_deriv = a0_derivative(a0, xi_list, yi_list)

        a1_new = a1 - alpha*a1_deriv
        a0_new = a0 - alpha*a0_deriv

        a1_list.append(a1_new)
        a0_list.append(a0_new)
    return(a0_list, a1_list)

a0_params, a1_params = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150, 1000)
print(a0_params)
print(a1_params)
----
