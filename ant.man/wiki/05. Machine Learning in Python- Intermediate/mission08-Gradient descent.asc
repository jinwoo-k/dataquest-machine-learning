== Gradient descent
=== 1. Introduction to the data
* 프로골퍼의 정확도와 거리
** 더 멀리칠수록 정확도 떨어짐을 예상
* 정확도는 퍼센트, 거리는 m 단위 -> 다른 단위이므로 표준화
```
import pandas
import matplotlib.pyplot as plt

# Read data from csv
pga = pandas.read_csv("pga.csv")

# Normalize the data
pga.distance = (pga.distance - pga.distance.mean()) / pga.distance.std()
pga.accuracy = (pga.accuracy - pga.accuracy.mean()) / pga.accuracy.std()
print(pga.head())

plt.scatter(pga.distance, pga.accuracy)
plt.xlabel('normalized distance')
plt.ylabel('normalized accuracy')
plt.show()
```
image:./images/normal_acc_distance_plt.png[]

=== 2. Linear model
* 그림으로 보아 음의 기울기인 선형모델
** image:./images/formular_linear_model.png[]
** θ: 계수, ϵ :에러
** http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html[LinearRegression]
```
from sklearn.linear_model import LinearRegression
import numpy as np

# We can add a dimension to an array by using np.newaxis
print("Shape of the series:", pga.distance.shape) # (197,)
print("Shape with newaxis:", pga.distance[:, np.newaxis].shape) # (197, 1)
print("Shape of the series:", pga.accuracy.shape) # (197,)

lm = LinearRegression()
lm.fit(pga.distance[:, np.newaxis], pga.accuracy)
theta1 = lm.coef_[0] # -0.6
```

=== 3. Cost function, introduction
* sklearn 라이브러리로 least square 사용하여 값 구함
** 너무 많은 메모리 사용기때문에 너무 크면 구할 수 없음.
* gradient descent 는 선형모델을 포함, 거의 모든 모델의 계수 추정 가능
** 코어에서 각 계수를 업데이트
* cost function 을 사용하려면 예측과 실제의 차이를 측정해야 함.
** 예측모델: image:./images/our_cost_formular.png[]
** 비용함수: image:./images/cost_function_model.png[]
*** 비용은 예측과 관측 제곱의 평균차이의 1/2
*** 계수를 무작위 선택 후 지능적으로 업데이트하여 비용을 최소화

```
# The cost function of a single variable linear model
def cost(theta0, theta1, x, y):
    # Initialize cost
    J = 0
    # The number of observations
    m = len(x)
    # Loop through each observation
    for i in range(m):
        # Compute the hypothesis
        h = theta1 * x[i] + theta0
        # Add to cost
        J += (h - y[i])**2
    # Average and normalize cost
    J /= (2*m)
    return J

# The cost for theta0=0 and theta1=1
print(cost(0, 1, pga.distance, pga.accuracy))

theta0 = 100
theta1s = np.linspace(-3,2,100)
costs = []
for theta1 in theta1s:
    costs.append(cost(theta0, theta1, pga.distance, pga.accuracy))

plt.plot(theta1s, costs)
```
image:./images/cost_function_plt.png[]

=== 4. Cost function, continued
* 그림으로 보아 비용함수는 기울기와 관련하여 2차함수, 최소값이 존재
** x,y축 기울기와 절편, z축이 비용이 되는 함수를 시각화
```
import numpy as np
from mpl_toolkits.mplot3d import Axes3D

# Example of a Surface Plot using Matplotlib
# Create x an y variables
x = np.linspace(-10,10,100)
y = np.linspace(-10,10,100)

# We must create variables to represent each possible pair of points in x and y
# ie. (-10, 10), (-10, -9.8), ... (0, 0), ... ,(10, 9.8), (10,9.8)
# x and y need to be transformed to 100x100 matrices to represent these coordinates
# np.meshgrid will build a coordinate matrices of x and y
X, Y = np.meshgrid(x,y)
print(X[:5,:5],"\n",Y[:5,:5])

# Compute a 3D parabola
Z = X**2 + Y**2

# Open a figure to place the plot on
fig = plt.figure()
# Initialize 3D plot
ax = fig.gca(projection='3d')
# Plot the surface
ax.plot_surface(X=X,Y=Y,Z=Z)

plt.show()

# Use these for your excerise
theta0s = np.linspace(-2,2,100)
theta1s = np.linspace(-2,2, 100)
COST = np.empty(shape=(100,100))
# Meshgrid for paramaters
T0S, T1S = np.meshgrid(theta0s, theta1s)
# for each parameter combination compute the cost
for i in range(100):
    for j in range(100):
        COST[i,j] = cost(T0S[0,i], T1S[j,0], pga.distance, pga.accuracy)

# make 3d plot
fig2 = plt.figure()
ax = fig2.gca(projection='3d')
ax.plot_surface(X=T0S,Y=T1S,Z=COST)
plt.show()
```
image:./images/cost_function_3d.png[]

=== 5. Cost function, slopes
* Gradient descen: 가장 큰 기울기로 가는 방향을 찾는 것.
* 다변수함수에서 단일 변수 관점에서 편미분을 통해 변수의 기울기를 확인
* 각 매개 변수와 관련된 함수의 경사를 찾는게 젤 중요

** theta0: image:./images/theta0_formular.png[]
** theta1: image:./images/theta1_formular.png[]

```
def partial_cost_theta1(theta0, theta1, x, y):
    # Hypothesis
    h = theta0 + theta1*x
    # Hypothesis minus observed times x
    diff = (h - y) * x
    # Average to compute partial derivative
    partial = diff.sum() / (x.shape[0])
    return partial

partial1 = partial_cost_theta1(0, 5, pga.distance, pga.accuracy)
print("partial1 =", partial1)

def partial_cost_theta0(theta0, theta1, x, y):
    # Hypothesis
    h = theta0 + theta1*x
    # Hypothesis minus observed times x
    diff = (h - y)
    # Average to compute partial derivative
    partial = diff.sum() / (x.shape[0])
    return partial

partial0 = partial_cost_theta0(1, 1, pga.distance, pga.accuracy)
print("partial0 =", partial0)
```

=== 6. Gradient descent algorithm
* 기울기와 편차를 변화시킴으로써 비용이 크게 달라짐
* 가설과 실제값의 오차를 최소화하기 위해 매개변수를 변경하여 비용함수의 최소값 구할 수 있음
* gradient descent 는 최저의 매개변수를 찾는데 쓰이는 많이 사용되는 방법
** 초기값으로 부터 가파른 기울기의 방향으로 이동하여 업데이트 (반복하여 최적값으로 이동)
image:./images/gradient_descent_algoritm.png[]
** α는 학습속도 (일반적으로 0.0001 ~ 1 사이), 너무 작으면 너무많은 반복, 너무 크면 최소값을 초과함
*** max_ecpoch를 사용해 무제한 반복을 하지 않도록 함
*** c 는 초기 매개변수를 이용해 초기 비용을 보유하는데 사용

=== 7. Gradient descent algorithm
```
# x is our feature vector -- distance
# y is our target variable -- accuracy
# alpha is the learning rate
# theta0 is the intial theta0
# theta1 is the intial theta1
def gradient_descent(x, y, alpha=0.1, theta0=0, theta1=0):
    max_epochs = 1000 # Maximum number of iterations
    counter = 0      # Intialize a counter
    c = cost(theta1, theta0, pga.distance, pga.accuracy)  ## Initial cost
    costs = [c]     # Lets store each update
    # Set a convergence threshold to find where the cost function in minimized
    # When the difference between the previous cost and current cost
    #        is less than this value we will say the parameters converged
    convergence_thres = 0.000001
    cprev = c + 10
    theta0s = [theta0]
    theta1s = [theta1]

    # When the costs converge or we hit a large number of iterations will we stop updating
    while (np.abs(cprev - c) > convergence_thres) and (counter < max_epochs):
        cprev = c
        # Alpha times the partial deriviative is our updated
        update0 = alpha * partial_cost_theta0(theta0, theta1, x, y)
        update1 = alpha * partial_cost_theta1(theta0, theta1, x, y)

        # Update theta0 and theta1 at the same time
        # We want to compute the slopes at the same set of hypothesised parameters
        #             so we update after finding the partial derivatives
        theta0 -= update0
        theta1 -= update1

        # Store thetas
        theta0s.append(theta0)
        theta1s.append(theta1)

        # Compute the new cost
        c = cost(theta0, theta1, pga.distance, pga.accuracy)

        # Store updates
        costs.append(c)
        counter += 1   # Count

    return {'theta0': theta0, 'theta1': theta1, "costs": costs}

print("Theta1 =", gradient_descent(pga.distance, pga.accuracy)['theta1'])
descend = gradient_descent(pga.distance, pga.accuracy, alpha=.01)
plt.scatter(range(len(descend["costs"])), descend["costs"])
plt.show()
```

image:./images/gradient_descent_plt.png[]

=== 8. Conclusion
* Gradient descent: 함수에서 로컬/전역의 최소를 계산하는데 많이 사용되는 알고리즘
** least squares와 결과 비슷, 하지만 데이터가 커지면 gradient descent 가 더 유리함. (메모리 사용)
** 따라서 gradient descent 를 사용하여 신경망을 학습할 예정
