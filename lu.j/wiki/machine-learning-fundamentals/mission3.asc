= Multivariate K-Nearest Neighbors

== 1. 요점 되풀이하기
* 이제까지 한가지의 feature, attribute로만 측정했는데 다른 여러가지를 적용해서 측정하려고 함
* 정확성을 높이기 위한 2가지 방법
** 가장 가까운 이웃의 순위를 매길때 유사성 측정에서 여러가지 속성을 이용함
** K 값을 증가시켜 예측을 계산할때 가까운 이웃의 수를 늘림

* 이 미션에서는 속성을 늘리는 것에 집중할 예정인데 그때의 주의점
** 수치가 아닌 속성값 (유클리드 거리는 수치값으로 계산하기 때문에)
** 값이 없는 경우
** 순위를 매길수 없는 경우

* 위 주의점에 해당하는 열을 필터해보자 (DataFrame.info() 함수 이용)

[source,python]
----
import pandas as pd
import numpy as np
np.random.seed(1)

dc_listings = pd.read_csv('dc_airbnb.csv')
dc_listings = dc_listings.loc[np.random.permutation(len(dc_listings))]
stripped_commas = dc_listings['price'].str.replace(',', '')
stripped_dollars = stripped_commas.str.replace('$', '')
dc_listings['price'] = stripped_dollars.astype('float')

print(dc_listings.info())
----


== 2. Features 제거
* 수치화 할수 없는 속성을 제거

[source,python]
----
dc_listings = dc_listings.drop(['room_type', 'city', 'state'], axis=1)
dc_listings = dc_listings.drop(['longitude', 'latitude', 'zipcode'], axis=1)
dc_listings = dc_listings.drop(['host_response_rate', 'host_acceptance_rate', 'host_listings_count'],axis=1)
----

== 3. 누락된 값에 대한 처리
* NA인 데이터 처리

[source,python]
----
dc_listings = dc_listings.drop(['cleaning_fee', 'security_deposit'], axis=1)
dc_listings = dc_listings.dropna(axis=0)
----

== 4. 열 표준화
* 각 속성마다 기준 값들이 달라 거리 계산에 영향을 줌
* 한 속성이 거리 계산에 영향을 주지 않게 하기위해 모든 속성을 평균 0, 표준편차를 1로 정규화함
* 표준 정규 분포를 하기 위한 작업 2가지
** 각 값에서 속성의 평균을 뺀다
** 각 값을 속성의 표준편차로 나눈다

[source,python]
----
normalized_listings = (dc_listings - dc_listings.mean()) / (dc_listings.std())
normalized_listings['price'] = dc_listings['price']
print(normalized_listings.head(3))
----

== 5. 다변수 케이스의 유클리드 거리
* 이제 두가지의 속성을 이용해보도록 한다
* 두가지 속성의 유클리드 거리 구하기

image::https://s3.amazonaws.com/dq-content/distance_two_features.png[distance]
* euclidean 함수가 필요한 것
** list-like 객체를 사용하여 표현할 두 벡터
** 두 벡터가 1차원이고 두 요소의 수가 같아야함

[source,python]
----
from scipy.spatial import distance
first = normalized_listings.iloc[0][['accommodates', 'bathrooms']]
fifth = normalized_listings.iloc[4][['accommodates', 'bathrooms']]
first_fifth_distance = distance.euclidean(first, fifth)
----


== 6. Scikit-Learn 소개
* Python에서 가장 많이 사용되는 Scikit-Lean 라이브러리에 대한 소개
* Scikit-Lean은 모든 주요 머신러닝 알고리즘 및 단순하고 통일된 워크 플로우를 포함함
* Scikit-Lean 4가지 워크플로우
** 사용할 특정 머신러닝 모델을 인스턴스화 함
** 모델을 트레이닝 데이터에 맞춘다
** 모델을 이용하여 예측값을 만든다
** 예측값의 정확성을 판단한다

* 우리가 한 케이스 같은 가격을 예측하는 모델에는 회귀(regression)모델이 어울림


== 7. 모델 피팅과 예측하기
* fit 메소드를 이용해 데이터를 모델을 맞출수 있다.
* fit 메소드는 2가지의 파라미터를 필요로 함
** 트레이닝 셋의 feature
** 트레이닝 셋의 대상값

* fit메소드를 호출할때 Scikit-Lean은 트레이닝 셋을 KNearestNeighbors 인스턴스에 저장
* 만약 누락된 값이나 숫자를 입력하려고 하면 에러를 리턴함
* predict() 함수가 필요한 파라미터
** 테스트 셋 feature

[source,python]
----
from sklearn.neighbors import KNeighborsRegressor

train_df = normalized_listings.iloc[0:2792]
test_df = normalized_listings.iloc[2792:]

knn = KNeighborsRegressor(n_neighbors=5, algorithm='brute')

knn.fit(train_df[['accommodates', 'bathrooms']], train_df['price'])
predictions = knn.predict(test_df[['accommodates', 'bathrooms']])
----

== 8. MSE를 Scikit-Lean을 이용해서 계산
* mean_squared_error() 함수를 이용해 앞서 계산한 MSE 값을 계산하기

[source,python]
----
from sklearn.metrics import mean_squared_error

train_columns = ['accommodates', 'bathrooms']
knn = KNeighborsRegressor(n_neighbors=5, algorithm='brute', metric='euclidean')
knn.fit(train_df[train_columns], train_df['price'])
predictions = knn.predict(test_df[train_columns])

two_features_mse = mean_squared_error(test_df['price'], predictions)
two_features_rmse = two_features_mse ** (1/2)
----

== 9. 더 많은 feature 이용하기

[source,python]
----
features = ['accommodates', 'bedrooms', 'bathrooms', 'number_of_reviews']
from sklearn.neighbors import KNeighborsRegressor
knn = KNeighborsRegressor(n_neighbors=5, algorithm='brute')

knn.fit(train_df[features], train_df['price'])
four_predictions = knn.predict(test_df[features])
four_mse = mean_squared_error(test_df['price'], four_predictions)
four_rmse = four_mse ** (1/2)
----

== 10. 모든 feature 이용하기

[source,python]
----
knn = KNeighborsRegressor(n_neighbors=5, algorithm='brute')

features = train_df.columns.tolist()
features.remove('price')

knn.fit(train_df[features], train_df['price'])
all_features_predictions = knn.predict(test_df[features])
all_features_mse = mean_squared_error(test_df['price'], all_features_predictions)
all_features_rmse = all_features_mse ** (1/2)
----

== 11. Next Steps
* 모델에서 사용할 feature를 선택하는 것을 feature selection이라고 한다.
* 이번 미션에서는 더 많은 feature를 이용하는 것이 K-nearest neighbors 의 정확성을 향상시키는지 탐험해보았다.


