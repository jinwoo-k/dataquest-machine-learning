== 도입
 * hold out validation
   ** 전체 데이터 세트를 2 개의 파티션으로 분할
      *** 트레이닝 세트
      *** 테스트 세트
   ** 트레이닝 세트의 모델 트레이닝,
   ** 훈련 된 모델을 사용하여 테스트 세트의 라벨 예측
   ** 모델의 효율성을 이해하기위한 오류 메트릭 계산
   ** 트레이닝 및 테스트 세트 전환
   ** 오류를 평균하고 반복.

image::./images/holdout_validation.png[holdout validation]

==  Holdout Validation
  *  첫번째 절반을 KNN 모델로 훈련시키고,
  *  두번째 절반으로 이 모델을 테스트하고,
  *  두번째 절반을 KNN모델로 훈련 시키고,
  *  첫번째 절반으로 이 모델을 테스트한다.

== K-Fold Cross Validation
 * 전체 데이터 세트를 k 개의 등 길이 파티션으로 분할
    ** 트레이닝 세트로 k-1 파티션을 선택하고
    ** 테스트 세트로 남아있는 파티션을 선택
  * 훈련세트로 모델을 트레이닝
  * 트레이닝 된 모델을 사용하여 테스트 폴드상의 라벨을 예측
  * 테스트 폴드의 에러 메트릭을 계산한다.
  * 각 파티션이 반복을위한 테스트 세트로 사용 될 때까지
    위의 모든 단계를 k-1 번 반복하여 실행하고
  * k 개의 오류 값의 평균을 계산합니다.

image::./images/kfold_cross_validation.png[kfold-cross validation]

== First Iteration (SKIP)
== Function For Training Models (SKIP)
== Performing K-Fold Cross Validation Using Scikit-Learn
 * KFoldClass : kf = KFold(n, n_folds, shuffle=False, random_state=None)
   ** n :  데이터 세트의 관측 수
   ** n_folds : 사용하려는 폴드의 수
   ** shuffle : 데이터 세트에서 관측 순서를 셔플링 하는 여부를 결정
   ** random_state : shuffle이 True로 설정된 경우 시드 값을 지정하는 데 사용
 * http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html[sklearn.cross_validation.cross_val_score]
   ** cross_val_score(estimator, X, Y, scoring=None, cv=None)
     *** estimator: fit 함수를 구현한 모델
     *** X : 트레이닝 할 속성이 포함 된 목록 또는 2D 배열
     *** Y : 예측할 값이 포함 된 목록
     *** scoring : http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values[채점기준]
     *** cv : 폴드의 개수
        **** KFold 클래스의 인스턴스
        **** 폴드 수를 나타내는 정수
 *  k-fold 교차 유효성 검사를 수행하는 일반적인 워크 플로우
   ** 적합하고 싶은 scikit-learn 모델 클래스를 인스턴스화
   ** 매개 변수를 사용하여 원하는 k- 교차 유효성 검사 특성을 지정
   ** cross_val_score 함수를 사용하여 관심있는 채점 척도를 구한다.

== Exploring Different K Values
    * LOOCK (leave-one-out cross validation) : k를 n (데이터 세트의 관측 수)과 같게 설정하는 것
    * 예제에서 폴드 수를 늘리면 RMSE의 표준 편차가 약 1.1에서 37.3으로 증가

== Bias-Variance Tradeoff
  * 모델에는 에러의 두가지 원인이 존재 :  편차,분산
     ** 편차는 학습 알고리즘에 대한 잘못된 가정을 초래하는 오류를 설명
     ** 분산은 모델의 예측 된 값의 다양성 때문에 발생하는 오류를 설명
  * k-nearest negihbors는 예측을 할 수 있지만 수학적 모델은 아닙니다.
  * 수학적 모델은 일반적으로 원래 데이터없이 존재할 수있는 방정식.

image::./images/bias_variance.png[편차와 분산]


---
==  참고
* http://bywords.tistory.com/entry/%EB%B2%88%EC%97%AD-%EC%9C%A0%EC%B9%98%EC%9B%90%EC%83%9D%EB%8F%84-%EC%9D%B4%ED%95%B4%ED%95%A0-%EC%88%98-%EC%9E%88%EB%8A%94-biasvariance-tradeoff[쉽게 이해하는 편향-분산 트레이드 오프]
* http://softgearko.blogspot.kr/2014/04/machine-learning-1-cross-validation.html[Cross Validation]




