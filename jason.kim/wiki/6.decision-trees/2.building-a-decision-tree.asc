= Building a Decision Tree

== 1. Introduction to the Data
* 이전 미션에서 사용한 소득 데이터를 그대로 활용한다.

== 2. Overview of the ID3 Algorithm
* 이번 미션에서는 파이썬을 이용해 지난 미션에서 다루었던 의사결정나무를 만들고, 이를통해 예측을 해 본다.
* ID3 알고리즘은 Recursion 과 시간복잡도에 대한 이해가 필요하다.
* ID3 알고리즘에 대한 pseudo code
[source,python]
----
def id3(data, target, columns)
    1 Create a node for the tree
    2 If all values of the target attribute are 1, Return the node, with label = 1
    3 If all values of the target attribute are 0, Return the node, with label = 0
    4 Using information gain, find A, the column that splits the data best
    5 Find the median value in column A
    6 Split column A into values below or equal to the median (0), and values above the median (1)
    7 For each possible value (0 or 1), vi, of A,
    8    Add a new tree branch below Root that corresponds to rows of data where A = vi
    9    Let Examples(vi) be the subset of examples that have the value vi for A
   10    Below this new branch add the subtree id3(data[A==vi], target, columns)
   11 Return Root
----

== 3. Walking Through an Example of the ID3 Algorithm
* ID3를 이해하기 위해 더미 데이터셋을 이용해 돌려본다.
  ** image:./images/m2_3_1.png[, 400]
* age 의 중위값(37.5)을 이용해 시작하며 노드6에 도달했을때 아래와 같이 트리가 구성된다.
  ** image:./images/m2_3_2.png[, 300]

== 4. Determining the Column to Split On
* 가장 정보획득(information gain)이 높은 컬럼을 찾는 메서드 find_best_column 를 구현한다.

[source,python]
----
def calc_information_gain(data, split_name, target_name):
    original_entropy = calc_entropy(data[target_name])

    column = data[split_name]
    median = column.median()

    left_split = data[column <= median]
    right_split = data[column > median]

    to_subtract = 0
    for subset in [left_split, right_split]:
        prob = (subset.shape[0] / data.shape[0])
        to_subtract += prob * calc_entropy(subset[target_name])

    return original_entropy - to_subtract

def find_best_column(data, target_name, columns):
    best_column = columns[0]
    best_ig = calc_information_gain(data, columns[0], target_name)

    for i in range(1, len(columns)):
        ig = calc_information_gain(data, columns[i], target_name)
        if (ig > best_ig):
            best_column = columns[i]
            best_ig = ig
    return best_column

columns = ["age", "workclass", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "hours_per_week", "native_country"]

income_split = find_best_column(income, 'high_income', columns)
----

== 5. Creating a Simple Recursive Algorithm
* id3 함수를 작성한다.

[source,python]
----
# We'll use lists to store our labels for nodes (when we find them)
# Lists can be accessed inside our recursive function, whereas integers can't.
# Look at the python missions on scoping for more information on this topic
label_1s = []
label_0s = []

def id3(data, target, columns):
    # The pandas.unique method will return a list of all the unique values in a series
    unique_targets = pandas.unique(data[target])

    if len(unique_targets) == 1:
        if (unique_targets[0] == 1):
            label_1s.append(1)
        else:
            label_0s.append(0)
        return

    # Find the best column to split on in our data
    best_column = find_best_column(data, target, columns)
    # Find the median of the column
    column_median = data[best_column].median()

    # Create the two splits
    left_split = data[data[best_column] <= column_median]
    right_split = data[data[best_column] > column_median]

    # Loop through the splits and call id3 recursively
    for split in [left_split, right_split]:
        # Call id3 recursively to process each branch
        id3(split, target, columns)

# Create the data set that we used in the example on the last screen
data = pandas.DataFrame([
    [0,20,0],
    [0,60,2],
    [0,40,1],
    [1,25,1],
    [1,35,2],
    [1,55,1]
    ])
# Assign column names to the data
data.columns = ["high_income", "age", "marital_status"]

# Call the function on our data to set the counters properly
id3(data, "high_income", ["age", "marital_status"])
----

== 6. Storing the Tree
* 이제 트리 전체를 중첩 딕셔너리를 이용해 만든다.

== 7. Storing the Tree
[source,python]
----
# Create a dictionary to hold the tree
# It has to be outside of the function so we can access it later
tree = {}

# This list will let us number the nodes
# It has to be a list so we can access it inside the function
nodes = []

def id3(data, target, columns, tree):
    unique_targets = pandas.unique(data[target])

    # Assign the number key to the node dictionary
    nodes.append(len(nodes) + 1)
    tree["number"] = nodes[-1]

    if len(unique_targets) == 1:
        tree["label"] = unique_targets[0]
        return

    best_column = find_best_column(data, target, columns)
    column_median = data[best_column].median()

    tree["column"] = best_column
    tree["median"] = column_median

    left_split = data[data[best_column] <= column_median]
    right_split = data[data[best_column] > column_median]
    split_dict = [["left", left_split], ["right", right_split]]

    for name, split in split_dict:
        tree[name] = {}
        id3(split, target, columns, tree[name])

# Call the function on our data to set the counters properly
id3(data, "high_income", ["age", "marital_status"], tree)
----

== 8. Printing Labels for a More Attractive Tree
* 깊이우선 탐색을 이용해 이쁘게 프린트 한다.

[source,python]
----
def print_with_depth(string, depth):
    # Add space before a string
    prefix = "    " * depth
    # Print a string, and indent it appropriately
    print("{0}{1}".format(prefix, string))


def print_node(tree, depth):
    # Check for the presence of "label" in the tree
    if "label" in tree:
        # If found, then this is a leaf, so print it and return
        print_with_depth("Leaf: Label {0}".format(tree["label"]), depth)
        # This is critical -- without it, you'll get infinite recursion
        return
    # Print information about what the node is splitting on
    print_with_depth("{0} > {1}".format(tree["column"], tree["median"]), depth)

    # Create a list of tree branches
    branches = [tree["left"], tree["right"]]

    for b in branches:
        print_node(b, depth + 1)

print_node(tree, 0)
----

== 9. Making Predictions With the Printed Tree
* 이전 스탭에서 만들어진 트리를 이용해 육안으로 새로운 데이터에 대해 예측해 본다.
* 앞으로도 계속 눈으로 확인 할 수 없으니 자동화 시켜 보자!

== 10. Making Predictions Automatically
[source,python]
----
def predict(tree, row):
    if "label" in tree:
        return tree["label"]

    column = tree["column"]
    median = tree["median"]

    if (row[column] <= median):
        return predict(tree["left"], row)
    else:
        return predict(tree["right"], row)

print(predict(tree, data.iloc[0]))
----

== 11. Making Multiple Predictions
* 여러 행에 대한 예측을 동시에 수행하는 함수를 작성한다.

[source,python]
----
new_data = pandas.DataFrame([
    [40,0],
    [20,2],
    [80,1],
    [15,1],
    [27,2],
    [38,1]
    ])
# Assign column names to the data
new_data.columns = ["age", "marital_status"]

def batch_predict(tree, df):
    return df.apply(lambda x: predict(tree, x),  axis=1)

predictions = batch_predict(tree, new_data)
----

== 12. Next Steps
* 이번 미션에서는 decisino tree model 을 만드고 이를 통해 예측하는 ID3의 수정 버전을 작성했다.
* 다음 미션에서는 큰 데이터셋을 의사결정나무에 적용하는 부분과, 다른 알고리즘과의 트레이드 오프를 배운다.
* 그리고 더욱 정확한 예측을 하는 의사결정나무를 만들기 위한 방법을 배운다.


* image:./images/m1_13_1.png[, 400]
