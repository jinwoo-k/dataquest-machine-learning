= Applying Decision Trees

== 1. Introduction to the Data Set
* 이전 미션에서 사용한 소득 데이터를 그대로 활용한다.
* 이번 미션에서는 Decision Trees를 언제 사용해야하는지와 가장 효과적으로 사용하는 방법에 대해 배운다.

== 2. Using Decision Trees With scikit-learn
* Decision Trees 를 scikit-learn을 이용해 다룬다.
  ** DecisionTreeClassifier : 문제를 분류할때 이용
  ** DecisionTreeRegressor : 문제를 회귀할때 이용

[source,python]
----
from sklearn.tree import DecisionTreeClassifier
columns = ["age", "workclass", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "hours_per_week", "native_country"]

clf = DecisionTreeClassifier(random_state=1)
clf.fit(income[columns], income['high_income'])
----

== 3. Splitting the Data into Train and Test Sets
* overfitting 을 막기 위해 트레이닝셋과 테스트셋을 구분한다.
* 이번 미션에서는 80% 를 트레이닝셋으로 이용하고 나머지를 테스트셋으로 이용한다.

[source,python]
----
import numpy
import math

numpy.random.seed(1)

# Shuffle the rows
income = income.reindex(numpy.random.permutation(income.index))

train_max_row = math.floor(income.shape[0] * .8)

train = income[:train_max_row]
test = income[train_max_row:]
----

== 4. Evaluating Error With AUC
* 분류의 에러를 평가하기 위한 방법으로 AUC 를 이용한다.
  ** ROC 커브(Receiver-Operating Characteristic curve) : 민감도(sensitivity)와 특이도(specificity)의 관계를 나타낸 커브
    *** 축은 FPR(1-Specificity), y 축은 Sensitivity 를 나타내는 그래프
  ** AUC가 0.5라 함은 특이도가 1일때 민감도는 0, 특이도가 0일때 민감도는 1이되는 비율이 정확하게 trade off관계로, 두값의 합이 항상 1이다.
  ** AUC가 0.5보다 큰 경우 특이도와 민감도를 함께 높일 수 있는 최적의 값를 갖는다.
  ** Sensitivity (TPR) : TP / TP + FN
  ** Specificity (TNR) : TN / TN + FP
  ** Precision (PPV) : TP / TP + FP
  ** FPR : FP / FP + TN

[source,python]
----
from sklearn.metrics import roc_auc_score

clf = DecisionTreeClassifier(random_state=1)
clf.fit(train[columns], train["high_income"])

predictions = clf.predict(test[columns])

error = roc_auc_score(test["high_income"], predictions)
print(error)
----

== 5. Computing Error on the Training Set
* 트레이닝셋으로 다시 AUC를 구해 모델이 오버핏 됐는지 확인한다.

[soruce,python]
----
predictions = clf.predict(train[columns])
error = roc_auc_score(train["high_income"], predictions)
print(error)
----


== 6. Decision Tree Overfitting
* 트레이닝셋의 AUC는 .947, 테스트셋의 AUC는 .694 로 오버핏됐을음 확인했다.
* 나무가 깊이가 너무 깊어지는 경우 트레이닝 데이터와 일치하는 지나치게 복잡한 규칙을 만든 경우 새로운 데이터로는 일반화되지 못하는 과적합을 유발한다.
* 적절한 수준으로 가지치기 (아래쪽의 잎 몇 개를 제거)를 해 트레이닝셋의 AUC는 낮지만 테스트셋의 AUC는 기존보다 높게 만들 수 있다.

== 7. Reducing Overfitting With a Shallower Tree
* 오버피팅을 피하는 세가지 방법
  ** 불필요한 잎(leaves) 가지치기(Prune)
  ** 여러 트리의 예측을 조합하는 앙상블
  ** 트리를 만들때 깊이를 제약
* DecisionTreeClassifier는 깊이를 제약하기 위한 여러가지 옵션 제공
  ** max_depth - Globally restricts how deep the tree can go
  ** min_samples_split - The minimum number of rows a node should have before it can be split; if this is set to 2, for example, then nodes with 2 rows won't be split, and will become leaves instead
  ** min_samples_leaf - The minimum number of rows a leaf must have
  ** min_weight_fraction_leaf - The fraction of input rows a leaf must have
  ** max_leaf_nodes - The maximum number of total leaves; this will cap the count of leaf nodes as the tree is being built

[source,python]
----
clf = DecisionTreeClassifier(random_state=1, min_samples_split=13)
clf.fit(train[columns], train["high_income"])

train_predictions = clf.predict(train[columns])
train_auc = roc_auc_score(train["high_income"], train_predictions)
print(train_auc)

test_predictions = clf.predict(test[columns])
test_auc = roc_auc_score(test["high_income"], test_predictions)
print(test_auc)
----

== 8. Tweaking Parameters to Adjust AUC
[source,python]
----
clf = DecisionTreeClassifier(random_state=1, max_depth = 7, min_samples_split = 13)
clf.fit(train[columns], train["high_income"])
predictions = clf.predict(test[columns])
test_auc = roc_auc_score(test["high_income"], predictions)

train_predictions = clf.predict(train[columns])
train_auc = roc_auc_score(train["high_income"], train_predictions)

print(test_auc)
print(train_auc)
----

== 9. Tweaking Tree Depth to Adjust AUC
[source,python]
----
# The first decision tree model we trained and tested
clf = DecisionTreeClassifier(random_state=1, max_depth=2, min_samples_split=100)
clf.fit(train[columns], train["high_income"])
predictions = clf.predict(test[columns])
test_auc = roc_auc_score(test["high_income"], predictions)

train_predictions = clf.predict(train[columns])
train_auc = roc_auc_score(train["high_income"], train_predictions)

print(test_auc)
print(train_auc)
----

== 10. Underfitting in Simplistic Trees
* 마지막 스탭에서 주었던 옵션은 언더피팅이 발생함을 알 수 있다.
* image:./images/m3_10_1.png[, 500]
* image:./images/m3_10_2.png[, 500]
* image:./images/m3_10_3.png[, 350]

== 11. The Bias-Variance Tradeoff
* 트리의 뎁스에 제약을 줘 특정 열에 분류되는 너무 복잡한 모델의 생산을 피할 수 있었다.
* 이러한 제약은 bias-variance tradeoff 를 나타낸다.
  ** 동일한 데이터에 대한 예측이 여러 모델간에 너무 떨어져 있는 경우 높은 분산을 갖는다고 한다.
    ** 모델이 너무 복잡해 특정 케이스에서만 결과를 맞춘다. (overfit)
  ** 동일한 데이터에 대한 예측이 여러 모델간에 가깝지만 실제 값에서 멀리 떨어지면 우리는 높은 편향성을 갖는다고 한다.
    ** 모델이 너무 단순해 제대로 에측하지 못한다. (underfit)
* Decision tree 는 일반적으로 높은 분산을 갖는다.
  ** 의사결정나무에서는 깊이의 제약을 통해 bias-variance tradeoff 에서 최적의 관계를 찾는다. (right fit)

== 12. Exploring Decision Tree Variance
* Decision tree 에 노이드를 도입해 분산을 유도, 어떤일이 발생하는지 알아본다.
* 분산이 큰 모델은 입력 데이터의 작은 변화에 매우 민감하게 반응한다. (overfit)

[source,python]
----
numpy.random.seed(1)

# Generate a column containing random numbers from 0 to 4
income["noise"] = numpy.random.randint(4, size=income.shape[0])

# Adjust "columns" to include the noise column
columns = ["noise", "age", "workclass", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "hours_per_week", "native_country"]

# Make new train and test sets
train_max_row = math.floor(income.shape[0] * .8)
train = income.iloc[:train_max_row]
test = income.iloc[train_max_row:]

# Initialize the classifier
clf = DecisionTreeClassifier(random_state=1)
clf.fit(train[columns], train["high_income"])
predictions = clf.predict(test[columns])
test_auc = roc_auc_score(test["high_income"], predictions)

train_predictions = clf.predict(train[columns])
train_auc = roc_auc_score(train["high_income"], train_predictions)

print(train_auc)
print(test_auc)
----

== 13. Pruning Leaves to Prevent Overfitting
* 이전 스탭에서 본것처럼 랜덤 노이즈 열은 상당한 오버피팅을 초래한다.
* 오버 피팅 (overfitting)을 방지법
  ** 트리가 특정 깊이 이상으로 커지지 않도록 차단
  ** 가지치기 : 트리를 완료한 후 정확도 향상에 도움이 되지 않는 leaves 를 제거
  ** 데이터 과학자는 가지치기보다는 파라미터 최적화와 앙상블을 자주 이용한다.

== 14. Knowing When to Use Decision Trees
* Decision tree 의 장점
  ** 해석하기 쉽다
  ** 비교적 빠르게 적응하여 예측
  ** 여러 유형의 데이터를 처리
  ** 데이터에서 비선형성을 픽업하며, 일반적으로 상당히 정확함
* 단점
  ** 오버핏팅되는 경향을 보임
* Decision tree는 해석이 중요한 일에 사용하면 좋다.
* 랜덤 포레스트
  ** Decision tree 의 오버핏을 줄이는 앙상블 기법 중 하나
  ** 예측 정확도가 가장 중요한 고려 사항 인 경우 일반적으로 랜덤 포리스트의 결과가 더 좋다.
