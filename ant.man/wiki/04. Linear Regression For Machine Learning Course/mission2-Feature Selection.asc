== Feature Selection
=== 1. Mission Values
* 모델 선택 후 적합한 기능선택이 중요
** 특성과 목표의 상관관계, 특성과 특성의 다양성을 사용하는 방법
** 누락값이 없거나 변환할 필요가 없는 값들 (예:집지은 년도)
```
import pandas as pd
data = pd.read_csv('AmesHousing.txt', delimiter="\t")
train = data[0:1460]
test = data[1460:]
numerical_train = train.select_dtypes(include=['int', 'float'])

numerical_train = numerical_train.drop(labels=['PID', 'Year Built', 'Year Remod/Add', 'Garage Yr Blt', 'Mo Sold', 'Yr Sold'], axis=1)

null_series = numerical_train.isnull().sum()
full_cols_series = null_series[null_series == 0]
```

=== 2.Correlating Feature Columns With Target Column
* 이전에 배운것은 pandas.DataFrame.corr() 를 통해 각 컬럼별 관계를 알았냈음 (n^2)
** 바로 전에 full_cols_series 갯수 처럼 실제에서도 모든 특성의 제곱갯수를 모두 다 볼수는 없음 (그래서 결과만 한번에 보기)
```
train_subset = train[full_cols_series.index]
cor=train_subset.corr()
sorted_corrs = cor['SalePrice'].abs().sort_values()
```

=== 3.Correlation Matrix Heatmap
* SalePrice 와 상관관계 열을 순서대로 봄
** 예제로 0.3 이상만 봄 (이 커트라인은 원래 얼마를 할지 실험을 통해 알아내는것이 필요)
** 너무 비슷한 특성인 것은 하나로 축약 (예측 정확도 떨어질 위험)
*** 이것도 corr() 를 사용할 순 있지만 모든 특성을 다 확인 할 수는 없음. Seaborn 을 사용할 예정 (시각적표현)
*** http://seaborn.pydata.org/examples/heatmap_annotation.html[correlation matrix heatmap] , http://seaborn.pydata.org/generated/seaborn.heatmap.html[func heatmap]
image:./images/correlation_heatmap_matrix.png[]

```
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))

strong_corrs = sorted_corrs[sorted_corrs>0.3]
corrmat = train_subset[strong_corrs.index].corr()

sns.heatmap(corrmat)
```
=== 4.Train And Test Model
* 위의 히트맵을 살펴본 후 밀접한 관계 특성 삭제
```
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np

final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])
features = final_corr_cols.drop(['SalePrice']).index
target = 'SalePrice'

#clean_test = test[final_corr_cols.index] ??
clean_test = test[final_corr_cols.index].dropna()

lr = LinearRegression()
lr.fit(train[features], train['SalePrice'])

train_predictions = lr.predict(train[features])
test_predictions = lr.predict(clean_test[features])

train_mse = mean_squared_error(train_predictions, train[target])
test_mse = mean_squared_error(test_predictions, clean_test[target])

train_rmse = np.sqrt(train_mse)
test_rmse = np.sqrt(test_mse)
```
=== 5.Removing Low Variance Features
* 분산이 낮은 특성 삭제: 분산이 낮으면 예측에 의미있는 기여를 안함
** 예를 들어 분산이 0 이라면 모두 같은 값이므로, 예측에 전혀 영향이 없음
** 표준화한 후 이전 연관성처럼 컷오프하여 제거
```
unit_train = train[features]/(train[features].max())
# https://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.var.html
sorted_vars = unit_train.var().sort_values()

```
=== 6.Final Model

* 위에서 계산한 분산이 낮은 'Open Porch SF' 삭제 후 재계산
```
remain_feature = features.drop(['Open Porch SF'])

lr = LinearRegression()
lr.fit(train[remain_feature], train['SalePrice'])

train_predictions2 = lr.predict(train[remain_feature])
test_predictions2 = lr.predict(clean_test[remain_feature])

train_mse2 = mean_squared_error(train_predictions2, train[target])
test_mse2 = mean_squared_error(test_predictions2, clean_test[target])

train_rmse_2 = np.sqrt(train_mse2)
test_rmse_2 = np.sqrt(test_mse2)
```
