= Ordinary Least Squares

== 1. Introduction
* Gradient Descent라는 모델 피팅을 위한 반복적인 기법을 탐구했음
* Gradient Descent 알고리즘은 최적의 매개변수 값에 수렴하기 위해 여러번 반복해야하며 반복 횟수는 초기 매개변수 값과 선택한 학습률에 따라 크게 달라진다.
* 우리는 최소 제곱 추정(OLS추정)이라는 기술을 탐구한다.
* Gradient Descent 와 달리 OLS 추정은 비용함수를 최소화하는 최적의 매개변수값을 직접 계산할 수 있는 명확한 수식을 제공함
* OLS 추정을 이해하기 위해 선형회귀문제를 행렬 형식으로 구성해야한다.

image::./images/4-1.PNG

* 여기서 X는 모델에서 사용하는 트레이닝 집합의 열을 나타내는 행렬이고 a는 매개변수 값을 나타내는 벡터이고 y^는 예측 벡터이다.
* 다음은 OLS 추정 공식

image::./images/4-2.PNG


== 2. Cost Function
* OLS 추정은 최적 매개변수 값을 찾는 문제에 대해 닫힌 형식의 솔루션으로 알려져있다.
* 닫힌 형태의 솔루션은 예측 가능한 양의 수학 연산을 이용해 해를 산술적으로 계산할 수 있는 솔루션
* 접근 방식은 다르지만 Gradient descent와 OLS 추정 방식 둘다 비용 함수를 최소화 하는 높은 수준의 목표를 공유함
* 비용 함수가 행렬 형식으로 표현되는 방법에 대해 알아보기 전에 오류가 어떻게 표현되는지 이해하고 넘어갈 것이다.

image::./images/4-3.PNG

image::./images/4-4.PNG

* Ax = b의 행렬 방정식과 매우 유사하지만 2개의 미지수(벡터 a 와 벡터 y^)를 가지고 있다.
* 비용함수는 다른말로 하면 평균 제곱 오차 이다.
* 비용함수를 행렬형식으로 변경하면 아래와 같다.

image::./images/4-5.PNG

== 3. Derivative Of The Cost Function
* 도함수를 이해하려면 행렬 미적분에 대해 어느정도 친숙해져야 한다.
* 미적분 개념을 행렬에 적용하기 위한 특정 표기법이다.
* 비용 함수의 미분은 아래와 같다.

image::./images/4-6.PNG

* 비용함수 J(a)를 minmize하는 벡터를 찾으려면 도함수 값을 0으로 설정하고 해결한다.

image::./images/4-7.PNG
image::./images/4-8.PNG

* OLS 추정 공식은 아래와 같다.

image::./images/4-9.PNG

== 4. Gradient Descent VS. Ordinary Least Squares
* OLS 추정의 가장 큰 한계는 데이터가 클때 OLS 추정 계산 비용이 많이 든다는 것
* 이는 역행렬을 계산할때 계산 복잡도가 O(n^3) 이기 때문이다.
* OLS는 일반적으로 데이터 집합의 요소 수가 수백만개 미만인 경우에 주로 사용됨
* 더 큰 데이터 셋에서는 Gradient Descent가 훨씬 더 융통성이 있다.


== 예제 모음

=== 1.

[source,python]
----
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv('AmesHousing.txt', delimiter="\t")
train = data[0:1460]
test = data[1460:]

features = ['Wood Deck SF', 'Fireplaces', 'Full Bath', '1st Flr SF', 'Garage Area',
       'Gr Liv Area', 'Overall Qual']

X = train[features]
y = train['SalePrice']

first_term = np.linalg.inv(
        np.dot(
            np.transpose(X),
            X
        )
    )
second_term = np.dot(
        np.transpose(X),
        y
    )
a = np.dot(first_term, second_term)
print(a)
----
