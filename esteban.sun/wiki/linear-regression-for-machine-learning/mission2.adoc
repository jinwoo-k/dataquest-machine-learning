== Feature Selection

 * Missing Values
   ** 이전장에서 다중선형회귀까지 학습함
   ** 이 장에서는 피처와 예측하고자 하는 열과의 상관관계에 대한 심화 학습 및 피처를 선택하기 위해 피처의 분산을 이용하는 방법등에 대해 학습
   ** 누락된 값이 없고, 변환이 필요 없는 피처를 선택하는데 중점을 두고 진행됨

 * Correlating Feature Columns With Target Column
   ** pandas 의 corr 기능을 이용하여 모든 Feature 간 상관계수를 구함
   ** 이때 의미있는 값을 가지는 Feature 가 27개나 되기 때문에 이를 쉽게 확인하기 위해 salePrice 열을 추출하여 상관계수를 Sorting 하여 확인

 * Correlation Matrix Heatmap
   ** 모든 Feature 간 상관관계를 확인하기에는 변수가 많으니 특정 값 이상의 상관관계를 가지는 항목들을 대상으로 처리
      (이때 특정 값은 임의로 선정했으므로 그 이하에 대해서 예측이 더 나아지지 않음을 확인 할 필요가 있음)
   ** Feature 로 사용할 열들간에 상호 연관관계가 큰 경우 *잠재적 공선성* 에 유의할 필요가 있음
   ** 공선성이 있는 Feature 들을 사용시 복제로 인한 예측 정확도 저하가 발생할 수 있으므로 정보 중복을 제거하기 위해 하나의 Feature 만 사용해야 함
   ** 다중공선성(Multicollinearity) 제거 필요 (Collinearity 라고도 함)
      *** 참고 : http://ai-times.tistory.com/268
   ** 상관계수를 확인할 수도 있으나 정보가 많은 경우 heatmap 를 사용하는게 쉽게 인지 됨
   ** Seaborn 기능을 이용하여 Heatmap 을 그릴 수 있음
   ** image:https://s3.amazonaws.com/dq-content/236/correlation_heatmap_matrix.png[] image:./images/image1.png[]

 * Train And Test Model
   ** 위에서 학습했던 공선성 제거, 의미없는값을 제거한 후 모델 실행
   ** Scikit 라이브러리를 이용하여 Train Set 으로 모델을 설정하고 Train/Test Sets 에 대해 가격 예측

 * Removing Low Variance Features
   ** 분산이 낮은 Feature 제거 방법
   ** 분산이 낮으면 값에 대한 변별력이 떨어짐 (예로 분산이 0 = 모든 값이 같음)
   ** 분산의 값을 0~1 로 표준화 하여 임의의 값을 설정하여 해당 값보다 분산이 적은 경우 Feature 에서 제거
   ** 정규화 : 각각의 값을 최대값으로 나누어주면 됨 (모두 양수인 경우)
   ** image:./images/image2.png[]

 * Final Model
   ** 분산이 0.015 보다 적은 Feature 를 제거하고 다시 RMSE 를 계산해 봄
      *** 공선성 제거 + 의미없는값 제거 : 41032.026120197705
      *** Low Variance 제거 : 40591.4270244

 * Next Steps
   ** 지금까지 데이터 셋의 Features 들을 변환/가공 하지 않고 처리할 수 있는 방법에 대한 학습을 진행함
   ** 이후에는 다른 모델 및 정확도를 향상 시키기 위한 기존 features 로 부터 새로운 feature 정의 및 가공에 대해 학습

== 활용 주요 function
 * 라이브러리 사용 선언
   ** import pandas as pd
   ** import numpy as np
   ** import matplotlib.pyplot as plt
   ** import seaborn
      *** Seaborn은 matplotlib을 기반으로 다양한 색상 테마와 통계용 챠트 등의 기능을 추가한 시각화 패키지
      *** matplotlib 패키지에 의존하며 통계 기능의 경우에는 statsmodels 패키지에 의존
      *** 참고 : http://stanford.edu/~mwaskom/software/seaborn/index.html
   ** from sklearn.metrics import mean_squared_error
   ** from sklearn import linear_model
 * function
   ** len : 배열(객체)의 크기
   ** pd.dataframe.corr() : 파라미터의 열들간의 상관계수를 구함, 파라미터가 없는 경우 전체에 대해서 계산
   ** seaborn.heatmap() : 파라미터로는 히트맵을 그릴 x, y 피벗테이블을 전달
   ** linear_model.fit : 예측에 사용할 Feature들과 예측할 열을 파라미터로 전달하여 모델을 생성
   ** linear_model.predict : 예측에 사용할 데이터 셋을 전달하여 예측값 생성
   ** mean_squared_error(값1, 값2) : 두 값간의 평균 제곱 오차를 구함
   ** dataframe.select_dtypes(include=['int', 'float']) : 특정 타입에 해당하는 열들만 선택
   ** dataframe.isnull() : null 인 항목들
   ** dataframe[조건] : 조건에 맞는 데이터 추출
   ** dataframe[indexes].dropna() : indexes 들의 데이터중 null 인 row 제거
   ** dataframe.min(), dataframe.max(), dataframe.sum() : 최소값, 최대값, 합계
   ** dataframe.var() : 분산
   ** dataframe.sort_values() : 정렬
   ** fig = plt.figure(figsize=(30,45)) : plot 사이즈 선언 (인치)
   ** fig.add_subplot(3, 1, 1) : 선언된 figure 에 서브 플롯을 선언
   ** 참고 : https://matplotlib.org/api/figure_api.html

