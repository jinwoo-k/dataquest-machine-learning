## Cross Validation
Learn how to use k-fold cross validation to perform more rigorous testing.

### Introduction

hold out 유효성체크

. training / test set 으로 나눔
. training 함
. 이걸로 test set 예측
. error metric 계산
. trainig / test set 을 바꿔서 위를 반복
. 에러 평균을 구함

핵심은 이전에 75:25 (3:1) 로 나누지 않고 50:50 으로 한 뒤 확인
image:https://s3.amazonaws.com/dq-content/holdout_validation.png[width:"500px"]

### Holdout Validation
실제 모델 두 번 계산 후 평균

### K-Fold Cross Validation
holdout 유효성 : k-fold cross-validation 의 유효성 테크닉의 큰 클래스 예

holdout 이 traning/test 보다 좋다. -> 편향이 적기떄문 (대신 반만 사용)

K-Fold Cross validation: 좀더 많은 부분 사용하면서 training/test의 이슈를 줄이기 위해 데이터의 부분을 돌린다.

. 전체에서 k 만큼 길이의 부분을 나눔 (k-1 개는 training vs 1 개는 test set)
. training set 에서 계산하여 error metric 만듦
. k-1 번 반복
. k 개 error 평균 구함

image:https://s3.amazonaws.com/dq-content/kfold_cross_validation.png[width="500px"]

> holdout validation은 k-fold cross validation에서 k = 2 (보통 k는 5 or 10)

fold 증가 -> 각각 fold 의 관찰수 감소 -> 폴드간 분산 에러 증가??

```
dc_listings.set_value(dc_listings.index[0:744], "fold", 1)
```

### First Iteration
1 을 test set 으로 , 2~5 으로 training

```
train = dc_listings[dc_listings['fold'] != 1]
test = dc_listings[dc_listings['fold'] == 1]
```
### Function For Training Models
loop 돌며 위에 한번했던 것 반복 > 결과 rmse 평균

### Performing K-Fold Cross Validation Using Scikit-Learn
일반적으로 함수 만드려면 임의화와 순서화를 해야함

scikit 활용하여 hyperparameter 를 조절하여 최적의 k-nearest neighbor model 을 구축
