= Gradient Descent

== 1.Introduction
    * 이번 미션과 다음에서 선형회귀모델에 대한 최적의 매개 변수 값을 찾는 가장 일반적인 두 가지 방법에 대해 설명한다.
      ** Gradient Descent
      ** Ordinary Least Squares (OLS)
    * Model Fitting
      ** 고유 한 매개 변수 값의 각 조합은 고유한 선형 회귀 모델을 형성
      ** 이러한 최적 값을 찾는 프로세스를 모델 적합이라고 한다.

    * 모델 피팅에 대한 두 가지 접근 방식은 다음 함수를 최소화하는 것을 목표로 한다.

image:./images/m3-1.PNG[]

      ** 이 함수는 주어진 모델을 사용하여 만들어진 예측 된 레이블과 실제 레이블 간의 평균 제곱 오차입니다.
      ** 다른 함수를 최소화하거나 최대화하는 값 집합을 선택하는 문제를 최적화 문제라고합니다.


== 2. Single Variable Gradient Descent
   * 그라디언트 디센트 알고리즘
     **   평균제곱 오류가 가장 낮은 모델이 발견 될 때까지 다른 매개 변수 값을 반복적으로 시도
     **  그라디언트 디센트는 신경망과 같은 다른 모델에도 일반적으로 사용되는 최적화 기술
    * 단일 매개 변수 선형 회귀 모델에 대한 그래디언트 디센트 알고리즘의 개요
      ** 파라메터 초기값 선택 : a1
      ** 수렴 될 때까지 반복 (일반적으로 최대 반복 횟수로 구현).
        *** 현재 매개 변수 값을 사용하는 모델의 오류 (MSE)를 계산.  image:./images/m3-2.PNG[]
        *** 현재 매개 변수 값에서 오류의 미분을 계산. image:./images/m3-3.PNG[]
        *** 미분 곱하기 a 상수 (α, 학습 속도라고 함)를 뺀 값을 업데이트. image:./images/m3-4.PNG[]
       ** 적절한 초기 매개 변수 및 학습률을 선택하면 수렴에 필요한 반복 횟수가 줄어든다.


== 3. Derivative Of The Cost Function
    * 수학적 최적화에서 우리가 최소화를 통해 최적화하는 함수는 비용 함수 또는 때로는 손실 함수라고 함.


[source,python]
----
def derivative(a1, xi_list, yi_list):
    sum = 0
    n = len(xi_list)
    for i in range(0, n):
        sum = sum + xi_list[i] * (a1*xi_list[i] - yi_list[i])
    return sum * 2 / n

def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial):
    a1_list = [a1_initial]

    for i in range(0, max_iterations):
        a1 = a1_list[i]
        deriv = derivative(a1, xi_list, yi_list)
        a1_new = a1 - alpha*deriv
        a1_list.append(a1_new)
    return(a1_list)

# Uncomment when ready.
param_iterations = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150)
print(param_iterations)
----

== 4. Understanding Multi Parameter Gradient Descent
   * 다중 매개 변수 그래디언트 디센트에 대한 직관적 이해를 해보자.
   * 다음 간단한 선형 회귀 모델에 대한 매개 변수 값의 함수로 MSE를 시각화하여 시작.

image:./images/m3-5.PNG[]

image:./images/m3-6.PNG[]

   * a0 on the x-axis
   * a1 on the y-axis
   * MSE on the z-axis


== 5. Gradient Of The Cost Function
   * Cost Function

image:./images/m3-7.PNG[]

   * 미분함수

image:./images/m3-8.PNG[]

image:./images/m3-9.PNG[]

    * 정답 수정본 및 MSE 계산.

```
def a1_derivative(a0, a1, xi_list, yi_list):
    len_data = len(xi_list)
    error = 0
    for i in range(0, len_data):
        error += xi_list[i]*(a0 + a1*xi_list[i] - yi_list[i])
    deriv = 2*error/len_data
    return deriv

def a0_derivative(a0, a1, xi_list, yi_list):
    len_data = len(xi_list)
    error = 0
    for i in range(0, len_data):
        error += a0 + a1*xi_list[i] - yi_list[i]
    deriv = 2*error/len_data
    return deriv

def gradient_descent(xi_list, yi_list, max_iterations, alpha1, alpha0, a1_initial, a0_initial):
    a1_list = [a1_initial]
    a0_list = [a0_initial]

    for i in range(0, max_iterations):
        a1 = a1_list[i]
        a0 = a0_list[i]

        a1_deriv = a1_derivative(a0, a1, xi_list, yi_list)
        a0_deriv = a0_derivative(a0, a1, xi_list, yi_list)

        a1_new = a1 - alpha1*a1_deriv
        a0_new = a0 - alpha0*a0_deriv

        a1_list.append(a1_new)
        a0_list.append(a0_new)
    return(a0_list, a1_list)

a0_params, a1_params = gradient_descent(
    train['Gr Liv Area'], train['SalePrice'],500, 0.00000003,0.0016, a1_initial= 110,a0_initial =5000)

#print(a0_params)
#print(a1_params)

def getMse(a0,a1,xi_list,yi_list):
    y_hat_list = a0 + a1*xi_list
    n = len(yi_list)
    sum = 0.0
    for i in range(0, n-1):
        sum +=  (yi_list[i] - y_hat_list[i])**2
    return(sum/n)

n = len(a0_params)
a0 = a0_params[n-1]
a1 = a1_params[n-1]

mse = getMse(a0,a1, train['Gr Liv Area'], train['SalePrice'])
print(a0)
print(a1)
print(mse)

```

== 6. Gradient Descent For Higher Dimensions
    * 그라디언트 디센트는 실제로 원하는만큼 많은 변수에 맞게 조절됩니다.
    * 각 매개 변수 값에는 자체 업데이트 규칙이 필요하며 a1의 업데이트 규칙과 거의 일치함

image:./images/m3-10.PNG[]

image:./images/m3-11.PNG[]

== 7. Next Step

 * 이 미션에서 우리는 그래디언트 디센트 알고리즘을 사용하여 선형 회귀 모델을 찾는 방법을 모색했습니다.
 * 그라데이션 강하와 관련된 주요 과제는 다음과 같습니다.
   ** 양호한 초기 매개 변수 값 선택하기
   ** 좋은 학습 속도 선택 (하이퍼 매개 변수 최적화 영역에 속함)
 *  다음 미션에서는 매개 변수 또는 하이퍼 매개 변수 값 선택이 필요하지 않은 OLS 추정이라는 기술을 살펴 보겠음.

