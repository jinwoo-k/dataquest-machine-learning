= Introduction To Evaluating Binary Classifiers

== 1. Introduction to the Data
* 분류, 로지스틱 회귀, 대학원 입시의 데이터 셋에 로지스틱 회귀를 맞추기 위해 scikit-learn을 사용하는 방법을 배웠다.

== 2. Accuracy
* predict_label열과 해당열의 실제 값으로 모델이 트레이닝 데이터에 얼마나 효과적이었는지 계산할 수 있다.
* 분류 모델의 효율성을 결정하는 방법은 예측 정확도이다.
** 어느부분의 예측이 가장 정확했는지?
* 예측 정확도는 올바르게 예측된 레이블 수를 전체 수로 나눈 값이다.
image::./images/2-1.PNG[정확도]

== 3. Binary classfication outcomes
* 예측 정확도가 많은것을 말해주지는 않는다.
* 정확성은 트레이닝 되지 않은 데이터에 대한 모델의 수행방식을 알려주지 않는다.
* 트레이닝 셋에서 평가할때 100% 정확도를 반환하는 모델은 모델이 이전에 전혀 본적 없는 데이터에 얼마나 효과적인지는 알려주지 않음
* 이 미션에서는 모델의 효과를 테스트해서 바이너리 분류 모델을 평가하도록 한다.
image::./images/2-2.PNG[분류]
* 모델의 예측은 단순한 정확도보다 더 세분화된 다른 효율성 측정 방법에 대해 생각해볼 수 있다.
** True Positive - 이 모델은 학생이 입학할 것이라고 정확하게 예측
** True Negative - 이 모델에서 학생이 거부될 것으로 정확하게 예측
** False Positive - 이 모델은 학생이 실제로 거부되었더라도 학생이 입학할 것이라고 예측
** False Negative - 이 모델은 학생이 실제로 입학했음에도 불구하고 거부될 것이라고 예측

== 5. Sensitivity
* 입학한 학생 비율
image::./images/2-3.PNG[비율]
* 긍정적인 결과를 확인하는데 이 모델이 얼마나 효과적인가?
* True Positive Rate가 낮으면 모델이 긍정적인 경우를 잡는데 효과적이지 않다는 의미

== 6. Specificity
* 앞서 계산했듯이 모델의 민감도가 12.7%이다.
* 학생 입학을 예측하는 맥락에서는 너무 나쁜 결과는 아니다.
* 하지만 의료환경에서는 민감도가 낮으면 심각할 수 있다.
* 특이성 또는 True Negative Rate - 올바르게 거절된 신청자의 비율
image::./images/2-4.PNG[비율]
* 이 모델은 부정적인 결과를 식별하는데 얼마나 효과적인가?

== 7. Next Steps

== 예제 모음

=== 1.

[source,python]
----
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

admissions = pd.read_csv("admissions.csv")
model = LogisticRegression()
model.fit(admissions[["gpa"]], admissions["admit"])
admissions = pd.read_csv("admissions.csv")
model = LogisticRegression()
model.fit(admissions[["gpa"]], admissions["admit"])

labels = model.predict(admissions[["gpa"]])
admissions["predicted_label"] = labels
print(admissions["predicted_label"].value_counts())
print(admissions.head())
----


=== 2.

[source,python]
----
admissions["actual_label"] = admissions["admit"]
matches = admissions["predicted_label"] == admissions["actual_label"]
correct_predictions = admissions[matches]
print(correct_predictions.head())
accuracy = len(correct_predictions) / len(admissions)
print(accuracy)
----

=== 4.

[source,python]
----
true_positive_filter = (admissions["predicted_label"] == 1) & (admissions["actual_label"] == 1)
true_positives = len(admissions[true_positive_filter])

true_negative_filter = (admissions["predicted_label"] == 0) & (admissions["actual_label"] == 0)
true_negatives = len(admissions[true_negative_filter])

print(true_positives)
print(true_negatives)
----


== 5.

[source,python]
----
# From the previous screen
true_positive_filter = (admissions["predicted_label"] == 1) & (admissions["actual_label"] == 1)
true_positives = len(admissions[true_positive_filter])
false_negative_filter = (admissions["predicted_label"] == 0) & (admissions["actual_label"] == 1)
false_negatives = len(admissions[false_negative_filter])

sensitivity = true_positives / (true_positives + false_negatives)

print(sensitivity)
----


== 6.

[source,python]
----
# From previous screens
true_positive_filter = (admissions["predicted_label"] == 1) & (admissions["actual_label"] == 1)
true_positives = len(admissions[true_positive_filter])
false_negative_filter = (admissions["predicted_label"] == 0) & (admissions["actual_label"] == 1)
false_negatives = len(admissions[false_negative_filter])
true_negative_filter = (admissions["predicted_label"] == 0) & (admissions["actual_label"] == 0)
true_negatives = len(admissions[true_negative_filter])
false_positive_filter = (admissions["predicted_label"] == 1) & (admissions["actual_label"] == 0)
false_positives = len(admissions[false_positive_filter])
specificity = (true_negatives) / (false_positives + true_negatives)
print(specificity)
----
