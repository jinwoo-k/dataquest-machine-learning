= Ordinary Least Squares

== 1. Introduction
    * 그래디언트 디센트와 달리 OLS 예측은 비용 함수를 최소화하는 최적의 매개 변수 값을 직접 계산할 수있는 명확한 수식을 제공
    * OLS 추정을 이해하기 위해서는 먼저 선형 회귀 문제를 행렬 형식으로 구성해야합니다.
    * 우리는 대부분 선형 회귀 모델의 다음 형식으로 작업했습니다.
image:./images/m4-1.PNG[]
    * 행렬 표기법을 사용하여 많은 변수가있는 선형 시스템에 대해 더 잘 나타내고 추론 할 수있는 방법을 모색했습니다.
    * 선형 회귀 모형의 행렬 형태는 다음과 같습니다.

image:./images/m4-2.PNG[]

    * 여기에서 X는 모델이 사용하는 트레이닝 집합의 열을 나타내는 행렬이고,
    * a는 매개 변수 값을 나타내는 벡터이고
    * y ^는 예측 벡터입니다.
    * 최적 벡터 A를 산출하는 OLS 예측 공식

image:./images/m4-3.PNG[]

== 2. Cost Function
    * 그라디언트 디센트와 달리 OLS 예측은 최적 매개 변수 값을 찾는 문제에 대한 닫힌 형식 솔루션으로 알려져 있습니다.
    * 닫힌 폼 솔루션은 예측 가능한 양의 수학 연산을 사용하여 해를 산술적으로 계산할 수있는 솔루션
    * 반면, Gradient descent는 초기 매개 변수 값, 학습 속도 등을 기준으로 다른 수의 반복 (따라서 수학 연산의 수를 달라야 함)을 요구할 수있는 알고리즘 방식.
    * 비용 함수가 행렬 형식으로 표현되는 방법에 대해 알아보기 전에 오류가 어떻게 표현되는지 이해합시다.
    * 오류는 모델 y ^와 실제 레이블 y를 사용하여 만들어진 예측의 차이이기 때문에 벡터로 표시됩니다.
    * E (epsilon  ε)에 대한 그리스 문자는 종종 오차 벡터를 나타 내기 위해 사용됩니다.

image:./images/m4-4.PNG[]

    * 행렬 형식의 비용함수는 다음과 같다.

image:./images/m4-5.PNG[]

== 3. Derivative Of The Cost Function

  * 행렬 형식 비용함수의 미분

image:./images/m4-6.PNG[]

image:./images/m4-7.PNG[]

== 4. Gradient Descent vs. Ordinary Least Squares
    *  OLS의 한계.
      ** 데이터가 클 때 OLS 추정이 계산 비용이 많이 든다는 것
      ** OLS는 일반적으로 데이터 집합의 요소 수 (따라서 반전 된 행렬)가 수백만 개 요소 미만인 경우에 주로 사용
    * Gradient Descent
      ** 큰 데이터 세트에서는 그라디언트 디센트가 훨씬 더 융통성이 있기 때문에 사용됩니다.
      ** 실용적인 많은 문제에 대해 우리는 임계 값 정확도 값 (또는 설정된 반복 횟수)을 설정하고 "충분히 좋은" 해결책을 사용할 수 있다

