= Introduction To Decision Trees
Learn about the building blocks of decision trees, including entropy and information gain.

== 1. Introduction
* Decision tree
  ** 매우 강력하고 대중적인 머신러닝 테크닉이다.
  ** 기본적인 컨셉은 의사결정을 돕는 트리와 유사하다.
  ** Supervised learning 알고리즘
  ** 이력 데이터로 트리를 구성한 다음 이를 사용하여 결과예측
  ** 선형회귀와 달리 데이터에서 변수 간 비선형 상호작용에도 이용 가능
  ** 분류나 회귀 문제에 대해서도 사용 가능

== 2. Overview of the Data Set
* 미국의 개인 소득 데이터 셋 사용
  ** 1994 년 인구 조사 결과로, 혼인 상태, 나이, 직업 유형 등에 대한 정보 포함
  ** 타겟은 소득이 연간 50k 이상 / 이하인지 분류

== 3. Converting Categorical Variables
* 문자형 데이터를 카테고리형 데이터로 변환한다.
  ** pandas 의 categorical 타입을 이용할 수 있지만, 다른 라이브러리와의 호환성을 위해 그냥 숫자형으로 변환한다.

[source,python]
----
for i in ['workclass', 'education', 'marital_status', 'occupation', 'relationship',
          'race', 'sex', 'native_country', 'high_income']:
    col = pandas.Categorical.from_array(income[i])
    income[i] = col.codes

print(income.head(5))
----

== 4. Splitting Data
* Decision tree 는 일련의 노드와 분기로 구성
  ** 노드는 변수를 기반으로 데이터를 분할
  ** 브랜치는 노드에 의해 분할된 한쪽을 의미

== 5. Creating Splits
* 변수에 의한 분할을 직접 수행해본다.

[source,python]
----
private_incomes = income[income['workclass'] == 4]
public_incomes = income[income['workclass'] != 4]
----

== 6. Decision Trees as Flows of Data
* tree 의 깊이가 깊어질 수 록 남는 rows는 적어진다.

== 7. Splitting Data to Make Predictions
* 트리의 맨 아래에있는 노드를 terminal nodes 또는 leaves 라 부른다.
* 미지의 데이터에 대해 예측을 하려면 모든 종단점은 target 컬럼에 대해 하나의 값만 가져야 한다.
  ** 우리의 경우 high_income 컬럼의 값이 모두 동일해야 한다. 즉 하나의 값만 남을 때 까지 분할한다.
  ** 예측을 하기 위해서는 새로운 행을 가져오고, 이를 의사결정나무에 통과하게 한다.

== 8. Overview of Data Set Entropy
* 분할은 가능한 한 high_income 열에서 많은 1과 0의 분리가 이뤄지도록 한다.
* 엔트로피 (무질서도)
  ** 1과 0이 "mixed together" 될 수록 엔트로피가 높다라고 하고, high_income 열에 1을 완전히 포함하는 데이터 세트는 엔트로피가 낮다고 한다.
  ** 물리학에서의 엔트로피와는 다른 개념으로 정보 이론에서 나온 개념
  ** 1 비트는 하나의 정보 단위입니다.
  ** image:./images/m1_8_1.png[, 400]

== 9. Overview of Data Set Entropy

[source,python]
----
t = len(income)
count_0 = len(income[income['high_income'] == 0])
count_1 = len(income[income['high_income'] == 1])

income_entropy = -(count_0/t * math.log(count_0/t, 2) + count_1/t * math.log(count_1/t, 2))
print(income_entropy)
----

== 10. Information Gain
* 정보 획득(information gain)
  ** 어떤 변수를 분할에 사용할지 찾기 위해 엔트로피 계산이 필요
  ** 엔트로피를 가장 줄이는 분할이 어떤건지 알 수 있음

* image:./images/m1_10_1.png[, 400]
  ** 1. 엔트로피 T 를 계산
  ** 2. 변수 A 에 대해 각각의 고유값 v 에 대해 v 값을 갖는 열 수를 계산. 그리고 전체열로 나눔
  ** 3. A 변수가 v 값을 갖는 열들의 엔트로피를 곱함
* 모든 고유 값에 대해 엔트로피를 구하지 않고, 중위값을 이용해 엔트로피를 구하는 방법도 있다.
  ** image:./images/m1_10_2.png[, 600]
*

== 11. Information Gain
* age 컬럼에 대한 정보획득량을 구한다.
[source,python]
----
median = income['age'].median()
left_branch = income[income['age'] <= median]
ratio_left = len(left_branch) / len(income)
entropy_left = calc_entropy(left_branch['high_income'])

right_branch = income[median < income['age']]
ratio_right = len(right_branch) / len(income)
entropy_right = calc_entropy(right_branch['high_income'])

total_entropy = calc_entropy(income['high_income'])

age_information_gain = total_entropy - ((ratio_left * entropy_left) + (ratio_right * entropy_right))
print(age_information_gain)
----

== 12. Finding the Best Split
* 최대의 정보획득이 가능한 변수를 찾는다.
[source,python]
----
def calc_information_gain(data, split_name, target_name):
    """
    Calculate information gain given a data set, column to split on, and target
    """
    # Calculate the original entropy
    original_entropy = calc_entropy(data[target_name])

    # Find the median of the column we're splitting
    column = data[split_name]
    median = column.median()

    # Make two subsets of the data, based on the median
    left_split = data[column <= median]
    right_split = data[column > median]

    # Loop through the splits and calculate the subset entropies
    to_subtract = 0
    for subset in [left_split, right_split]:
        prob = (subset.shape[0] / data.shape[0])
        to_subtract += prob * calc_entropy(subset[target_name])

    # Return information gain
    return original_entropy - to_subtract

# Verify that our answer is the same as on the last screen
print(calc_information_gain(income, "age", "high_income"))

columns = ["age", "workclass", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "hours_per_week", "native_country"]

information_gains = []
for c in columns:
    information_gains.append(calc_information_gain(income, c, 'high_income'))

highest_gain = columns[information_gains.index(max(information_gains))]
print(highest_gain)
----

== 13. Build the Whole Tree
* image:./images/m1_13_1.png[, 400]
  ** 나이가 50인 경우 동일한 값에 high_income 이 0 과 1 모두 존재한다. 이런경우 0.5로 끝낸다.
* 위와 같은 형식으로 모든 트리를 만든다.

== 14.
* 지금까지 의사 결정 트리를 구성하기 위해 ID3 알고리즘을 따라했다.
* 분할 기준에 대해 다른 측정 값을 사용하는 CART와 같은 다른 알고리즘이 있다.
* 다음 임무에서는 ID3 알고리즘을 사용하여 전체 트리를 재귀적으로 생성하는 방법을 알아 본다.
