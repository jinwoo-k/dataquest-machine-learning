== Feature Selection
=== 1. Mission Values
* 모델 선택 후 속성선택이 중요
 ** 속성과 목표의 상관관계
 ** 속성간의 상관관계
 ** 속성의 다양성

=== 2.Correlating Feature Columns With Target Column
* 이전에 배운것은 pandas.DataFrame.corr() 를 통해 각 컬럼별 관계를 알았냈음 (n^2)
** 실제에서도 모든 특성의 제곱갯수를 모두 다 볼 수는 없음
** 대신 속성이 목표와 어떻게 관련되는지에 초점을 맞춤.

=== 3.Correlation Matrix Heatmap
* SalePrice 와 상관관계 열을 순서대로 봄
** 너무 비슷한 특성인 것은 하나로 축약 (예측 정확도 떨어질 위험)
** corr() 를 사용할 순 있지만 모든 특성을 다 확인 할 수는 없음.

*** http://seaborn.pydata.org/examples/heatmap_annotation.html[correlation matrix heatmap]
*** http://seaborn.pydata.org/generated/seaborn.heatmap.html[func heatmap]

image:./images/heatmap.png[]

=== 4.Train And Test Model
* 위의 히트맵을 살펴본 후 밀접한 관계 특성 삭제

=== 5.Removing Low Variance Features
* 분산이 낮은 특성 삭제: 분산이 낮으면 예측에 의미있는 기여를 안함
** 예를 들어 분산이 0 이라면 모두 같은 값이므로, 예측에 전혀 영향이 없음
** 표준화(예제에서는 최대값으로 나눔)한 후 컷오프하여 제거

=== 6.Final Model
* 위에서 계산한 분산이 낮은 'Open Porch SF' 삭제 후 재계산
* 최종 모델의 rmse 값

[width="100%", options="header"]
|=======
| rmse | 단순선형회귀모델 | 최종모델
| train_rmse | 56034.3620014 | 34372.6967078
| test_rmse  | 57088.2516126 | 40591.4270244
|=======

* 참고
 ** 결정계수(R2)

image:./images/R2-1.png[]

image:./images/R2-2.png[]

 ** VIF(분산 팽창 계수) : VIF = 1 / (1 - R2)
   *** 각 설명변수에 대하여 VIF를 계산
   *** 원래 종속변수는 제외하고 특정 설명변수를 종속변수로 두고 나머지 설명변수로 R2를 계산 구한다
   *** VIF > 10 이면 다중공선성이 존재하는 것으로 판단
 ** http://wiki.daumkakao.com/pages/viewpage.action?pageId=306744525[(실무 적용의 예) 다음 vs TMAP 길찾기 결과 소요시간 편차 최소화]