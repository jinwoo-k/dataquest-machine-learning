== Introduction To Decision Trees
=== 1. Introduction
* 의사 결정 트리 (decision tree): 의사결정을 돕기 위해 흔히 사용되는 나무와 비슷
** 예: image:./images/ex_decisiontree.png[width="400px"]
* 의사결정 트리는 supervised (감독) 학습 알고리즘
** supervised (지도) 학습 알고리즘? 입력/출력 샘플데이터를 모두 넣고 처음보는 데이터에 대한 정확한 출력을 예측
** 이와 반대인, unsupervised 학습 알고리즘이란? 데이터가 어떻게 구성되었는지 알아내는 문제의 범주 -> 무리로 나누는 것
* 의사결정 트리의 주요 이점: 선형 회귀가 수행할 수 없는 데이터에서 변수간 비선형 상호작용을 선택 가능
* 결정 트리는 탈출 불가능 시 , 큰곰과 싸움 vs 선형회귀는 이 경우 두 요인의 가중치를 증가 시킴
* 분류나 회귀 문제가 tree 사용 가능

=== 2. Overview of the Data Set
* 미국의 개인소득에서 개인이 연간 50k 이하 혹은 그 이상을 생산하는 지 예측

=== 3. Converting Categorical Variables
* 카테고리형 변수 존재 -> 숫자 변형

```
cate_col = ["education", "marital_status", "occupation", "relationship", "race", "sex", "native_country", "high_income"]

for col_name in cate_col:
    col = pandas.Categorical.from_array(income[col_name])
    income[col_name] = col.codes
```

=== 4. Splitting Data
* 의사결정 트리의 구성: 노드와 분기
** 노드는 변수를 기반으로 데이터를 분할하는 지점, 분기는 분할의 한면
** image:./images/ex_decisiontree2.png[width="400px"]
*** 노드는 private sector 에 대해 두 개 분기로 분할
*** private sector 코드값은 4 이므로 workclass != 4 는 No, workclass == 4 는 Yes
*** 지금 트리의 레벨은 2이다.

=== 5. Creating Splits
```
private_incomes = income[income["workclass"] == 4]
public_incomes = income[income["workclass"] != 4]
```

=== 6. Decision Trees as Flows of Data
* 분할하면 행이 오른쪽/왼쪽으로 나뉘게 됨. 트리를 깊게 만들수록 각 노드는 적은 수의 행을 가짐
image:./images/decision_tree_size.png[width="400px"]

=== 7. Splitting Data to Make Predictions
* 종단노드 or 종단: 분할을 중지하기로한, 결정된 맨 아래 노드
** 목표가 있으므로 각 노드에 목표열에 대한 값만 있어야 함
** 우리의 목표는 high_income (연간 50k 초과인지 이하인지)
** 결론적으로, 종단은 high_income 에 대해 0,1 둘의 값을 가질 수 없다.

=== 8. Overview of Data Set Entropy
* 작동원리 및 세부정보를 탐색하고 분할 수행하는 방법
** 특정 척도를 사용해 분할하는 변수 파악
** 우리는 high_income 이 0 or 1 만 가지는 나뭇잎만 가지도록 분할하기 때문에 각 분할은 목표에 가깝게 해야 함

* 엔트로피: 측정기준, 무질서를 지칭함.
** 혼합된.. 0과1이 많을 수록 엔트로피 높아짐, high_income 에 1만 포함하면 엔트로피가 낮음
** 정보는 이진수로 표현 (bit), 동전의 앞뒤 혹은 날씨의 맑음의 확률 (확률이 같다고 가정)
*** 동전이 모두 앞면만 있다면 뒤집어서 얻을 새로운 정보는 없으므로 엔트로피는 0
** 공식 : image:./images/entropy_formular.png[width="200px"]
*** 단일 열의 각 고유값(이 경우 high_income)을 반복하여 i 에 할당. 그다음 데이터 P(xi) 에서 그 값이 발생할 확률 계산 (로그 밑수는 일반적으로 2 사용. 10이나 다른값 가능)
```
age    high_income
25     1
50     1
30     0
50     0
80     1
<위 데이터로 계산>
- (2/5 * log2/5 + 3/5 * log 3/5)
= - (-0.528 - 0.442) = 0.97
```
*** 위 샘플데이터에서 0보다 약간 많은 1이 있기 떄문에 0.97의 정보만 가져오게 됨
*** 새 값을 예측할 경우 대답은 1이 되고, 원래의 대답확률이 0.6 이기 때문에 잘못된 예측이 될 가능성이 높음
*** -> 이 사전지식으로 새 값을 확인 시 완전한 비트를 못얻음
** ref: https://www.youtube.com/watch?v=zJmbkp9TCXY[엔트로피], https://en.wikipedia.org/wiki/Entropy[엔트로피]

=== 9. Overview of Data Set Entropy
```
entropy = -(2/5 * math.log(2/5, 2) + 3/5 * math.log(3/5, 2))
print(entropy)
prob_0 = income[income["high_income"] == 0].size / income.size
prob_1 = income[income["high_income"] == 1].size / income.size
income_entropy = -(prob_0 * math.log(prob_0, 2) + prob_1 * math.log(prob_1, 2))
```

=== 10. Information Gain
* 엔트로피 계산서 변수 분리방법 파악필요. 정보분할을 통해 가능. -> 분할은 엔트로피를 가장 많이 줄이는 부분을 알려줌
** 정보획득공식 : image:./images/information_gain_formular.png[width="400px"]
*** 공식요약: 목표변수(T) 에 대한 정보이득 (IG)와 분할하고자 하는 변수 (A)를 계산하는 식
. 위를 계산하기 위해 T 에 대한 엔트로피 계산
. 변수 A에 각 고유값 v 에 대해 A 가 값 v 를 취하는 행 수를 계산 후 총 행수로 나눔
. 결과에 A 가 v 인 행의 엔트로피를 곱함.
. 모든 하위 엔트로피를 더한 후 전체 엔트로피에서 뺴서 정보 얻기
*** 다른 설명: 분할 후 각 집합의 엔트로피를 찾고, 각 분할의 항목 수로 가중치를 부여 후 현재 엔트로피에서 빼기
*** 위의 결과가 양수면 분할로 엔트로피를 낮춤. 결과가 높을수록 엔트로피가 낮아짐
* 트리 구성하기 위한 전략은 각 노드에 분기를 많이 만드는 것
** 변수에 3 ~ 4 개의 값이 있으면 3 ~ 4 개의 분기로 끝남
** 이 방법은 더 복잡하고 예측정확도를 향상시키진 않지만 할만한 가치는 있음
* 정보 획득 계산을 단순화 하기 위해 각 고유값에 대해 계산하지 않는다.
** 대신에 분할할 변수의 중간값을 사용한다.
** 변수의 중앙값보다 낮은 모든 행을 왼쪽으로 분기, 나머지는 오른쪽으로 분기
** 정보 획득을 계산하기 위해 하위 집합에 대한 엔트로피만 계산하면 됨
```
age    high_income
25     1
50     1
30     0
50     0
80     1
```
* 위 예제 계산
. 먼저 중위 age 구하기 (여기선 50). 그다음 그 값보다 작거나 같으면 0 나머지는 1을 새로운 컬럼 (split_age)에 넣기
age    high_income    split_age
25     1              0
50     1              0
30     0              0
50     0              0
80     1              1
. 이제 엔트로피 계산
image:./images/entropy_example.png[]
** 계산 후 얻은 값 "0.17". 이 의미는 나이 변수 데이터 셋에서 0.17 비트의 정보를 얻음을 의미

=== 11. Information Gain
```
import numpy

def calc_entropy(column):
    """
    Calculate entropy given a pandas series, list, or numpy array.
    """
    # Compute the counts of each unique value in the column
    counts = numpy.bincount(column)
    # Divide by the total column length to get a probability
    probabilities = counts / len(column)

    # Initialize the entropy to 0
    entropy = 0
    # Loop through the probabilities, and add each one to the total entropy
    for prob in probabilities:
        if prob > 0:
            entropy += prob * math.log(prob, 2)

    return -entropy

# Verify that our function matches our answer from earlier
entropy = calc_entropy([1,1,0,0,1])
print(entropy) # 0.97

information_gain = entropy - ((.8 * calc_entropy([1,1,0,0])) + (.2 * calc_entropy([1])))
print(information_gain) # 0.17
income_entropy = calc_entropy(income["high_income"])

median_age = income["age"].median()

left_split = income[income["age"] <= median_age]
right_split = income[income["age"] > median_age]

age_information_gain = income_entropy - ((left_split.size / income.size) * calc_entropy(left_split["high_income"]) + ((right_split.size / income.size) * calc_entropy(right_split["high_income"]))) # 0.047
```

=== 12. Finding the Best Split
* 정보 획득을 계산하는 방법을 알았으므로 노드를 분할하는 최상의 변수를 결정할 수 있음
* 노드 초기 분할 시 어떤 분할이 가장 높은 정보 이득을 가질지 계산하여 분할할 변수를 찾음

```
def calc_information_gain(data, split_name, target_name):
    """
    Calculate information gain given a data set, column to split on, and target
    """
    # Calculate the original entropy
    original_entropy = calc_entropy(data[target_name])

    # Find the median of the column we're splitting
    column = data[split_name]
    median = column.median()

    # Make two subsets of the data, based on the median
    left_split = data[column <= median]
    right_split = data[column > median]

    # Loop through the splits and calculate the subset entropies
    to_subtract = 0
    for subset in [left_split, right_split]:
        prob = (subset.shape[0] / data.shape[0])
        to_subtract += prob * calc_entropy(subset[target_name])

    # Return information gain
    return original_entropy - to_subtract

# Verify that our answer is the same as on the last screen
print(calc_information_gain(income, "age", "high_income"))

columns = ["age", "workclass", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "hours_per_week", "native_country"]

information_gains = []
# Loop through and compute information gains
for col in columns:
    information_gain = calc_information_gain(income, col, "high_income")
    information_gains.append(information_gain)

print(information_gains) # [0.047028661304691965, 0.0068109840543966182, 0.065012984132774232, 0.1114272573715438, 0.0015822303843424645, 0.047362416650269412, 0.0, 0.0, 0.040622468671234868, 0.00013457344495848567]
# Find the name of the column with the highest gain
highest_gain_index = information_gains.index(max(information_gains))
highest_gain = columns[highest_gain_index] # 'marital_status'
```

=== 13. Build the Whole Tree
* 이제 하나의 분할을 만드는 방법을 알고 있음. 전체 트리를 구성하려면 단일 클래스만 가질때까지 분할을 반복
```
age    high_income
25     1
50     1
30     0
50     0
80     1
```
image:./images/income_age_tree.png[width="400px"]

* 결과가 좀 다름, 나눈 후 결과 값이 하나이기 때문. 한행에 두행의 나이가 50 이므로 결과를 더 나눌 수 없음
** 한 leaf의 예를 보면, 나이가 같은 50 이지만 하나는 high_income 이 0, 하나는 1 이므로 이 leaf 에서는 0.5 를 예측 (원래는 더 분할 가능하지만 이 예제에선 불가능)

=== 14. Next Steps
* 지금껏 한것은 https://en.wikipedia.org/wiki/ID3_algorithm[ID3] 알고리즘
* 분할 기준에 대한 다른 측정값을 사용하는 ftp://public.dhe.ibm.com/software/analytics/spss/support/Stats/Docs/Statistics/Algorithms/13.0/TREE-CART.pdf[CART] 같은 다른 알고리즘이 있음
* 다음 장에선 ID3 알고리즘으로 전체트리를 재귀적 생성하는 방법
