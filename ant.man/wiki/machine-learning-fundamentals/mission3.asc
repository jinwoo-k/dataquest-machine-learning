## Multivariate K-Nearest Neighbors
### step.1 개요

아까는 k-nearest neighbors ML model을 쉽게 feature (attribute)한 개로만 함

정확성 늘리기 위해 시도하는 2가지 방법 (유효성 검증하면서 RMSE 줄이기)

. attribute 늘리기 : ranking에 가까운 이웃의 attribute 수
. k 늘리기 : 예측 계산시 사용하는 가까운 이웃 수

이번에는 attribute를 늘림. 단, 아래가 포함된 컬럼은 빼야함

. 숫자 아닌 것 (예: 도시)
. 없는 값
. 일반적인 순서가진 값이 아닌 것 (예: 위도/경도 같이 한 쌍이여야 의미, zipcode)

### step2. Removing Features

.숫자 아닌 것 (룸타입, 도시 등)
.순서값 아닌 것 (위도/경도 등)
.ML 목적에 안맞는 값 (주거에 관계된 목적일 경우 host 에 관한 값 삭제)

### step3. 빈 값 처리
컬럼에 빈 값 적으면 그냥 해당 row 삭제, 빈값 많으면 그 컬럼 자체 삭제

### step4. Normalize Columns
각 컬럼들을 조합해서 분석한다면 큰 범위 컬럼에 영향을 많이 받게 됨 (예: 1~12 범위 컬럼 vs 4~ 1825 범위 컬럼)

따라서 normalize 해서 0~1의 분포를 갖게 함

latexmath:[$x=\frac{x-\mu}{\sigma}$]

asciimath:[`x=\frac{x-\mu}{\sigma}`]

normalized_listings = (dc_listings - dc_listings.mean()) / (dc_listings.std())

### step5. Euclidean Distance For Multivariate Case

asciimath:[`d = \sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \cdots + (q_n-p_n)^2}`]

2개로 유클리안거리 계산 방법
```
from scipy.spatial import distance
first_listing = [-0.596544, -0.439151]
second_listing = [-0.596544, 0.412923]
dist = distance.euclidean(first_listing, second_listing)
```

### step6. Introduction To Scikit-Learn

Scikit-Learn: 파이썬의 유명 머신러닝 라이브러리, simple/unified workflow
. 사용할 머신러닝 모델을 인스턴스화
. 트레이닝 데이터를 맞춤
. 모델 사용하여 예측
. 예측의 정확도 계산

regression :

### step7. Fitting A Model And Making Predictions
