= Gradient Descent
Learn how to fit a model using gradient descent.

== 1. Introduction
* 이전 미션에서 선형 회귀 모델이 피쳐 와 타겟 간의 관계를 예측하는 방법을 배웠다.
* 이번 미션에서는 최적의 파라미터 값을 찾는 과정인 모델 피팅에 대해 배운다.
* 모델 피팅은 MSE를 최소화하는 것을 목표로 한다.
  ** optimization problem : 특정 함수의 결과를 최소화시키거나 최대화 시키는 값 집합을 구하는 문제
* single parameter linear regression model

image::./images/m3_1_1.png[, 250]
image::./images/m3_1_2.png[, 600]

* 가중치 a1 의 변화에 따라 MSE 가 어떻게 변화하는지를 나타낸 그래프가 좌측과 같고, 그 a1에 대한 기울기값을 나타낸 그래프가 우측과 같다.

== 2. Single Variable Gradient Descent
* 이전 스탭에서 다뤘던 MSE를 최소화 시키는 과정은 곡선의 도함수로부터 임계점을 찾는 과정과 같다.
* 이 방식은 다중 파라미터 함수에서는 이용할 수 없다. (시각화도 불가능 하다.)
* Gradient Descent
  ** 반복적으로 다른 파라미터를 적용해 MSE 가 가장 적어지는 파라미터 셋을 구하는 기술
* 단일 파라미터 선형회귀 모델에서의 Gradient Descent 적용
  ** 초기값 a1 선택
  ** 결과값이 수렴할때까지 아래 과정 반복
    *** 현재 파라미터에서의 MSE 구함 image:./images/m3_2_1.png[, 250]
    *** 현재 파라미터에서의 MSE 함수를 미분하여 기울기 d를 구함 image:./images/m3_2_2.png[, 120]
    *** a1을 a1 - ⍺d 로 바꿔서 위의 과정 반복 (여기서 ⍺는 learning rate 를 의미함) image:./images/m3_2_3.png[, 200]
* Gradient Descent 알고리즘은 적절한 초기 파라미터 / learning rate 를 잡아야 반복횟수를 줄이고 최적화를 잘 수행한다. 하지만 우리는 알고리즘 학습에 중점을 둔다.

== 3. Derivative Of The Cost Function
* 우리가 최소화를 통해 최적화 하고 싶은 함수를 cost function (loss function)이라 부른다. => MSE 함수
* 미적분 속성을 이용해 미적분 함수를 단순화 시킬것이며, 연필과 종이를 이용해 먼저 따라가 봐라. => 직접 구현할 필요는 없어도 디버깅에 도움이 된다.

image::./images/m3_3_1.png[, 800]

[source,python]
----
def derivative(a1, xi_list, yi_list):
    sum = 0
    n = len(xi_list)
    for i in range(0, n):
        sum = sum + xi_list[i] * (a1*xi_list[i] - yi_list[i])
    return sum * 2 / n

def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial):
    a1_list = [a1_initial]

    for i in range(0, max_iterations):
        a1 = a1_list[i]
        deriv = derivative(a1, xi_list, yi_list)
        a1_new = a1 - alpha*deriv
        a1_list.append(a1_new)
    return(a1_list)

# Uncomment when ready.
param_iterations = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150)
print(param_iterations)
----

== 4. Understanding Multi Parameter Gradient Descent
image::./images/m3_4_1.png[, 600]

== 5. Gradient Of The Cost Function
image::./images/m3_5_1.png[, 800]

[source,python]
----
def a1_derivative(a1, xi_list, yi_list):
    len_data = len(xi_list)
    error = 0
    for i in range(0, len_data):
        error += xi_list[i]*(a1*xi_list[i] - yi_list[i])
    deriv = 2*error/len_data
    return deriv

def a0_derivative(a0, xi_list, yi_list):
    len_data = len(xi_list)
    error = 0
    for i in range(0, len_data):
        error += a0*xi_list[i] - yi_list[i]
    deriv = 2*error/len_data
    return deriv

def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial, a0_initial):
    a1_list = [a1_initial]
    a0_list = [a0_initial]

    for i in range(0, max_iterations):
        a1 = a1_list[i]
        a0 = a0_list[i]

        a1_deriv = a1_derivative(a1, xi_list, yi_list)
        a0_deriv = a0_derivative(a0, xi_list, yi_list)

        a1_new = a1 - alpha*a1_deriv
        a0_new = a0 - alpha*a0_deriv

        a1_list.append(a1_new)
        a0_list.append(a0_new)
    return(a0_list, a1_list)

# Uncomment when ready.
a0_params, a1_params = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150, 1000)
----

== 6. Gradient Descent For Higher Dimensions
image::./images/m3_6_1.png[, 300]

== 7. Next Steps
* Gradient Descent 는 초기 파라미터 세팅과 적절한 학습률 (learning rate) 세팅이 핵심이다.
