= Gradient Descent
Learn about the gradient descent technique while predicting a professional golfer's accuracy.

== 1. Introduction to the data
* 프로골퍼의 드라이빙 통계(정확도, 거리) 가 포함된 데이터 셋 이용
  ** 정확도는 드라이브에서 fairway를 치는 백분률을 의미
  ** 거리는 평균 드라이브 거리(야드)를 의미
  ** 공을 치는 횟수가 많다면 정확도가 떨어지는 것을 의미함.
* 목표 : 거리를 통한 정확성 예측
* 많은 기계 학습 알고리즘의 경우 데이터를 사용하기 전에 데이터를 크기 조정하거나 표준화하는 것이 중요
  ** 거리를 미터 단위로 측정하고 정확도를 백분율로 측정
  ** 두 필드(거리, 정확도)는 학습 알고리즘에 편향을 일으킬 수 있는 매우 다른 척도
  ** 많은 알고리즘은 두 관측 사이의 Eucilidean Distance를 계산하며, 피쳐들 중 하나가 다른 피쳐보다 훨씬 큰 경우 편향 발생
  ** 데이터를 표준화하려면 각 값에 대해 평균을 뺀 다음 표준 편차로 나눈다. (표준값(표준점수) z 값을 의미)
  ** 데이터를 정규화 한 후 데이터를 시각적으로 표시

image:./images/m8_1_1.png[, 500]

[source,python]
----
import pandas
import matplotlib.pyplot as plt

# Read data from csv
pga = pandas.read_csv("pga.csv")

# Normalize the data
pga.distance = (pga.distance - pga.distance.mean()) / pga.distance.std()
pga.accuracy = (pga.accuracy - pga.accuracy.mean()) / pga.accuracy.std()
print(pga.head())

plt.scatter(pga.distance, pga.accuracy)
plt.xlabel('normalized distance')
plt.ylabel('normalized accuracy')
plt.show()
----

== 2. Linear model
* 이전 스탭의 산점도에서 음의 기울기로 데이터가 선형으로 보이고 거리가 더 멀면 정확도가 떨어진다.
* image:./images/m8_2_1.png[, 300] (θ는 계수이고 ε은 오차항)

[source,python]
----
from sklearn.linear_model import LinearRegression
import numpy as np

print("Shape of the series:", pga.distance.shape)
print("Shape with newaxis:", pga.distance[:, np.newaxis].shape)

lr = LinearRegression()
lr.fit(pga.distance[:, np.newaxis], pga.accuracy)
theta0 = lr.intercept_
theta1 = lr.coef_[0]
----

== 3. Cost function, introduction
* 최소자승법(OLS)를 사용하여 선형 모델의 계수를 추정하기 위해 기존 라이브러리 sklearn을 활용했다.
* 간혹 너무 많은 데이터를 메모리에 저장해야해 최소자승법(least square)를 이용할 수 없다.
* Gradient descent
  ** 선형 모델을 포함하여 거의 모든 모델의 계수를 추정하는 데 사용할 수있는 일반적인 방법
  ** Gradient descent의 핵심은 각 계수를 업데이트하여 추정 된 모델의 잔차를 최소화 하는 것

* cost function
  ** 대부분의 비용 함수는 모델 예측 간의 차이를 측정하고 매개 변수로 계수를 포함한 관측 결과를 측정한다.
  ** 모델 : image:./images/m8_3_1.png[, 150]
  ** cost function : image:./images/m8_3_2.png[, 300]
  ** 비용은 예측과 관측 제곱의 평균 차이의 1/2 이다.
  ** 모델의 계수를 변경함에 따라이 비용이 변경됩니다.
  ** 모델링 중에 계수를 무작위로 선택하고 지능적으로 계수를 업데이트하여 이 비용을 최소화한다.
  ** image:./images/m8_3_3.png[, 500]

[source,python]
----
# The cost function of a single variable linear model
def cost(theta0, theta1, x, y):
    # Initialize cost
    J = 0
    # The number of observations
    m = len(x)
    # Loop through each observation
    for i in range(m):
        # Compute the hypothesis
        h = theta1 * x[i] + theta0
        # Add to cost
        J += (h - y[i])**2
    # Average and normalize cost
    J /= (2*m)
    return J

# The cost for theta0=0 and theta1=1
print(cost(0, 1, pga.distance, pga.accuracy))

theta0 = 100
theta1s = np.linspace(-3,2,100)

plt.plot(theta1s, cost(theta0, theta1s, pga.distance, pga.accuracy))
----

== 4. Cost function, continued
* 이전의 그래프에서 비용이 최소가 되는 지점(global minimum) 이 있음을 확인했다.
* 비용 함수를 최소화하기 위해 최선의 매개 변수 집합을 찾아야하지만 Gradient descent에서는 기울기를 변경하고 절편을 일정하게 유지합니다.
* 절편까지 나타내는 3D 플롯을 만들어 본다. (x축 : 기울기, y축 : 절편, z축 비용)

[source,python]
----
import numpy as np
from mpl_toolkits.mplot3d import Axes3D

# Example of a Surface Plot using Matplotlib
# Create x an y variables
x = np.linspace(-10,10,100)
y = np.linspace(-10,10,100)

# We must create variables to represent each possible pair of points in x and y
# ie. (-10, 10), (-10, -9.8), ... (0, 0), ... ,(10, 9.8), (10,9.8)
# x and y need to be transformed to 100x100 matrices to represent these coordinates
# np.meshgrid will build a coordinate matrices of x and y
X, Y = np.meshgrid(x,y)
print(X[:5,:5],"\n",Y[:5,:5])

# Compute a 3D parabola
Z = X**2 + Y**2

# Open a figure to place the plot on
fig = plt.figure()
# Initialize 3D plot
ax = fig.gca(projection='3d')
# Plot the surface
ax.plot_surface(X=X,Y=Y,Z=Z)

plt.show()

# Use these for your excerise
theta0s = np.linspace(-2,2,100)
theta1s = np.linspace(-2,2, 100)
COST = np.empty(shape=(100,100))

T0S, T1S = np.meshgrid(theta0s, theta1s)
# for each parameter combination compute the cost
for i in range(100):
    for j in range(100):
        COST[i,j] = cost(T0S[0,i], T1S[j,0], pga.distance, pga.accuracy)

# make 3d plot
fig2 = plt.figure()
ax = fig2.gca(projection='3d')
ax.plot_surface(X=T0S,Y=T1S,Z=COST)
plt.show()
----

== 5. Cost function, slopes
* 비용 함수를 각 매개변수(파라미터)에 대해 편미분을 통해 기울기를 구한다.
* theta0 에 대한 편미분 : image:./images/m8_5_1.png[, 280]
* theta1 에 대한 편미분 : image:./images/m8_5_2.png[, 300]

[source,python]
----
def partial_cost_theta1(theta0, theta1, x, y):
    # Hypothesis
    h = theta0 + theta1*x
    # Hypothesis minus observed times x
    diff = (h - y) * x
    # Average to compute partial derivative
    partial = diff.sum() / (x.shape[0])
    return partial

def partial_cost_theta0(theta0, theta1, x, y):
    h = theta0 + theta1*x
    diff = (h - y)
    partial = diff.sum() / (x.shape[0])
    return partial

partial1 = partial_cost_theta1(0, 5, pga.distance, pga.accuracy)
print("partial1 =", partial1)

partial0 = partial_cost_theta0(1, 1, pga.distance, pga.accuracy)
print("partial0 =", partial0)
----

== 6. Gradient descent algorithm
* Gradient descent를 이용해 가설 모델과 관측치 간의 오차를 최소화하기 위해 매개 변수를 변경하여 비용 함수의 최소값을 구할 수 있다.
* 그라디언트 디센트 실행 순서
  ** 매개 변수 집합을 무작위로 초기화
  ** 하향 기울기의 방향으로 이동하여 매개 변수 집합 업데이트
  ** 비용이 특정 값에 수렴(최적의 매개 변수 집합) 할때 까지 위의 과정 반복
* 반복시 파라미터 갱신 방법 : image:./images/m8_6_1.png[, 450]
  ** α 는 learning rate 로, 너무 크면 파라미터의 방향을 자주 바꾸게 되고, 너무 작으면 느려 적당한 수치를 찾는게 중요하다.

== 7. Gradient descent algorithm

[source,python]
----
# x is our feature vector -- distance
# y is our target variable -- accuracy
# alpha is the learning rate
# theta0 is the intial theta0
# theta1 is the intial theta1
def gradient_descent(x, y, alpha=0.1, theta0=0, theta1=0):
    max_epochs = 1000 # Maximum number of iterations
    counter = 0      # Intialize a counter
    c = cost(theta1, theta0, pga.distance, pga.accuracy)  ## Initial cost
    costs = [c]     # Lets store each update
    # Set a convergence threshold to find where the cost function in minimized
    # When the difference between the previous cost and current cost
    #        is less than this value we will say the parameters converged
    convergence_thres = 0.000001
    cprev = c + 10
    theta0s = [theta0]
    theta1s = [theta1]

    # When the costs converge or we hit a large number of iterations will we stop updating
    while (np.abs(cprev - c) > convergence_thres) and (counter < max_epochs):
        cprev = c
        # Alpha times the partial deriviative is our updated
        update0 = alpha * partial_cost_theta0(theta0, theta1, x, y)
        update1 = alpha * partial_cost_theta1(theta0, theta1, x, y)

        # Update theta0 and theta1 at the same time
        # We want to compute the slopes at the same set of hypothesised parameters
        #             so we update after finding the partial derivatives
        theta0 -= update0
        theta1 -= update1

        # Store thetas
        theta0s.append(theta0)
        theta1s.append(theta1)

        # Compute the new cost
        c = cost(theta0, theta1, pga.distance, pga.accuracy)

        # Store updates
        costs.append(c)
        counter += 1   # Count

    return {'theta0': theta0, 'theta1': theta1, "costs": costs}



print("Theta1 =", gradient_descent(pga.distance, pga.accuracy)['theta1'])

costs = gradient_descent(pga.distance, pga.accuracy, 0.01)['costs']
plt.scatter(range(0, len(costs)), costs)
----
