## Cross Validation
Learn how to use k-fold cross validation to perform more rigorous testing.

### step.1 Introduction

hold out 유효성체크

. training / test set 으로 나눔
. training 함
. 이걸로 test set 예측
. error metric 계산
. trainig / test set 을 바꿔서 위를 반복
. 에러 평균을 구함

핵심은 이전에 75:25 (3:1) 로 나누지 않고 50:50 으로 한 뒤 확인
image:https://s3.amazonaws.com/dq-content/holdout_validation.png[width:"500px"]

### step.2 Holdout Validation
실제 모델 두 번 계산 후 평균

### step.3 K-Fold Cross Validation
holdout 유효성 : k-fold cross-validation 의 유효성 테크닉의 큰 클래스 예

holdout 이 traning/test 보다 좋다. -> 편향이 적기떄문 (대신 반만 사용)

K-Fold Cross validation: 좀더 많은 부분 사용하면서 training/test의 이슈를 줄이기 위해 데이터의 부분을 돌린다.

. 전체에서 k 만큼 길이의 부분을 나눔 (k-1 개는 training vs 1 개는 test set)
. training set 에서 계산하여 error metric 만듦
. k-1 번 반복
. k 개 error 평균 구함

image:https://s3.amazonaws.com/dq-content/kfold_cross_validation.png[width="500px"]

> holdout validation은 k-fold cross validation에서 k = 2 (보통 k는 5 or 10)

fold 증가 -> 각각 fold 의 관찰수 감소 -> 폴드간 분산 에러 증가??

```
dc_listings.set_value(dc_listings.index[0:744], "fold", 1)
```

### step.4 First Iteration
1 을 test set 으로 , 2~5 으로 training

```
train = dc_listings[dc_listings['fold'] != 1]
test = dc_listings[dc_listings['fold'] == 1]
```
### step.5 Function For Training Models
loop 돌며 위에 한번했던 것 반복 > 결과 rmse 평균

### step.6 Performing K-Fold Cross Validation Using Scikit-Learn
일반적으로 함수 만드려면 임의화와 순서화를 해야함

scikit 활용하여 hyperparameter 를 조절하여 최적의 k-nearest neighbor model 을 구축

```
kf = KFold(n, n_folds, shuffle=False, random_state=None)
```
. n : 관찰 데이터 셋의 수
. n_folds : is the number of folds you want to use,
. shuffle : 데이터 셋의 관측 순서를 바꾸기
. random_state : 특정 seed 를 사용하려면 `True`
```
cross_val_score(estimator, X, Y, scoring=None, cv=None)
```
. estimator : fit 모델이 구현된 sklearn model (예. instance of KNeighborsRegressor)
. X : train 할 속성이 포함된 2차 배열 or 리스트
. y : 예측할 타겟 컬럼이 포함된 값 리스트
. scoring : 점수 기준을 설명할 문자열
. cv : 폴드수
** KFold 클래스의 인스턴스
** fold 수 (정수)

[source,python]
----
from sklearn.cross_validation import KFold
from sklearn.cross_validation import cross_val_score

#kf = KFold(n, n_folds, shuffle=False, random_state=None)
kf = KFold(len(dc_listings), 5, True, 1)

knn = KNeighborsRegressor()

#cross_val_score(estimator, X, Y, scoring=None, cv=None)
mses = cross_val_score(knn, dc_listings[['accommodates']], dc_listings[['price']], scoring="mean_squared_error", cv=kf)

rmses = [np.sqrt(np.absolute(mse)) for mse in mses]
avg_rmse = np.mean(rmses)
----

### stop.7 Exploring Different K Values

k 가 2일때 holdout, 그 외에 k 가 n (데이터 셋의 관측 수) 일 때 leave-one-out cross validation(LOOCV) 라고 함

데이터 과학자들은 k 가 10 을 일반적으로 사용함

예제 확인) k (폴드수)가 3 부터 홀수로 23까지 올릴 때, RMSE의 표준 편차가 1.1 에서 37.3 까지 올라감.

### step.8 Bias-Variance Tradeoff

* 모델은 두가지 원인의 에러가 있다! 편향과 분산

** 편향?
잘못된 가정에 의한 학습이 원인. (예: 오직 차의 무게가 연료 효율에 영향을 미친다. -> 다른 요소도 많음)
** 분산? 예측된 값의 다양성 때문에 생김.

* rmse의 표준편차는 모델의 분산의 프록시, rmse의 평균은 모델 편향의 프록시
* k-nearest negihbors는 예측을 할 수 있지만 수학적 모델은 아님 (수학모델은 원본 데이터 없이 존재)

image::https://s3.amazonaws.com/dq-content/bias_variance.png[width:"500px"]

image::./images/biasVsVariance.jpg[width="500px"]

### step.9 Next Steps

## ref
편향,분산 이해 : http://bywords.tistory.com/entry/%EB%B2%88%EC%97%AD-%EC%9C%A0%EC%B9%98%EC%9B%90%EC%83%9D%EB%8F%84-%EC%9D%B4%ED%95%B4%ED%95%A0-%EC%88%98-%EC%9E%88%EB%8A%94-biasvariance-tradeoff
