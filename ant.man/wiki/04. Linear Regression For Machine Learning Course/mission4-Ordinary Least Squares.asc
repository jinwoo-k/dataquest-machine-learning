== Ordinary Least Squares
=== 1. Introduction
* Gradient descent : 수렴을 위해 반복 (반복 횟수는 초기 매개변수와 학습률에 따라 달라짐)
* OSL (Ordinary least square) : 최적 파라미터를 계산하는 최소한 비용 함수가 명확
** y (예측값) = a0 + a1*x1 + ... + an*xn
image:./images/osl_equation.png[]

```
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv('AmesHousing.txt', delimiter="\t")
train = data[0:1460]
test = data[1460:]

features = ['Wood Deck SF', 'Fireplaces', 'Full Bath', '1st Flr SF', 'Garage Area',
       'Gr Liv Area', 'Overall Qual']

X = train[features]
y = train['SalePrice']

a = np.dot(
        np.dot(
            np.linalg.inv(
                np.dot(
                    np.transpose(X),
                    X
                )
            ),
            np.transpose(X)
        ),
        y
    )
```

=== 2. Cost Function

* OSL: 닫힌 솔루션 (예측 가능한 수학 연산으로 계산가능)
* gradient descent: 초기 매개변수 + 학습 비율에 따라 연산 수 다름 (반복수)
* 둘 다 비용함수를 최소화하는 목표

* 표기
** E (epsilon) : 에러 벡터 (= y^ - y)
image:./images/epsilon.png[]
image:./images/epsilon_y.png[]

* 비용함수는 (cost functino) MSE
image:./images/cost_function.png[]

=== 3. Derivative Of The Cost Function
* 비용함수의 도함수
image:./images/derivate_cost_func.png[]

* 최소를 구하기 위해 도함수 값을 0 으로 셋팅 후 a 를 구함
image:./images/calc_derivate_cost_func.png[]

=== 4. Gradient Descent vs. Ordinary Least Squares
* 역행렬을 구할때 계산 복잡도가 거의 O(n^3)이므로 비용이 많이 듦 (https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra[matrix cost])
** 따라서, 수백만개 미만일 때 사용

=== 5. Next step
