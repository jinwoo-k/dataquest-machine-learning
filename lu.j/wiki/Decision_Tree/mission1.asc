= Introduction To Decision Tree

== 1. Introduction
* 의사 결정 트리는 강력하고 널리 사용되는 기계 학습 기술이다.

image::./images/1-1.PNG[의사결정트리]

[width="20%"]
|=======
|Bear name |   Size  |  Escape possible?  |  Action   |
|Yogi      |   Small |  No                |  Wrestle  |
|Winnie    |   Small |  Yes               |  Wrestle  |
|Baloo     |   Large |  Yes               |  Run away |
|Gentle Ben|   Large |  No                |  Wrestle  |
|=======

* 특정 상황에서 우리가 예측해야하는 결과를 알려주는 의사결정 트리를 자동으로 구성할 수 있다.
** 이력 데이터로 트리를 구성한 다음 이를 사용해 결과를 예측한다
** 선형 회귀가 수행할 수 없는 데이터에서 변수 간의 비선형 상호 작용을 선택할 수 있다는 점이 주요이점이다.

== 2. Overview of the Data Set
* 데이터는 미국의 개인소득을 살펴볼 것이다.
** 1994년 인구조사에서 나온 개인의 혼인상태, 나이, 직업 유형 등에 대한 정보를 이용할 예정이다.
** 예측하고자 하는 것는 개인이 연간 50K 또는 연간 50K이상 생산하는지 여부

[source,python]
----
import pandas

# Set index_col to False to avoid pandas thinking that the first column is row indexes (it's age)
income = pandas.read_csv("income.csv", index_col=False)
print(income.head(5))
----

== 3. Converting Categorical Variables
* 업무 유형에는 Stat-gov, Self-emp-not-inc, Private 이 포함된다.
* 범주 열의 또 다른 예는 성별 - 남자, 여자
* 데이터 셋의 범주형 변수를 숫자형으로 변환해야한다.

[source,python]
----
# Convert a single column from text categories to numbers
col = pandas.Categorical.from_array(income["workclass"])
income["workclass"] = col.codes
print(income["workclass"].head(5))
for name in ["education", "marital_status", "occupation", "relationship", "race", "sex", "native_country", "high_income"]:
    col = pandas.Categorical.from_array(income[name])
    income[name] = col.codes
----

== 4. Splitting Data
* 의사결정트리는 일련의 노드와 분기로 구성된다.
** 노드 - 변수를 기반으로 데이터를 분할하는 곳
** 분기 - 분할의 한면

image::./images/1-2.PNG[의사결정트리]

* 노드는 두개의 분기(no and yes)로 분할된다.
** 개인이 민간 부문에서 근무하는지 여부
* WorkClass 열에 Private 값을 숫자코드 4에 매핑했다.
** No - WorkClass != 4, Yes - WorkClass == 4
* 의사결정트리는 변수를 기반으로 데이터를 계속 분할한다.
* 이렇게 트리는 더 많은 레벨을 축적한다.

image::./images/1-3.PNG[의사결정트리]

== 5. Creating Splits
* 의사 결정 트리를 통해 흐르는 데이터 행을 생각해보자.
* 개인이 민간 부문에서 근무하는지 아닌지에 따라 데이터 셋을 두 부분으로 나눌수 있다.

[source,python]
----
private_incomes = income[income["workclass"] == 4]
public_incomes = income[income["workclass"] != 4]

print(private_incomes.shape)
print(public_incomes.shape)
----

== 6. Decision Trees as Flows of Data
* 의사결정 트리를 데이터 행의 흐름으로 생각하면 유용하다.
* 분할하면 일부 행이 오른쪽으로 이동하고 일부는 왼쪽으로 이동한다.
* 트리를 깊고 깊게 만들수록 각 노드는 더 적은 수의 행을 받는다.

image::./images/1-4.PNG[의사결정트리]

== 7. Splitting Data to Make Predictions
* 분할을 중지하기로 결정한 트리의 맨 아래에 있는 노드를 종단노드 또는 잎이라고 한다.
* 각 리프의 모든 행에 목표열에 대해 하나의 값만 있어야 한다.
* high_income이 1이면 연간 50K보다 높은 소득을 가지고 있고 high_income이 0이면 이하이다.
* 리프는 우리가 high_income을 예측해야하는 가치를 알려준다.
* 노드의 모든 행이 high_income에 대해 동일한 값을 가질때까지 노드를 계속 분할해야한다.

== 8. Overview of Data Set Entropy
* 특정 척도를 사용해 노드를 분할해야하는 변수를 파악한다.
* high_income에서 1과 0을 분리하려고 시도할 것이다. 그러려면 high_income 열의 여러값을 함께 사용하는 방법에 대한 매트릭이 필요하다.
* 이때 엔트로피라는 측정 기준을 사용한다.
* 정보의 한 비트는 하나의 정보 단위이다.
* 엔트로피의 공식은 아래와 같다.

image::./images/1-5.PNG[엔트로피]

image::./images/1-6.PNG[엔트로피공식대입]

== 9. Overview of Data set Entropy

[source,python]
----
import math
# We'll do the same calculation we did above, but in Python
# Passing in 2 as the second parameter to math.log will take a base 2 log
entropy = -(2/5 * math.log(2/5, 2) + 3/5 * math.log(3/5, 2))
print(entropy)
prob_0 = income[income["high_income"] == 0].shape[0] / income.shape[0]
prob_1 = income[income["high_income"] == 1].shape[0] / income.shape[0]
income_entropy = -(prob_0 * math.log(prob_0, 2) + prob_1 * math.log(prob_1, 2))
----

== 10. Information Gain
* 정보분할을 통해 엔트로피를 가장 많이 줄일수 있는 부분을 알수 있다.

image::./images/1-7.PNG[정보획득공식]

* 목표변수(T)에 대한 정보 이득(IG)와 우리가 나누고자하는 변수(A)를 계산하고 있다.
* T에 대한 엔트로피를 계산한다. 그 뒤 변수 A의 고유값 V에 대해 A가 값 V를 취하는 행의 수를 계산하고 총 행수로 나눈다.
* A가 V인 행에 엔트로피를 곱한다.
* 분할 후 각 집합의 엔트로피를 찾고 각 분할의 항목 수로 가중치를 부여한 후 현재 엔트로피에서 뺀다.
* 결과가 양수이면 분할로 엔트로피를 낮춘다.

image::./images/1-8.PNG[결과식]

== 11. Information Gain
[source,python]
----
import numpy

def calc_entropy(column):
    """
    Calculate entropy given a pandas series, list, or numpy array.
    """
    # Compute the counts of each unique value in the column
    counts = numpy.bincount(column)
    # Divide by the total column length to get a probability
    probabilities = counts / len(column)

    # Initialize the entropy to 0
    entropy = 0
    # Loop through the probabilities, and add each one to the total entropy
    for prob in probabilities:
        if prob > 0:
            entropy += prob * math.log(prob, 2)

    return -entropy

# Verify that our function matches our answer from earlier
entropy = calc_entropy([1,1,0,0,1])
print(entropy)

information_gain = entropy - ((.8 * calc_entropy([1,1,0,0])) + (.2 * calc_entropy([1])))
print(information_gain)
income_entropy = calc_entropy(income["high_income"])

median_age = income["age"].median()

left_split = income[income["age"] <= median_age]
right_split = income[income["age"] > median_age]

age_information_gain = income_entropy - ((left_split.shape[0] / income.shape[0]) * calc_entropy(left_split["high_income"]) + ((right_split.shape[0] / income.shape[0]) * calc_entropy(right_split["high_income"])))
----

== 12. Finding the Best Split
* 정보 이득을 계산하는 방법을 알았으므로 노드를 분할하는 최상의 변수를 결정 할 수 있따.

[source,python]
----
def calc_information_gain(data, split_name, target_name):
    """
    Calculate information gain given a data set, column to split on, and target
    """
    # Calculate the original entropy
    original_entropy = calc_entropy(data[target_name])

    # Find the median of the column we're splitting
    column = data[split_name]
    median = column.median()

    # Make two subsets of the data, based on the median
    left_split = data[column <= median]
    right_split = data[column > median]

    # Loop through the splits and calculate the subset entropies
    to_subtract = 0
    for subset in [left_split, right_split]:
        prob = (subset.shape[0] / data.shape[0])
        to_subtract += prob * calc_entropy(subset[target_name])

    # Return information gain
    return original_entropy - to_subtract

# Verify that our answer is the same as on the last screen
print(calc_information_gain(income, "age", "high_income"))

columns = ["age", "workclass", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "hours_per_week", "native_country"]
information_gains = []
# Loop through and compute information gains
for col in columns:
    information_gain = calc_information_gain(income, col, "high_income")
    information_gains.append(information_gain)

# Find the name of the column with the highest gain
highest_gain_index = information_gains.index(max(information_gains))
highest_gain = columns[highest_gain_index]
----

== 13. Build the Whole Tree
* 우리는 하나의 분할을 만드는 방법을 알고 있다.
* 전체 트리를 구성하려면 리프가 단일 클래스를 가질 때까지 계속 분할을 해야한다.

image::./images/1-9.PNG[분할하기]

== 14. Next Steps
* 우리는 의사결정트리를 구성하기 위해 ID3 알고리즘을 따왔다.
* 분할 기준에 대해 다른 측정값을 사용하는 CART와 같은 다른 알고리즘이 있다.
* 다음 임무에서는 ID3 알고리즘을 사용해 전체 트리를 재귀적으로 생성하는 방법을 알아볼 예정이다.
