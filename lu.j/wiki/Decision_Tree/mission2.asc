= Building a Decision Tree

== 1. Introduction to the Data
* 전 미션에서 사용했던 1994년 인구조사에서 얻은 데이터를 계속 사용하도록 한다.
** 결혼상태, 나이, 직장 유형등의 컬럼이 포함되어 있음
** high_income은 연간 50K 이하(0), 연간 50K 초과 (1) 로 나타낸다

== 2. Overview of the ID3 Algorithm
* 마지막 임무에서 엔트로피와 정보 이득을 포함한 의사 결정 트리의 기초에 대해 배웠다.
* 의사 결정 트리를 구성하기 위해 ID3 알고리즘을 사용한다.
* 재귀
** 큰 문제를 작은 덩어리로 분할하는 과정이다.
** 스스로를 호출 한 후 결과를 최종 출력으로 결합한다.
* 트리를 만드는 것은 재귀 알고리즘을 사용한다.

[source,python]
----
def id3(data, target, columns)
    1 Create a node for the tree
    (트리 노드를 만든다)
    2 If all values of the target attribute are 1, Return the node, with label = 1
    (타겟 속성의 모든 값이 1이면 label = 1로 노드를 반환)
    3 If all values of the target attribute are 0, Return the node, with label = 0
    (타겟 속성의 모든 값이 0이면 label = 0로 노드를 반환)
    4 Using information gain, find A, the column that splits the data best
    (정보 이득을 사용하여 데이터를 가장 잘 나눌수 있는 열 A를 찾는다.)
    5 Find the median value in column A
    (열 A에서 중앙 값을 찾는다.)
    6 Split column A into values below or equal to the median (0), and values above the median (1)
    (열 A를 중앙값 보다 작거나 같은 값(0)으로 나누고 중앙값보다 큰 값(1)으로 나눈다)
    7 For each possible value (0 or 1), vi, of A,
    (A에 대해 가능한 값(0 or 1)을 반복한다.)
    8    Add a new tree branch below Root that corresponds to rows of data where A = vi
    (A = vi이면 새로운 가지를 Root 밑에 추가한다)
    9    Let Examples(vi) be the subset of examples that have the value vi for A
    (각 하위 노드들은 가지의 조건을 만족하는 레코드들이다.(?))
   10    Below this new branch add the subtree id3(data[A==vi], target, columns)
   (이 새 가지에 id3를 호출해 얻은 서브트리를 추가한다.)
   11 Return Root
   (Root 를 반환한다)
----

== 3. Walking Through an Example of the ID3 Algorithm
* 더미 데이터 셋으로 ID3를 따라가보도록 하자
* 우리는 age, marital_status를 사용해 high_income를 예측하려고 한다.
** marital_status - 미혼:0, 결혼:1, 이혼:2

[width="15%"]
|=======
|high_income |   age |   marital_status |
|0           |   20  |   0              |
|0           |   40  |   1              |
|0           |   60  |   2              |
|1           |   35  |   2              |
|1           |   25  |   1              |
|1           |   55  |   1              |
|=======

* 4번 - 정보이득을 계산하지 않고 나이를 이용할 것이다.
* 5번 - 중앙값 37.5를 찾는다.
* 6번 - 모든 값을 중앙값보다 작거나 같은 값을 0로 큰값은 1로 만든다.
* 7번 - 루프를 시작한다.
* A에 대해 가능한 값을 순서대로 처리하기 때문에 0값이 먼저 온다.
* 10번 - 지점 끝에있는 새 노드에서 id3()를 호출한다. 그렇게 되면 현재 실행을 "일시중지" 하게되고 일시중지된 노드 1을 호출하게 된다.

image::./images/2-1.PNG[Node2]

* 새 노드에 대한 데이터는 다음과 같다.

[width="15%"]
|=======
|high_income  |  age  |  marital_status |
|0            |  20   |  0              |
|1            |  25   |  1              |
|1            |  35   |  2              |
|=======

* 우리가 재귀적으로 id3() 함수를 호출했으므로 처음부터 다시 시작한 데이터만 사용해 다시 시작한다.
* age는 다시 25의 중앙값을 가진 가장 좋은 split 변수이다. age = 25일 때 왼쪽으로 분기를 만든다.

image::./images/2-2.PNG[Node3]

* 새 노드의 데이터는 다음과 같다

[width="15%"]
|=======
|high_income  |  age  |  marital_status |
|0            |  20   |  0              |
|1            |  25   |  1              |
|=======

* 노드2를 "일시정지" 하여 id3() 함수에서 다시 시작할 것이다.
* 분할 할 열은 age이고 중앙값은 22.5이다.

image::./images/2-3.PNG[Node4]

* 노드4의 high_income 값은 모두 0이다.
* 여기서 더이상의 노드를 추가하지 않으면 노드3의 idx3() 함수가 "해제"되고 트리의 오른쪽으로 이동한다.
* 7번 - for 루프를 수행한다.
* 노드 4에대한 id3() 알고리즘이 반환되면 노드3은 for루프 다음 반복으로 이동한다.
* 노드3에서 오른쪽 가지를 만들도록 한다. 이 함수는 노드 5의 id3() 함수를 호출한다. 이 함수는 2번에서 멈추고 리턴한다.
* 이 분할에는 단 하나의 행이 있으며 잎 노드로 끝나게 된다. label 은 1

image::./images/2-4.PNG[Node5]

* 노드3에 대한 전체 루프가 끝났다. 우리는 왼쪽 열의 하위 트리와 오른쪽 하위트리를 만들었다.
* 노드 3에 대한 id3() 함수를 반환한다. 이렇게 하면 노드2가 비활성화되어 오른쪽 분할을 구성하게 된다.

image::./images/2-5.PNG[Node6]

* 그러면 노드2가 처리를 끝내게 된다. 노드1은 일시중지가 해제되고 트리의 오른쪽을 구성하게 된다.

== 4. Determining the column to Split on
* 전 미션에서 엔트로피 및 정보 이득을 계산하는 함수 (calc_entropy() or calc_infomation_gain())를 작성했다.

[source,python]
----
# A list of columns to potentially split income with
columns = ["age", "workclass", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "hours_per_week", "native_country"]

def find_best_column(data, target_name, columns):
    information_gains = []
    # Loop through and compute information gains
    for col in columns:
        information_gain = calc_information_gain(data, col, "high_income")
        information_gains.append(information_gain)

    # Find the name of the column with the highest gain
    highest_gain_index = information_gains.index(max(information_gains))
    highest_gain = columns[highest_gain_index]
    return highest_gain

income_split = find_best_column(income, "high_income", columns)
----

* 결과값 - income_split : marital_status

== 5. Creating a Simple Recursive Algorithm
* 우리가 확장할 수 있는 더 간단한 알고리즘을 만들어서 완전한 id3() 함수를 만들어보자

[source,python]
----
def id3(data, target, columns)
    1 Create a node for the tree
    2 If all values of the target attribute are 1, add 1 to counter_1
    3 If all values of the target attribute are 0, add 1 to counter_0
    4 Using information gain, find A, the column that splits the data best
    5 Find the median value in column A
    6 Split A into values below or equal to the median (0), and values above the median (1)
    7 For each possible value (0 or 1), vi, of A,
    8    Add a new tree branch below Root that corresponds to rows of data where A = vi
    9    Let Examples(vi) be the subset of examples that have the value vi for A
   10    Below this new branch, add the subtree id3(data[A==vi], target, columns)
   11 Return Root
----

* 위 알고리즘과 다른 것은 전체 트리를 저장하는 것보다 레이블이 1로 끝나는 잎의 수와 0으로 끝나는 잎의 수를 계산한다.
* 해당 알고리즘을 코드화 하여 아래 데이터를 얻었다.

[width="15%"]
|=======
|high_income  |  age  |  marital_status |
|0            |  20   |  0              |
|0            |  60   |  2              |
|0            |  40   |  1              |
|1            |  25   |  1              |
|1            |  35   |  2              |
|1            |  55   |  1              |
|=======

[source,python]
----
data = pandas.DataFrame([
    [0,20,0],
    [0,60,2],
    [0,40,1],
    [1,25,1],
    [1,35,2],
    [1,55,1]
    ])

data.columns = ["high_income", "age", "marital_status"]

label_1s = []
label_0s = []

def id3(data, target, columns):
    unique_targets = pandas.unique(data[target])

    if len(unique_targets) == 1:
        if 0 in unique_targets:
            label_0s.append(0)
        elif 1 in unique_targets:
            label_1s.append(1)
        return

    best_column = find_best_column(data, target, columns)
    column_median = data[best_column].median()

    left_split = data[data[best_column] <= column_median]
    right_split = data[data[best_column] > column_median]

    for split in [left_split, right_split]:
        id3(split, target, columns)


id3(data, "high_income", ["age", "marital_status"])
----

image::./images/2-6.PNG[결과값]

== 6. Stroing the Tree
* 이제 전체 트리를 저장할 수 있게 되었다.
* 우리는 dictionary로 트리를 표현할 수 있다.

[width="15%"]
|=======
|high_income  |  age  |  marital_status |
|0            |  20   |  0              |
|0            |  60   |  2              |
|0            |  40   |  1              |
|1            |  25   |  1              |
|1            |  35   |  2              |
|1            |  55   |  1              |
|=======

[source,python]
----
{
   "left":{
      "left":{
         "left":{
            "number":4,
            "label":0
         },
         "column":"age",
         "median":22.5,
         "number":3,
         "right":{
            "number":5,
            "label":1
         }
      },
      "column":"age",
      "median":25.0,
      "number":2,
      "right":{
         "number":6,
         "label":1
      }
   },
   "column":"age",
   "median":37.5,
   "number":1,
   "right":{
      "left":{
         "left":{
            "number":9,
            "label":0
         },
         "column":"age",
         "median":47.5,
         "number":8,
         "right":{
            "number":10,
            "label":1
         }
      },
      "column":"age",
      "median":55.0,
      "number":7,
      "right":{
         "number":11,
         "label":0
      }
   }
}
----

* 트리를 추적하기 위해서는 id3()를 약간 수정해야할 필요가 있다.

[source,python]
----
def id3(data, target, columns, tree)
    1 Create a node for the tree
    2 Number the node
    3 If all of the values of the target attribute are 1, assign 1 to the label key in tree
    4 If all of the values of the target attribute are 0, assign 0 to the label key in tree
    5 Using information gain, find A, the column that splits the data best
    6 Find the median value in column A
    7 Assign the column and median keys in tree
    8 Split A into values less than or equal to the median (0), and values above the median (1)
    9 For each possible value (0 or 1), vi, of A,
   10    Add a new tree branch below Root that corresponds to rows of data where A = vi
   11    Let Examples(vi) be the subset of examples that have the value vi for A
   12    Create a new key with the name corresponding to the side of the split (0=left, 1=right).  The value of this key should be an empty dictionary.
   13    Below this new branch, add the subtree id3(data[A==vi], target, columns, tree[split_side])
   14 Return Root
----

* 이 접근법에서 우리는 트리 dictionary를 id3 함수에 전달하고 몇가지 키를 설정한다.

`tree["left"] = {}`
`tree["right"] = {}`


== 7. String the Tree

[source,python]
----
tree = {}
nodes = []

def id3(data, target, columns, tree):
    unique_targets = pandas.unique(data[target])
    nodes.append(len(nodes) + 1)
    tree["number"] = nodes[-1]

    if len(unique_targets) == 1:
        if 0 in unique_targets:
            tree["label"] = 0
        elif 1 in unique_targets:
            tree["label"] = 1
        return

    best_column = find_best_column(data, target, columns)
    column_median = data[best_column].median()

    tree["column"] = best_column
    tree["median"] = column_median

    left_split = data[data[best_column] <= column_median]
    right_split = data[data[best_column] > column_median]
    split_dict = [["left", left_split], ["right", right_split]]

    for name, split in split_dict:
        tree[name] = {}
        id3(split, target, columns, tree[name])


id3(data, "high_income", ["age", "marital_status"], tree)
----

== 8. Printing Labels for a More Attractive Tree
* 트리 dictionary를 반복해서 라벨을 print 해보도록 하자.
* depth를 추적할 필요가 있다. 어떤 것을 출력하기 전에 depth 변수에 해당하는 수를 접두어로 붙이도록 한다.
* 아래는 수도 코드이다.

[source,python]
----
def print_node(tree, depth):
    1 Check for the presence of the "label" key in the tree
    (트리에 label이라는 키가 있는지 확인한다.)
    2     If found, print the label and return
    (만약 찾으면 그 label을 print하고 리턴한다)
    3 Print out the tree's "column" and "median" keys
    (트리의 '열', '중앙값' 키를 print한다)
    4 Iterate through the tree's "left" and "right" keys
    (tree의 left, right 키를 iterate하면서)
    5     Recursively call print_node(tree[key], depth+1)
    (재귀적으로 print_node 함수를 호출한다)
----

[source,python]
----
def print_with_depth(string, depth):
    # Add space before a string
    prefix = "    " * depth
    # Print a string, and indent it appropriately
    print("{0}{1}".format(prefix, string))


def print_node(tree, depth):
    if "label" in tree:
        print_with_depth("Leaf: Label {0}".format(tree["label"]), depth)
        return
    print_with_depth("{0} > {1}".format(tree["column"], tree["median"]), depth)
    for branch in [tree["left"], tree["right"]]:
        print_node(branch, depth+1)

print_node(tree, 0)
----

image::./images/2-7.PNG[결과값]

== 9. Making Predictions With The Printed Tree
* 트리를 print했더니 분리 포인트를 한눈에 볼 수 있다.

----
age > 37.5
    age > 25.0
        age > 22.5
            Leaf: Label 0
            Leaf: Label 1
        Leaf: Label 1
    age > 55.0
        age > 47.5
            Leaf: Label 0
            Leaf: Label 1
        Leaf: Label 0
----

* 이 트리로 다음 행을 예측한다고 해보자.

----
age    marital_status
50     1
----

** 먼저 트리가 37.5세 이상으로 나뉘니 오른쪽으로 간다.
** 그후 55.0세보다 적으니 왼쪽으로 이동한다. 그후 47.5세보다 많으니 오른쪽으로 간다.
** 우리는 high_income이 1일 것으로 예상할 수 있다!

* 이런 작은 나무로 예측하는건 매우 간단한데 전체 소득의 데이터 프레임을 사용하기 위해 자동화 된 방법을 원하게 된다.

== 10. Making Predictions Automatically
* 예측을 자동으로 수행하는 함수를 만들어보도록 한다.

[source,python]
----
def predict(tree, row):
    1 Check for the presence of "label" in the tree dictionary
    (label이 트리 안에 있는지 체크한다.)
    2    If found, return tree["label"]
    (만약 찾으면 tree['label']을 리턴)
    3 Extract tree["column"] and tree["median"]
    (tree["column"] and tree["median"] 를 구한다)
    4 Check whether row[tree["column"]] is less than or equal to tree["median"]
    (row[tree['column']]이 tree['median']보다 작거나 같은지를 체크한다)
    5    If it's less than or equal, call predict(tree["left"], row) and return the result
    (만약 작거나 같으면 predict(tree["left"], row) 함수를 호출하고 리턴)
    6    If it's greater, call predict(tree["right"], row) and return the result
    (만약 크면 predict(tree["right"], row) 함수를 호출하고 리턴
----

[source,python]
----
def predict(tree, row):
    if "label" in tree:
        return tree["label"]

    column = tree["column"]
    median = tree["median"]
    if row[column] <= median:
        return predict(tree["left"], row)
    else:
        return predict(tree["right"], row)

print(predict(tree, data.iloc[0]))
----

== 11. Making Multiple Predictions
* 단일 행에 대한 예측은 위 step으로 알아보았고 이제 여러행에 대한 예측을 수행하는 함수를 작성하도록 한다.

[source,python]
----
new_data = pandas.DataFrame([
    [40,0],
    [20,2],
    [80,1],
    [15,1],
    [27,2],
    [38,1]
    ])
# Assign column names to the data
new_data.columns = ["age", "marital_status"]

def batch_predict(tree, df):
    return df.apply(lambda x: predict(tree, x), axis=1)

predictions = batch_predict(tree, new_data)
----

== 12. Next Steps
* 이 임무에서는 전체 의사 결정 트리 모델을 작성하고 결과를 인쇄하고 트리를 사용해 예측하는 방법을 배웠다.
* 앞으로는 더 큰 데이터 셋 전반에 의사결정 트리를 적용하고 서로 다른 알고리즘과 관련된 절충사항을 배우고 정확한 예측을 생성하는 방법을 모색할 것이다.
