= Feature Selection
Learn how to select features for linear regression.

== 1. Missing Values
* 이 미션에서는 피처와 대상 열 간의 상관 관계, 피처 간 상관 관계 및 피쳐를 선택하는 피쳐의 다양성에 대해 알아본다.
* 먼저 누락 된 값이 없거나 변환 할 필요가 없는 피쳐들 중에서 선택한다.

[source,python]
----
import pandas as pd
data = pd.read_csv('AmesHousing.txt', delimiter="\t")
train = data[0:1460]
test = data[1460:]
numerical_train = train.select_dtypes(include=['int', 'float'])
numerical_train.drop(['PID', 'Year Built', 'Year Remod/Add', 'Garage Yr Blt', 'Mo Sold', 'Yr Sold'], axis=1, inplace=True)

null_series = numerical_train.isnull().sum()

full_cols_series = null_series[null_series == 0]

print(full_cols_series)
----

== 2. Correlating Feature Columns With Target Column
* 목표 열과 각 피쳐들간의 상관관계를 살펴본다.

[source,python]
----
train_subset = train[full_cols_series.index].corr()
sorted_corrs = train_subset['SalePrice'].abs().sort_values()
print(sorted_corrs)
----

== 3. Correlation Matrix Heatmap
* 사용 가능한 피쳐들 중 상관관계가 0.3 이상인 피쳐만 남긴다.(컷오프)
  ** 이는 임의적이며, 실험을 통해 컷오프 기준을 설정할 수 있다.
* 공선성(한선에 있는, collinearity)이 높은 피쳐를 찾는다.
  ** 공선성은 두개의 피쳐가 높은 연관성을 가지며, 중복된 정보의 위험을 갖는다.
  ** 상관 행렬을 이용해서 두개의 피쳐간 공선성을 확인 가능하지만, 너무 많은 계산이 필요하다.
  ** Seaborn을 사용하여 상관 관계를 시각적으로 비교하는 상관 행렬 히트 맵을 생성 할 수 있다.

image::./images/m2_3_1.png[, 400]

[source,python]
----
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))

strong_corrs = sorted_corrs[sorted_corrs > 0.3]
subset = train_subset[strong_corrs.index].corr()

sns.heatmap(subset)
----

image::./images/m2_3_2.png[, 400]

== 4. Train And Test Model
* 이전 스탭을 통해 발견한 연관관계가 높은 컬럼 쌍 중 한개만 남긴다.
* 테스트 세트에 사용할 열에 대해 누락 된 값이 없는지 확인한다.

[source,python]
----
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np

final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])
features = final_corr_cols.drop(['SalePrice']).index
target = 'SalePrice'

clean_test = test[final_corr_cols.index].dropna()

lr = LinearRegression()
lr.fit(train[features], train[target])

train_predictions = lr.predict(train[features])
train_mse = mean_squared_error(train_predictions, train[target])
train_rmse = np.sqrt(train_mse)

test_predictions = lr.predict(clean_test[features])
test_mse = mean_squared_error(test_predictions, clean_test[target])
test_rmse = np.sqrt(test_mse)

print(train_rmse)
print(test_rmse)
----

== 5. Removing Low Variance Features
* 모든값이 비슷한 컬럼의 경우 (분산이 낮음) 모델의 예측 기능에 기여ㅏ지 못하므로 제거하는게 낫다.

[source,python]
----
subset_train = train[features]

norm_train = subset_train / subset_train.max()
sorted_vars = norm_train.var().sort_values()
print(sorted_vars)
----

== 6. Final Model
* 마지막 스탭에서 발견한 낮은 분산을 갖는 값을 제거한 후 최종적으로 모델을 트레이닝하고 테스트 한다.

[source,python]
----
features = features.drop('Open Porch SF')

lr = LinearRegression()
lr.fit(train[features], train[target])

train_predictions = lr.predict(train[features])
train_mse = mean_squared_error(train_predictions, train[target])
train_rmse_2 = np.sqrt(train_mse)

test_predictions = lr.predict(clean_test[features])
test_mse = mean_squared_error(test_predictions, clean_test[target])
test_rmse_2 = np.sqrt(test_mse)

print(train_rmse_2)
print(test_rmse_2)
----
