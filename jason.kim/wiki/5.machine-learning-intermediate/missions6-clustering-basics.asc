= Clustering basics

== 1. Clustering overview
* supervised machine learning
  ** linear regression, logistic regression
  ** 알고있는 변수로부터 알고리즘 학습을 통해 모르는 변수를 예측하는 것
* unsupervised learning
  ** 데이터로부터 패턴을 찾는 것
  ** clustering 이 대표적
    *** 데이터를 탐색하며 특정 열들 사이의 연결성을 이해하는 것
    *** NBA 선수의 통계치를 이용한 클러스터링 결과 : * image:./images/m6_1_1.png[, 600]

== 2. The dataset
* 미국은 상원의원이 입법을 위해 투표를 하며, 이를 roll call vote 라 한다.
* 미국은 두개의 주요 정당(민주당, 공화당)이 있으며 정당별 투표 방식을 따르지만, 개인별로 독립적인 투표를 할 수도 있다.
* 데이터는 114번째 상원의 투표 결과가 있으며, 각 행은 하나의 의원의 투표 결과를 나타낸다.
* 0은 반대표, 1은 찬성표, 0.5는 기권표를 의미한다.
* 이 데이터를 통해 당내 주류 의원도 찾을 수 있다.

[source,python]
----
import pandas as pd
votes = pd.read_csv('114_congress.csv')
----

== 3. Exploring the data
[source,python]
----
import numpy as np
print(votes['party'].value_counts())
print(np.mean(votes))
----


== 4. Distance between Senators
* 의원들간 투표 결과가 얼마나 유사한지를 거리를 이용해 수학적으로 판단하다.(유클리드 거리 이용

[source, python]
----
from sklearn.metrics.pairwise import euclidean_distances

print(euclidean_distances(votes.iloc[0,3:].reshape(1, -1), votes.iloc[1,3:].reshape(1, -1)))

distance = euclidean_distances(votes.iloc[0,3:], votes.iloc[2,3:])
----

== 5. Initial clustering
* 유클리드 거리를 이용하는 k-means 클러스터링을 이용할 것이며, scikit-learn 라이브러리를 이용한다.
* 투표 결과에 따라 집단으로 묶는다.
  ** 각 클러스터의 센터 지정
  ** 각 센터와 의원간의 유클리드 거리가 계산
  ** 각 의원은 가장 가까운 센터(클러스터)에 배정
* k-means 알고리즘은 클러스터의 수를 미리 지정해야 함 (공화당, 민주당 으로 구분되므로 2개 선택)
* KMeans 클래스를 이용하며, 결과는 두개의 열을 갖는 numpy array가 나온다. 첫번째 열은 첫번째 클러스터와 의원과의 거리, 두번째 열은 두번째 클러스터와 의원의 거리이다.

[source,python]
----
import pandas as pd
from sklearn.cluster import KMeans

kmeans_model = KMeans(n_clusters=2, random_state=1)
senator_distances = kmeans_model.fit_transform(votes.iloc[:,3:])
----

== 7. Exploring the clusters
* Pandas의 crosstab()을 이용하면 두개의 백터 사이의 조합 수를 알 수 있다.
* 아래 예제는 정당 + 클러스터 별 의원수를 표현하는 것이다.

[source,python]
----
labels = kmeans_model.labels_
pd.crosstab(labels, votes["party"])
----

== 8. Exploring Senators in the wrong cluster
* 실제값과 다르게 클러스터링된 데이터를 살펴보자

[source,python]
----
democratic_outliers = votes[(labels == 1) & (votes["party"] == "D")]
print(democratic_outliers)
----

== 9. Plotting out the clusters
* 클러스터를 확인하는 좋은 방법으로 클러스터를 산점도로 그리기가 있다.
* image:./images/m6_9_1.png[, 500]

[source,python]
----
new_labels = list(map(lambda x: 'r' if x else 'b', labels))
plt.scatter(senator_distances[:,0], senator_distances[:,1], c=new_labels)
plt.show()
----

== 10. Finding the most extreme
* 가장 극단적인 성향을 갖는 의원은 특정 클러스터에서 거리가 가장 먼 의원이다.
* 각 값에 세제곱(큐브)을 하여 더해 극단주의자를 찾는다.

[source,python]
----
import numpy as np
extreamism = np.sum(senator_distances ** 3, axis=1)
votes['extreamism'] = extreamism
votes.sort_values('extreamism', ascending=False, inplace=True)
print(votes.head(10))
----

== 11. Next steps
* 클러스터링은 데이터를 탐색하고 패턴을 찾는 강력한 방법
* 일반적으로 지도학습 머신러닝을 시도하기 전에 비지도학습을 통해 데이터를 탐색하는 것이 좋다.
