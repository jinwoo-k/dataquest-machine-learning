= Ordinary Least Squares
== 1. Introduction
* Gradient descent : 수렴을 위해 반복 (반복 횟수는 초기 매개변수와 학습률에 따라 달라짐)
* OSL (Ordinary least square) : 최적 파라미터를 계산하는 최소한 비용 함수가 명확

* 기존 선형 회귀 모델: image:./images/4-1.png[]

* 행렬 형태: image:./images/4-2.png[]
** X:트레이닝 집합의 열, a는 매개 변수 값을 나타내는 벡터, y ^는 예측 벡터

* 최적 벡터 A를 산출하는 OLS 예측 공식: image:./images/4-3.png[]


```
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv('AmesHousing.txt', delimiter="\t")
train = data[0:1460]
test = data[1460:]

features = ['Wood Deck SF', 'Fireplaces', 'Full Bath', '1st Flr SF', 'Garage Area',
       'Gr Liv Area', 'Overall Qual']

X = train[features]
y = train['SalePrice']

a = np.dot(
        np.dot(
            np.linalg.inv(
                np.dot(
                    np.transpose(X),
                    X
                )
            ),
            np.transpose(X)
        ),
        y
    )
```

== 2. Cost Function

* OSL: 닫힌 솔루션 (예측 가능한 수학 연산으로 계산가능)
* gradient descent: 초기 매개변수 + 학습 비율에 따라 연산 수 다름 (반복수)
* 둘 다 비용함수를 최소화하는 목표

* 표기
** E (epsilon ε) : 오차 벡터 (ε= y^ - y)
*** image:./images/4-4.png[]
*** image:./images/4-5.png[] (y 에 대해서 정리)

* Ax = b의 행렬 방정식과 매우 유사하지만 2개의 미지수(벡터 a 와 벡터 y^)를 가지고 있다.
* 비용함수는 다른말로 하면 평균 제곱 오차
* 행렬형식의 비용함수는 (cost function)
image:./images/4-6.png[]

== 3. Derivative Of The Cost Function
* 비용함수의 도함수
image:./images/4-7.png[]

* 최소를 구하기 위해 도함수 값을 0 으로 셋팅 후 a 를 구함
image:./images/4-8.png[]

== 4. Gradient Descent vs. Ordinary Least Squares
* OSL
** 역행렬을 구할때 계산 복잡도가 거의 O(n^3)이므로 비용이 많이 듦 (https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra[matrix cost])
** 따라서, 수백만개 미만일 때 사용
* Gradient Descent
** 큰 데이터 셋트에서 훨씬 더 융통성 있게 사용
** 실용적인 많은 문제에 대해 우리는 임계 값 정확도 값 (또는 설정된 반복 횟수)을 설정하고 "충분히 좋은" 해결책을 사용

== 5. Next step
