== Introduction To Neural Networks
=== 1. Neural networks and iris flowers
* 많은 기계학습 문제: 복잡한 데이터와 그의 비선형적인 특성를 기반으로 됨
** 신경회로망은 비선형 이런 상호작용하는 변수들에서 학습할 수 있는 모델 부류

* 아래 특징을 가진 홍채 꽃의 종을 예측하는 신경회로망 소개
** sepal_length, sepal_width, petal_length, petal_width: 각각 센티미터 단위 연속 변수
** species : 2 종류 [Iris-virginica, Iris-versicolor]
image:./images/iris-hist.png[]

=== 2. Neurons
* 지금까지 많은양의 비선형 성을 허용하지 않는 방법을 얘기함. 예: 아래 같이 O,X 로 정확히 나눠져있는 걸 찾고 싶음
image:./images/OX_divide.png[]

* 선형 모델이나 logistic 모델은 이런 함수를 구축 못하기 때문에 신경망 같은 다른 옵션을 찾아야 함
** 신경망: 인간 두되의 뉴런 구조에 아주 느슨하게 영향을 받음.
** 이 모델은 활성화 단위의 연속 (뉴런)을 사용한 결과로 예측
** 뉴런을 입력을 받고 변환함수를 적용하고 출력을 반환함
image:./images/neuron.png[]
** 위 뉴런은 x, bias 단위 및 4개 feature 로 표시된 5개 단위를 취함
*** 이 bias 단위는 선형회귀에서 절편과 개념이 비슷하고 뉴런의 활동을 한 방향 혹은 다른 방향으로 이동시킴
*** 이 5개 입력은 활성화 함수 h 에 공급됨
*** 0~1 사이의 값을 반환하고 확률로 취급되고 인기있는 sigmoid (logistic) 활성화 함수를 사용함.
image:./images/sigmoid_active_func.png[]
*** sigmoid 함수를 이용해 활성화 함수를 유도했음, 자세히 보면 이전 강의에서 배운 logistic 회귀 함수를 뉴런으로 여기에 나타낼 수 있음.

* ref: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.exp.html[exp], https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html[dot]
```
z = np.asarray([[9, 5, 4]])
y = np.asarray([[-1, 2, 4]])

# np.dot is used for matrix multiplication
# z is 1x3 and y is 1x3,  z * y.T is then 1x1
print(np.dot(z,y.T))

# Variables to test sigmoid_activation
iris["ones"] = np.ones(iris.shape[0])
X = iris[['ones', 'sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values
y = (iris.species == 'Iris-versicolor').values.astype(int)

# The first observation
x0 = X[0]

# Initialize thetas randomly
theta_init = np.random.normal(0,0.01,size=(5,1))

def sigmoid_activation(x, theta):
    x = np.asarray(x)
    theta = np.asarray(theta)
    return 1 / (1 + np.exp(-np.dot(theta.T, x)))

a1 = sigmoid_activation(x0, theta_init)
```

=== 3. Cost function
* gradient descent 를 이용해 단일 뉴런을 2계층 네트워크로 훈련할 수 있음
* 이전에 배운것처럼 오류를 측정하는 비용함수의 최소화가 필요
* 비용함수는 원하는 것과 실제의 결과의 차이를 측정, 아래와 같음
image:./images/cost_actual_desire.png[width="400px"]
* 위 함수 증명: https://www.youtube.com/watch?v=6vzchGYEJBc&feature=youtu.be
** y 의 값에 따라 cost 함수의 값의 변화가 있기 때문에 수식 사용 (일단 x 는 0~1만 나옴)
** y 가 0 일때, -log(1-H(x))
** y 가 1 일때, -log(H(x))

```
# First observation's features and target
x0 = X[0]
y0 = y[0]

# Initialize parameters, we have 5 units and just 1 layer
theta_init = np.random.normal(0,0.01,size=(5,1))

def singlecost(X, y, theta):
    h = sigmoid_activation(X, theta)
    return -np.mean(y * np.log(h) + (1-y) * np.log(1-h))

first_cost = singlecost(x0, y0, theta_init) # 0.64781198784027283
```

=== 4. Compute the Gradients
* 이전 미션에서 기울기를 얻기 위해 cost 함수의 편미분을 계산해야 하는걸 배움
* 편미분 계산은 신경망에서 더 복잡함
* 여기선 전체 에러를 계산 후 해당 오류를 각 매개 변수에 적용해야 함
* 편비분 계산은 체인 규칙을 이용함, image:./images/derivative_chain_rule.png[width="150px"]

. 목표 변수와 예측간의 오차 계산
. 각 매개 변수와 관련된 감도 계산

* 결과적으로 나온 기울기 : image:./images/compute_gradients.png[width="350px"]
** (yi-hΘ (xi)) : 스칼라, 목표와 예측사이의 오차
** hΘ (xi) * (1-hΘ (xi)) : 스칼라, 활성화 함수의 민감도
** xi : 우리가 우리가 관찰하는 i 의 feature
** δ : 길이 5의 벡터이고, 4개의 특징 + 1개 편차인 기울기

* 이를 구현하기 위해 각 관측치의 δ 를 계산 후 평균 기울기를 구하기 위해 평균 계산
* 이후 평균 기울기를 사용해 해당 매개편수를 업데이트

```
# Initialize parameters
theta_init = np.random.normal(0,0.01,size=(5,1))

# Store the updates into this array
grads = np.zeros(theta_init.shape)

# Number of observations
n = X.shape[0]

j = 0
for obs in X:
    # Compute activation
    h = sigmoid_activation(obs, theta_init)
    # Get delta
    delta = (y[j]-h) * h * (1-h) * obs
    # accumulate
    grads += delta[:,np.newaxis]/n
    j+=1
```

=== 5. Two layer network
* 이제 기울기를 계산하고 gradient descent 사용하여 파라미터를 학습하고 4개의 기등을 주어 종을 예측.
* gradient descent 는 매개변수를 조절해 cost함수를 최소화한다.
* "이전매개변수 + 기울기 * learning rate" 를 통해 매개 변수를 조절.
** 위 과정은 최대반복 횟수에 도달하거나 수렴할때까지 반복
```
while (number_of_iterations < max_iterations and (prev_cost - cost) > convergence_thres ) {
    update paramaters
    get new cost
    repeat
}
```

* 두 레이어 네트워크에서 배울 수 있는 하나의 learn() 함수를 통해 이 모든 요소를 구현함
** 초기 변수 셋팅 후 수렴할때까지 반복 시작.
```
theta_init = np.random.normal(0,0.01,size=(5,1))

# set a learning rate
learning_rate = 0.1
# maximum number of iterations for gradient descent
maxepochs = 10000
# costs convergence threshold, ie. (prevcost - cost) > convergence_thres
convergence_thres = 0.0001

def learn(X, y, theta, learning_rate, maxepochs, convergence_thres):
    costs = []
    cost = singlecost(X, y, theta)  # compute initial cost
    costprev = cost + convergence_thres + 0.01  # set an inital costprev to past while loop
    counter = 0  # add a counter
    # Loop through until convergence
    for counter in range(maxepochs):
        grads = np.zeros(theta.shape)
        for j, obs in enumerate(X):
            h = sigmoid_activation(obs, theta)   # Compute activation
            delta = (y[j]-h) * h * (1-h) * obs   # Get delta
            grads += delta[:,np.newaxis]/X.shape[0]  # accumulate

        # update parameters
        theta += grads * learning_rate
        counter += 1  # count
        costprev = cost  # store prev cost
        cost = singlecost(X, y, theta) # compute new cost
        costs.append(cost)
        if np.abs(costprev-cost) < convergence_thres:
            break

    plt.plot(costs)
    plt.title("Convergence of the Cost Function")
    plt.ylabel("J($\Theta$)")
    plt.xlabel("Iteration")
    plt.show()
    return theta

theta = learn(X, y, theta_init, learning_rate, maxepochs, convergence_thres)
# 결과값
#[[ 0.65676666]
# [ 1.20285719]
# [ 0.94988597]
# [-1.70517503]
# [-1.56862499]]
```

image:./images/plt_ConvergenceoftheCostFunction.png[]

=== 6. Neural Network
* 신경망은 일반적으로 다중 층의 뉴런읠 사용함
* 층을 더 추가하면 더 복잡한 기능을 배울 수 있다.

image:./images/3layer_neural.png[]

** x1,x2,x3,x4와 편차로 이뤄진 입력
** 각 변수와 편차는 hidden layer (hidden unit 4개)로 보내짐
** hidden unit 들은 각각 다른 파라미터 θ 로 구해짐
image:./images/hidden_unit_compute.png[width="400px"]

* 마지막 계층은 우리 모델의 산출물 혹은 예측
* 각 변수가 숨겨진 계층의 각 뉴련으로 보내는 것처럼 각 뉴련의 활성화 단위는 다음 계층의 각 뉴련으로 전송
* 만약 단일 레이어일 경우 : image:./images/single_layer.png[width="400px"]

```
theta0_init = np.random.normal(0,0.01,size=(5,4))
theta1_init = np.random.normal(0,0.01,size=(5,1))

def feedforward(X, theta0, theta1):
    # feedforward to the first layer
    a1 = sigmoid_activation(X.T, theta0).T
    # add a column of ones for bias term (1)
    a1 = np.column_stack([np.ones(a1.shape[0]), a1])
    # activation units are then inputted to the output layer
    out = sigmoid_activation(a1.T, theta1)
    return out

h = feedforward(X, theta0_init, theta1_init)
```

추가 참고 : https://www.youtube.com/watch?v=iJ6Kj4XZBzA

=== 7. Multiple neural network cost function
* 다중 계층 신경망의 비용함수는 마지막과 동일하지만 image:./images/h_theta.png[width="40px"] 는 더 복잡함

image:./images/cost_actual_desire.png[width="400px"]

```
theta0_init = np.random.normal(0,0.01,size=(5,4))
theta1_init = np.random.normal(0,0.01,size=(5,1))

# X and y are in memory and should be used as inputs to multiplecost()
def multiplecost(X, y, theta0, theta1):
    # feed through network
    h = feedforward(X, theta0, theta1)
    # compute error
    inner = y * np.log(h) + (1-y) * np.log(1-h)
    # negative of average error
    return -np.mean(inner)

c = multiplecost(X, y, theta0_init, theta1_init) # 0.69313560903639371
```

=== 8. Backpropagation
* 여러 계층의 매개변수가 있으므로 backpropagation 매서드 구현 필요
* 각 레이어를 통해 공급과 출력을 반환하여 순방향 전파를 이미 구현.
* backpropagation은 마지막 레이어에서 싲가하여 각 레이어를 순환하는 매개변수를 업데이트 하여 그에 따라 업데이트를 함
* 다중 층이 있을 때, l 층에서는 다음을 계산. image:./images/theta_devide.png[width="100px"]
** 3층의 예일 경우, image:./images/3layer_compute.png[width="500px"]
*** 첫번째 레이어는 feature 이며 오류가 없으므로 δ1이 없음
*** 우리는 코드쉘에서 3계층신경망을 훈련시키는 코드를 작성함.
*** 많은 매개변수와 많은 부분이 알고리즘화 됨. 이는 코드 모듈화하기 위함
*** 우리는 feedforward() 와 multiplecost() 를 통해 좀 더 응축된 형태로 사용함
*** 초기화하는 동안 학습속도, 수렴 반복에 대한 최대수 및 hidden layer의 단위 수 와 같은 속성을 설정
*** learn() 을 통해 기울기를 계산하고 파라미터를 업데이트하는 backpropagation (역전파) 알고리즘을 발견할 것임.

```
# Use a class for this model, it's good practice and condenses the code
class NNet3:
    def __init__(self, learning_rate=0.5, maxepochs=1e4, convergence_thres=1e-5, hidden_layer=4):
        self.learning_rate = learning_rate
        self.maxepochs = int(maxepochs)
        self.convergence_thres = 1e-5
        self.hidden_layer = int(hidden_layer)

    def _multiplecost(self, X, y):
        # feed through network
        l1, l2 = self._feedforward(X)
        # compute error
        inner = y * np.log(l2) + (1-y) * np.log(1-l2)
        # negative of average error
        return -np.mean(inner)

    def _feedforward(self, X):
        # feedforward to the first layer
        l1 = sigmoid_activation(X.T, self.theta0).T
        # add a column of ones for bias term
        l1 = np.column_stack([np.ones(l1.shape[0]), l1])
        # activation units are then inputted to the output layer
        l2 = sigmoid_activation(l1.T, self.theta1)
        return l1, l2

    def predict(self, X):
        _, y = self._feedforward(X)
        return y

    def learn(self, X, y):
        nobs, ncols = X.shape
        self.theta0 = np.random.normal(0,0.01,size=(ncols,self.hidden_layer))
        self.theta1 = np.random.normal(0,0.01,size=(self.hidden_layer+1,1))

        self.costs = []
        cost = self._multiplecost(X, y)
        self.costs.append(cost)
        costprev = cost + self.convergence_thres+1  # set an inital costprev to past while loop
        counter = 0  # intialize a counter

        # Loop through until convergence
        for counter in range(self.maxepochs):
            # feedforward through network
            l1, l2 = self._feedforward(X)

            # Start Backpropagation
            # Compute gradients
            l2_delta = (y-l2) * l2 * (1-l2)
            l1_delta = l2_delta.T.dot(self.theta1.T) * l1 * (1-l1)

            # Update parameters by averaging gradients and multiplying by the learning rate
            self.theta1 += l1.T.dot(l2_delta.T) / nobs * self.learning_rate
            self.theta0 += X.T.dot(l1_delta)[:,1:] / nobs * self.learning_rate

            # Store costs and check for convergence
            counter += 1  # Count
            costprev = cost  # Store prev cost
            cost = self._multiplecost(X, y)  # get next cost
            self.costs.append(cost)
            if np.abs(costprev-cost) < self.convergence_thres and counter > 500:
                break

# Set a learning rate
learning_rate = 0.5
# Maximum number of iterations for gradient descent
maxepochs = 10000
# Costs convergence threshold, ie. (prevcost - cost) > convergence_thres
convergence_thres = 0.00001
# Number of hidden units
hidden_units = 4

# Initialize model
model = NNet3(learning_rate=learning_rate, maxepochs=maxepochs,
              convergence_thres=convergence_thres, hidden_layer=hidden_units)
# Train model
model.learn(X, y)

# Plot costs
plt.plot(model.costs)
plt.title("Convergence of the Cost Function")
plt.ylabel("J($\Theta$)")
plt.xlabel("Iteration")
plt.show()
```

image:./images/plt_ConvergenceoftheCostFunction2.png[]
ref : https://www.youtube.com/watch?v=fhrORKjjU7w

=== 9.Splitting data
* 이제 신경망에 대해 배웠고 역전파에 대해 배웠으며 3계층 신경망을 훈련할 코드가 생겼다. 따라서 데이터를 train/test 셋트로 분할하고 모델 실행

```
# First 70 rows to X_train and y_train
# Last 30 rows to X_test and y_test
X_train = X[:70]
y_train = y[:70]

X_test = X[70:] # X_test = X[-30:]
y_test = y[70:]
```

=== 10. Predicting iris flowers
* 홍채꽃의 종을 예측할때 세 계층 신경망이 얼마나 잘 수행하는지 벤치마킹하려면 AUC 곡선 아래 영역 특성 작동 수신기 점수를 계산해야 함.
** http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html[roc_auc_score] , best value at 1 and its worst score at 0
* 함수 NNet3은 모델을 훈련할 뿐아니라 예측을 반환함
* predict()는 2D 행렬을 리턴
* 이 신경망에는 단 하나의 타겟변수만 있기 때문에 꽃의 유형에 해당하는 첫번째 행을 선택해야 함.
